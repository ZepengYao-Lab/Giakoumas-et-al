{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Workflow overview (from exploratory checks to final portrait figure)\n",
    "\n",
    "### Task (what this notebook does)\n",
    "This notebook builds **multi-hop synaptic flow summaries** from curated **sensory neuron sets** and visualizes them as **Sankey diagrams**. The earlier cells are **exploratory** (single-panel Sankeys for individual sets and minor variants), while the last section assembles the **final multi-panel portrait figure** (all Sankeys stacked and exported as one SVG for Illustrator).\n",
    "\n",
    "---\n",
    "\n",
    "### Abbreviations / naming\n",
    "- **FlyWire**: the connectome dataset used here.\n",
    "- **GRN**: gustatory receptor neuron (input lists loaded from CSVs; each has a `root_id` column).\n",
    "- **PhN**: pharyngeal nerve (here: **PhN-SA_v2** sets; 6 CSVs). NOTE: the PhN contains the stomodeal nerve (StN) gut sensory afferents.\n",
    "- **PSO**: presumed pharyngeal sensory organ group (here: **aPhN-SA** sets; 3 CSVs labeled `DCSO`, `aPhN1`, `aPhN2`).\n",
    "- **MxLbN**: maxillary labellar nerve group (here: 4 GRN lists: `Sugar/Water`, `Bitter`, `Ir94e`, `Taste Peg`).\n",
    "- **SA**: sensory axon (used in dataset naming).\n",
    "- **`root_id`**: unique neuron identifier (node ID).\n",
    "- **`pre_root_id` / `post_root_id`**: presynaptic / postsynaptic neuron IDs in `connections`.\n",
    "- **`syn_count`**: number of synapses between a pre→post pair.\n",
    "- **Superclass / `super_class`**: broad anatomical/functional category (used to label nodes and color them).\n",
    "- **Hop**: one step of directed connectivity (pre→post).  \n",
    "  - **Hop 1**: directly downstream of the input set  \n",
    "  - **Hop 2**: downstream of Hop 1 targets  \n",
    "  - **Hop 3**: downstream of Hop 2 targets\n",
    "\n",
    "---\n",
    "\n",
    "### Inputs (tables and lists)\n",
    "- **Connectome edges**: `flywire_data/connections.csv.(gz|zip)` with columns like `pre_root_id`, `post_root_id`, `syn_count`.\n",
    "- **Neuron metadata**: `flywire_data/classification.csv.gz` (subset used: `root_id`, `super_class`).\n",
    "- **Seed neuron sets (CSV lists)**:\n",
    "  - **PhN-SA_v2**: `input/PhN/set_1.csv` … `set_6.csv`\n",
    "  - **PSO/aPhN-SA**: `input/aPhN-SA/set_1.csv` … `set_3.csv` (mapped to `DCSO`, `aPhN1`, `aPhN2`)\n",
    "  - **MxLbN-SA**: `input/MxLbN-SA/*.csv` (4 stimulus/modality lists)\n",
    "\n",
    "---\n",
    "\n",
    "### Core computation (how a Sankey is built)\n",
    "1. **Filter edges by seed set**  \n",
    "   Select rows where `pre_root_id ∈ seed_root_ids`.\n",
    "\n",
    "2. **Aggregate to pair-level synapses**  \n",
    "   Group by `(pre_root_id, post_root_id)` and sum `syn_count` so each directed pair has a total weight.\n",
    "\n",
    "3. **Threshold weak connections**  \n",
    "   Keep only pairs with **total `syn_count ≥ min_syn`** (default used throughout: `min_syn = 5`).\n",
    "\n",
    "4. **Annotate targets with superclass**  \n",
    "   Merge `post_root_id` with `classification` to get `output_super_class`.\n",
    "\n",
    "5. **Repeat for downstream hops (1→3)**  \n",
    "   Hop 1 uses the seed IDs; Hop 2 uses Hop 1 target IDs; Hop 3 uses Hop 2 target IDs.\n",
    "\n",
    "6. **Summarize flows at the superclass level**  \n",
    "   Convert hop-level edges into flow tables:\n",
    "   - **Seed → Hop1 superclass totals**\n",
    "   - **Hop1 superclass → Hop2 superclass totals**\n",
    "   - **Hop2 superclass → Hop3 superclass totals**\n",
    "\n",
    "7. **Plot Sankey with consistent labeling and colors**  \n",
    "   Nodes are labeled by hop, e.g. `1: central`, `2: motor`, `3: endocrine`.  \n",
    "   Colors are assigned by `super_class` so the same superclass has the same color within a figure (and, where implemented, globally).\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook structure (exploratory → final)\n",
    "- **Exploratory section(s)**:\n",
    "  - Generate **single Sankey diagrams** per set for:\n",
    "    - the 6 **PhN-SA_v2** sets,\n",
    "    - the 3 **PSO/aPhN-SA** sets,\n",
    "    - the 4 **MxLbN-SA** sets.\n",
    "  - These cells help validate thresholds, hop logic, labels, and layout.\n",
    "\n",
    "- **Final figure section (for Supplementary Fig. S3)**:\n",
    "  - Defines `make_sankey_trace(...)` to return a Plotly Sankey trace.\n",
    "  - Loads **all 13 datasets** (6 PhN + 3 PSO + 4 MxLbN).\n",
    "  - Uses `plotly.subplots.make_subplots` to stack them into a **13×1 portrait layout**.\n",
    "  - Exports a **single SVG** (intended for Adobe Illustrator edits).\n",
    "\n",
    "---\n",
    "\n",
    "### Outputs\n",
    "- Individual Sankey panels are shown interactively, and the final portrait figure is written to:\n",
    "  - `./Giakoumas-et-al/output/figures/fig_S3/all_sankeys_portrait.svg`\n"
   ],
   "id": "48140d198b81d963"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and super_class classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    './Giakoumas-et-al/flywire_data/connections.csv.zip'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    './Giakoumas-et-al/flywire_data/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Build a GLOBAL color map for every superclass once, up-front\n",
    "# ----------------------------------------------------------------------------\n",
    "all_classes_global = sorted(classification['super_class'].unique())\n",
    "palette = px.colors.qualitative.Safe\n",
    "GLOBAL_COLOR_MAP = {\n",
    "    cls: palette[i % len(palette)]\n",
    "    for i, cls in enumerate(all_classes_global)\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2. Load your six PhN-SA_v2 sets (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "sets = {\n",
    "    f'Set{i}': pd.read_csv(\n",
    "        f'./Giakoumas-et-al/input/PhN/set_{i}.csv'\n",
    "    )\n",
    "    for i in range(1, 7)\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(\n",
    "            columns={'root_id':'post_root_id','super_class':'output_super_class'}\n",
    "        ),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build and show a Sankey diagram with consistent colors\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # hops\n",
    "    df1 = build_hop_df(grn_df['root_id'],            connections, classification, min_syn)\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum().reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'])['syn_count_2']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'])['syn_count_3']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # nodes\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx   = {node:i for i,node in enumerate(nodes)}\n",
    "\n",
    "    # colors: reuse the GLOBAL_COLOR_MAP\n",
    "    node_colors = [\n",
    "        'lightgrey' if n == title else GLOBAL_COLOR_MAP[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # links\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    def add_links(df, src_col, tgt_col):\n",
    "        for _, r in df.iterrows():\n",
    "            s = idx[r[src_col]]\n",
    "            t = idx[r[tgt_col]]\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(r['count'])\n",
    "            link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "\n",
    "    # flow1 → 1:class\n",
    "    flow1 = flow1.rename(columns={'source':'src','output_super_class':'dst'})\n",
    "    flow1['dst'] = flow1['dst'].map(lambda c: f\"1: {c}\")\n",
    "    add_links(flow1, 'src', 'dst')\n",
    "\n",
    "    # flow2 → 2:class\n",
    "    flow2 = flow2.rename(columns={\n",
    "        'output_super_class_1':'src','output_super_class_2':'dst'\n",
    "    })\n",
    "    flow2['src'] = flow2['src'].map(lambda c: f\"1: {c}\")\n",
    "    flow2['dst'] = flow2['dst'].map(lambda c: f\"2: {c}\")\n",
    "    add_links(flow2, 'src', 'dst')\n",
    "\n",
    "    # flow3 → 3:class\n",
    "    flow3 = flow3.rename(columns={\n",
    "        'output_super_class_2':'src','output_super_class_3':'dst'\n",
    "    })\n",
    "    flow3['src'] = flow3['src'].map(lambda c: f\"2: {c}\")\n",
    "    flow3['dst'] = flow3['dst'].map(lambda c: f\"3: {c}\")\n",
    "    add_links(flow3, 'src', 'dst')\n",
    "\n",
    "    # hover\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # positions\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    y = []\n",
    "    for col in (col1, col2, col3, col4):\n",
    "        n = len(col)\n",
    "        y.extend([0.5] if n==1 else list(np.linspace(0,1,n)))\n",
    "\n",
    "    # draw\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=f\"{title}\", font_size=14)\n",
    "    fig.write_image(f\"{title.replace('/','_')}.svg\", width=1200, height=800, scale=2)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3. Generate a Sankey for each of the six PhN-SA_v2 sets\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "2991077c0e5c47c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate a Sankey for each PSO set",
   "id": "e4315134fddf1bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and super_class classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    './Giakoumas-et-al/flywire_data/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    './Giakoumas-et-al/flywire_data/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO sets (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_1.csv')\n",
    "set_2 = pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_2.csv')\n",
    "set_3 = pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_3.csv')\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(\n",
    "            columns={'root_id':'post_root_id','super_class':'output_super_class'}\n",
    "        ),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build and show a Sankey diagram with maximal vertical spacing\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # 1) build hops\n",
    "    df1 = build_hop_df(grn_df['root_id'],            connections, classification, min_syn)\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # 2) summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum().reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id', suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'])['syn_count_2']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id', suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'])['syn_count_3']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # 3) build node labels\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx   = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # 4) color mapping\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    all_classes = sorted(\n",
    "        set(df1['output_super_class']) |\n",
    "        set(df2['output_super_class']) |\n",
    "        set(df3['output_super_class'])\n",
    "    )\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = ['lightgrey' if n==title else color_map[n.split(': ',1)[1]] for n in nodes]\n",
    "\n",
    "    # 5) assemble links\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    def add_links(flow_df, src_col, tgt_col):\n",
    "        for _, r in flow_df.iterrows():\n",
    "            s = idx[r[src_col]]\n",
    "            t = idx[r[tgt_col]]\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(r['count'])\n",
    "            link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "\n",
    "    flow1 = flow1.rename(columns={'source':'src','output_super_class':'dst'})\n",
    "    flow1['dst'] = flow1['dst'].map(lambda c: f\"1: {c}\")\n",
    "    add_links(flow1, 'src','dst')\n",
    "\n",
    "    flow2 = flow2.rename(columns={'output_super_class_1':'src','output_super_class_2':'dst'})\n",
    "    flow2['src'] = flow2['src'].map(lambda c: f\"1: {c}\")\n",
    "    flow2['dst'] = flow2['dst'].map(lambda c: f\"2: {c}\")\n",
    "    add_links(flow2, 'src','dst')\n",
    "\n",
    "    flow3 = flow3.rename(columns={'output_super_class_2':'src','output_super_class_3':'dst'})\n",
    "    flow3['src'] = flow3['src'].map(lambda c: f\"2: {c}\")\n",
    "    flow3['dst'] = flow3['dst'].map(lambda c: f\"3: {c}\")\n",
    "    add_links(flow3, 'src','dst')\n",
    "\n",
    "    # 6) hover info\n",
    "    incoming = dict.fromkeys(nodes,0)\n",
    "    outgoing = dict.fromkeys(nodes,0)\n",
    "    for s,t,v in zip(source,target,value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # 7) x positions\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "\n",
    "    # 8) y positions: spread evenly down the column\n",
    "    y = []\n",
    "    for col in (col1, col2, col3, col4):\n",
    "        n = len(col)\n",
    "        if n == 1:\n",
    "            y.append(0.5)\n",
    "        else:\n",
    "            y.extend(np.linspace(0, 1, n))\n",
    "\n",
    "    # 9) draw Sankey with freeform arrangement\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=f\"{title}\", font_size=14)\n",
    "    fig.write_image(f\"{title.replace('/','_')}.svg\", width=1200, height=800, scale=2)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 10. Generate a Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "c76c624bdbe3a58b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate Sankey for each MxLbN set",
   "id": "59f082531fbf9068"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    './Giakoumas-et-al/flywire_data/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    './Giakoumas-et-al/flywire_data/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your four MxLbN GRN lists (each CSV has a 'root_id' column).\n",
    "# ----------------------------------------------------------------------------\n",
    "sugar_water = pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/sugar_water_GRNs.csv\")\n",
    "bitter      = pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/bitter_GRNs.csv\")\n",
    "ir94e       = pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/Ir94e_GRNs.csv\")\n",
    "taste_peg   = pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/taste_peg_GRNs.csv\")\n",
    "\n",
    "sets = {\n",
    "    'Sugar/Water': sugar_water,\n",
    "    'Bitter':      bitter,\n",
    "    'Ir94e':       ir94e,\n",
    "    'Taste Peg':   taste_peg,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={'root_id':'post_root_id','super_class':'output_super_class'}),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build and show a Sankey diagram using Plotly's Safe palette\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # build hop-level dataframes\n",
    "    df1 = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum()\n",
    "           .reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(\n",
    "        df1, df2,\n",
    "        left_on='post_root_id', right_on='pre_root_id',\n",
    "        suffixes=('_1','_2')\n",
    "    )\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'])['syn_count_2']\n",
    "           .sum()\n",
    "           .reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(\n",
    "        df2, df3,\n",
    "        left_on='post_root_id', right_on='pre_root_id',\n",
    "        suffixes=('_2','_3')\n",
    "    )\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'])['syn_count_3']\n",
    "           .sum()\n",
    "           .reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # assemble nodes\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx   = {node:i for i,node in enumerate(nodes)}\n",
    "\n",
    "    # assign colors via Safe palette\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    all_classes = sorted(\n",
    "        set(df1['output_super_class'])\n",
    "      | set(df2['output_super_class'])\n",
    "      | set(df3['output_super_class'])\n",
    "    )\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n == title else color_map[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # build link lists\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    # flow1 links\n",
    "    for _, r in flow1.iterrows():\n",
    "        s = idx[title]\n",
    "        t = idx[f\"1: {r['output_super_class']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "    # flow2 links\n",
    "    for _, r in flow2.iterrows():\n",
    "        s = idx[f\"1: {r['output_super_class_1']}\"]\n",
    "        t = idx[f\"2: {r['output_super_class_2']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "    # flow3 links\n",
    "    for _, r in flow3.iterrows():\n",
    "        s = idx[f\"2: {r['output_super_class_2']}\"]\n",
    "        t = idx[f\"3: {r['output_super_class_3']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "\n",
    "    # compute hover info\n",
    "    incoming, outgoing = dict.fromkeys(nodes,0), dict.fromkeys(nodes,0)\n",
    "    for s,t,v in zip(source,target,value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [\n",
    "        f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\"\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # set x-positions\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "\n",
    "    # plot Sankey\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x,\n",
    "            color=node_colors,\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate Sankey for each MxLbN set\n",
    "# ----------------------------------------------------------------------------\n",
    "datasets = [\n",
    "    ('Sugar/Water', sugar_water),\n",
    "    ('Bitter',      bitter),\n",
    "    ('Ir94e',       ir94e),\n",
    "    ('Taste Peg',   taste_peg),\n",
    "]\n",
    "for label, df in datasets:\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "19ac422c20615fdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Generate Sankey for each PSOS set",
   "id": "3528d4796eb40caa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Only psos:\n",
    "psos = [\n",
    "    ('DCSO',  pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_1.csv')),\n",
    "    ('aPhN1', pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_2.csv')),\n",
    "    ('aPhN2', pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_3.csv')),\n",
    "]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    vertical_spacing=0.02,\n",
    "    specs=[[{\"type\":\"sankey\"}]] * 3\n",
    ")\n",
    "for i, (label, df) in enumerate(all_sets, start=1):\n",
    "    sankey_trace = make_sankey_trace(\n",
    "        grn_df         = df,\n",
    "        title          = label,\n",
    "        connections    = connections,\n",
    "        classification = classification,\n",
    "        min_syn        = 5\n",
    "    )\n",
    "    fig.add_trace(sankey_trace, row=i, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,    # 3 panels × 250 px each = 750 px + small margins\n",
    "    width=600,\n",
    "    margin=dict(l=20, r=20, t=30, b=20),\n",
    "    font=dict(size=10),\n",
    "    title=\"PSO Sets Sankeys (stacked)\",\n",
    ")\n",
    "fig.show()\n"
   ],
   "id": "b0e6608bbad554e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Main figure for Supplementary Figure S3 to be edited in Adobe Illustrator: all 13 Sankey diagrams in one portrait figure",
   "id": "a116551577fd2ea6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "def make_sankey_trace(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Build hop‐level dataframes without using .query()\n",
    "    def build_hop_df(src_ids):\n",
    "        df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "        summed = (\n",
    "            df.groupby(['pre_root_id', 'post_root_id'], as_index=False)\n",
    "              .agg({'syn_count': 'sum'})\n",
    "        )\n",
    "        summed = summed[summed['syn_count'] >= min_syn]\n",
    "        merged = pd.merge(\n",
    "            summed,\n",
    "            classification.rename(\n",
    "                columns={'root_id': 'post_root_id', 'super_class': 'output_super_class'}\n",
    "            ),\n",
    "            on='post_root_id', how='left'\n",
    "        )\n",
    "        return merged[['pre_root_id', 'post_root_id', 'output_super_class', 'syn_count']]\n",
    "\n",
    "    # 1) Build the three hops\n",
    "    df1 = build_hop_df(grn_df['root_id'])\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique())\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique())\n",
    "\n",
    "    # 2) Summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum().reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(\n",
    "        df1, df2,\n",
    "        left_on='post_root_id', right_on='pre_root_id',\n",
    "        suffixes=('_1', '_2')\n",
    "    )\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1', 'output_super_class_2'])['syn_count_2']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(\n",
    "        df2, df3,\n",
    "        left_on='post_root_id', right_on='pre_root_id',\n",
    "        suffixes=('_2', '_3')\n",
    "    )\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2', 'output_super_class_3'])['syn_count_3']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # 3) Node labels\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n: i for i, n in enumerate(nodes)}\n",
    "\n",
    "    # 4) Node colors\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    all_classes = sorted(\n",
    "        set(df1['output_super_class']) |\n",
    "        set(df2['output_super_class']) |\n",
    "        set(df3['output_super_class'])\n",
    "    )\n",
    "    color_map = {cls: palette[i % len(palette)] for i, cls in enumerate(all_classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n == title else color_map[n.split(': ', 1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # 5) Build link lists\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "\n",
    "    # flow1 → “1:Class”\n",
    "    flow1 = flow1.rename(columns={'source': 'src', 'output_super_class': 'dst'})\n",
    "    flow1['dst'] = flow1['dst'].map(lambda c: f\"1: {c}\")\n",
    "    for _, r in flow1.iterrows():\n",
    "        s = idx[r['src']]\n",
    "        t = idx[r['dst']]\n",
    "        source.append(s)\n",
    "        target.append(t)\n",
    "        value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb', 'rgba').replace(')', ',0.5)'))\n",
    "\n",
    "    # flow2 → “2:Class”\n",
    "    flow2 = flow2.rename(columns={'output_super_class_1': 'src', 'output_super_class_2': 'dst'})\n",
    "    flow2['src'] = flow2['src'].map(lambda c: f\"1: {c}\")\n",
    "    flow2['dst'] = flow2['dst'].map(lambda c: f\"2: {c}\")\n",
    "    for _, r in flow2.iterrows():\n",
    "        s = idx[r['src']]\n",
    "        t = idx[r['dst']]\n",
    "        source.append(s)\n",
    "        target.append(t)\n",
    "        value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb', 'rgba').replace(')', ',0.5)'))\n",
    "\n",
    "    # flow3 → “3:Class”\n",
    "    flow3 = flow3.rename(columns={'output_super_class_2': 'src', 'output_super_class_3': 'dst'})\n",
    "    flow3['src'] = flow3['src'].map(lambda c: f\"2: {c}\")\n",
    "    flow3['dst'] = flow3['dst'].map(lambda c: f\"3: {c}\")\n",
    "    for _, r in flow3.iterrows():\n",
    "        s = idx[r['src']]\n",
    "        t = idx[r['dst']]\n",
    "        source.append(s)\n",
    "        target.append(t)\n",
    "        value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb', 'rgba').replace(')', ',0.5)'))\n",
    "\n",
    "    # 6) Compute hover info\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s_, t_, v_ in zip(source, target, value):\n",
    "        outgoing[nodes[s_]] += v_\n",
    "        incoming[nodes[t_]] += v_\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # 7) X positions\n",
    "    x0 = [0.0] * len(col1)\n",
    "    x1 = [0.33] * len(col2)\n",
    "    x2 = [0.66] * len(col3)\n",
    "    x3 = [1.0] * len(col4)\n",
    "    x = x0 + x1 + x2 + x3\n",
    "\n",
    "    # 8) Y positions\n",
    "    y = []\n",
    "    for col in (col1, col2, col3, col4):\n",
    "        n = len(col)\n",
    "        if n == 1:\n",
    "            y.append(0.5)\n",
    "        else:\n",
    "            y.extend(list(np.linspace(0, 1, n)))\n",
    "\n",
    "    # 9) **Return the Sankey trace directly** (no go.Trace wrapping)\n",
    "    sankey = go.Sankey(\n",
    "        name=title,\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            color=node_colors,\n",
    "            pad=8,\n",
    "            thickness=12,\n",
    "            line=dict(color='black', width=0.3),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    )\n",
    "\n",
    "    return sankey\n"
   ],
   "id": "2f9957b8178ad6ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Now stack all 13 Sankey traces in a single portrait figure:\n",
    "\n",
    "# (A) Gather all dataframes:\n",
    "sets_phn_sa = {\n",
    "    f'PhN-SA_v2_{i}': pd.read_csv(\n",
    "        f'./Giakoumas-et-al/input/PhN/set_{i}.csv'\n",
    "    )\n",
    "    for i in range(1, 7)\n",
    "}\n",
    "sets_pso = {\n",
    "    'DCSO':  pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_1.csv'),\n",
    "    'aPhN1': pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_2.csv'),\n",
    "    'aPhN2': pd.read_csv('./Giakoumas-et-al/input/aPhN-SA/set_3.csv'),\n",
    "}\n",
    "sets_mxlbn = {\n",
    "    'Sugar/Water': pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/sugar_water_GRNs.csv\"),\n",
    "    'Bitter':      pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/bitter_GRNs.csv\"),\n",
    "    'Ir94e':       pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/Ir94e_GRNs.csv\"),\n",
    "    'Taste Peg':   pd.read_csv(\"./Giakoumas-et-al/input/MxLbN-SA/taste_peg_GRNs.csv\"),\n",
    "}\n",
    "\n",
    "all_sets = []\n",
    "all_sets += list(sets_phn_sa.items())\n",
    "all_sets += list(sets_pso.items())\n",
    "all_sets += list(sets_mxlbn.items())\n",
    "\n",
    "n_plots = len(all_sets)  # 13\n",
    "\n",
    "# (B) Create subplots (13 rows × 1 column, each type=\"sankey\")\n",
    "fig = make_subplots(\n",
    "    rows = n_plots,\n",
    "    cols = 1,\n",
    "    shared_xaxes = False,\n",
    "    shared_yaxes = False,\n",
    "    vertical_spacing = 0.02,\n",
    "    specs = [[{\"type\": \"sankey\"}] for _ in range(n_plots)],\n",
    ")\n",
    "\n",
    "# (C) Add each Sankey trace into its own row\n",
    "for i, (label, df) in enumerate(all_sets, start=1):\n",
    "    sankey_trace = make_sankey_trace(\n",
    "        grn_df         = df,\n",
    "        title          = label,\n",
    "        connections    = connections,\n",
    "        classification = classification,\n",
    "        min_syn        = 5\n",
    "    )\n",
    "    fig.add_trace(sankey_trace, row=i, col=1)\n",
    "\n",
    "# (D) Layout tweaks for portrait\n",
    "fig.update_layout(\n",
    "    height = 300 * n_plots,    # Still 3900 px for 13 plots\n",
    "    width  = 534,              # One third of original 1600 px\n",
    "    margin = dict(l=20, r=20, t=40, b=20),\n",
    "    font   = dict(size=10),\n",
    "    title  = \"All Sankey Panels in One Portrait Figure\",\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# To export as a single SVG:\n",
    "fig.write_image(\"./Giakoumas-et-al/output/figures/fig_S3/all_sankeys_portrait.svg\")\n"
   ],
   "id": "cc25df63bde232c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5ba6dd8f8ccae6c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
