{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:52.118949Z",
     "start_time": "2025-04-24T23:33:52.116553Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d119c6e1fe6ea15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:52.369926Z",
     "start_time": "2025-04-24T23:33:52.365886Z"
    }
   },
   "outputs": [],
   "source": [
    "#from aPhN2-SA_Activation import set_1\n",
    "#%pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb850e28ae0da34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:52.908370Z",
     "start_time": "2025-04-24T23:33:52.424708Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.optimize import curve_fit\n",
    "from venn import venn\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e1a132f1c84ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:52.927573Z",
     "start_time": "2025-04-24T23:33:52.923661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seaborn theme to white\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# set up matplot lib theme\n",
    "andy_theme = {'axes.grid': True,\n",
    "              'grid.linestyle': '--',\n",
    "              'legend.framealpha': 1,\n",
    "              'legend.facecolor': 'white',\n",
    "              'legend.shadow': False,\n",
    "              'legend.fontsize': 14,\n",
    "              'legend.title_fontsize': 14,\n",
    "              'font.sans-serif':'Helvetica',\n",
    "              'xtick.labelsize': 8,\n",
    "              'ytick.labelsize': 8,\n",
    "              'axes.labelsize': 12,\n",
    "              'axes.titlesize': 16,\n",
    "              'figure.dpi': 300}\n",
    "\n",
    "plt.rcParams.update(andy_theme)\n",
    "\n",
    "#Uncomment next 2 lines if matplotlib can not find Helvetica font\n",
    "#plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "#plt.rcParams['font.sans-serif'] = ['Arial']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce21ff8d2fcca52",
   "metadata": {},
   "source": [
    "## 1. FIRST ORDER ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa8f5d8fdb7b9f",
   "metadata": {},
   "source": [
    "### Load the datasets with neurons and connections.\n",
    "\n",
    "* This script assumes that the **CSV** files (`.csv.gz`) and **aPhN2-SAs lists** (`_new.csv` files) are in the same folder as this notebook or script.\n",
    "* These files include four CSVs containing manually curated  aPhN2-SAs lists and four connectome datasets from FlyWire:\n",
    "  1. **`classification.csv.gz`**\n",
    "  2. **`connections.csv.gz`**\n",
    "  3. **`neuropil_synapse_table.csv.gz`**\n",
    "  4. **`neurons.csv.gz`**\n",
    "* **Axon lists** were curated manually as described in the paper.\n",
    "* **Connectome datasets** were downloaded from the FlyWire website using **snapshot 783** (previous snapshot 630).\n",
    "* We focus on putative sensory axons from the Drosophila **pharyngeal nerve** in this analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e039d8f67fe1a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:57.032503Z",
     "start_time": "2025-04-24T23:33:52.963393Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connections dataset and additional data sets\n",
    "\n",
    "# Load the connections dataset\n",
    "# columns: pre_root_id, post_root_id, neuropil, syn_count, nt_type\n",
    "connections = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz')\n",
    "\n",
    "# Neuropil synapses\n",
    "# columns: root_id, input synapses, input partners, output synapses, output partners, etc\n",
    "# Keep only root_id, input syanapses, output synapses\n",
    "neuropil_synapse = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/neuropil_synapse_table.csv.gz')[['root_id', 'input synapses', 'output synapses']]\n",
    "\n",
    "# Rename with underscores\n",
    "neuropil_synapse.rename(columns={'input synapses': 'input_synapses','output synapses': 'output_synapses'}, inplace=True)\n",
    "\n",
    "# Load classification table\n",
    "# columns: root_id, flow, super_class, side, etc\n",
    "# Keep only root_id and side\n",
    "classification = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz')[['root_id', 'side']]\n",
    "classification_other = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz')[['root_id', 'super_class', 'class']]\n",
    "\n",
    "# Load data about each neuron\n",
    "# columns: root_id, group, nt_type, etc\n",
    "# Keep only root_id, nt_type\n",
    "neurons = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/neurons.csv.gz')[['root_id', 'nt_type']]\n",
    "\n",
    "# Merging additional data in one data set\n",
    "neurons_data = pd.merge(neurons, pd.merge(classification, neuropil_synapse, on='root_id',how= 'outer'), on='root_id',how='outer')\n",
    "\n",
    "# Load putative PSA lists\n",
    "set_1 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN2-SA_v1/set_1.csv')\n",
    "set_2 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN2-SA_v1/set_2.csv')\n",
    "set_3 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN2-SA_v1/set_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3110ef054687b1c",
   "metadata": {},
   "source": [
    "### Find downstream connections of aPhN2-SAs\n",
    "- includes all neurons downstream of aPhN2-SAs - we will filter out set-set connections later\n",
    "- minimum of 5 synapses between the two neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2857fd896c31e49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:57.055253Z",
     "start_time": "2025-04-24T23:33:57.050034Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to get outputs of aPhN2-SAs\n",
    "def neuronal_outputs(aph1_sa):\n",
    "    # Merge the aph1_sa DataFrame with the 'connections' data, filtering out any connections\n",
    "    # that have fewer than 5 synapses\n",
    "    connectivity = pd.merge(\n",
    "        aph1_sa['root_id'],\n",
    "        connections[['pre_root_id','post_root_id','neuropil','syn_count','nt_type']],\n",
    "        left_on='root_id',\n",
    "        right_on='pre_root_id',\n",
    "        how='inner'\n",
    "    ).query(\"syn_count >= 5\")\n",
    "\n",
    "    # Remove the temporary 'root_id' column that came from the aph1_sa DataFrame\n",
    "    connectivity = connectivity.drop(columns='root_id')\n",
    "\n",
    "    # Define function to categorize connection location\n",
    "    def projection(neuropil):\n",
    "        if neuropil in ['GNG', 'PRW', 'SAD', 'FLA_L', 'FLA_R', 'CAN']:  # Example SEZ-related regions\n",
    "            return 'local'\n",
    "        else:\n",
    "            return 'outside_SEZ'\n",
    "\n",
    "    # Apply the projection categorization to each row in 'connectivity'\n",
    "    connectivity['location_of_connection'] = connectivity['neuropil'].apply(projection)\n",
    "\n",
    "    return connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa9d110b3597ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:57.713521Z",
     "start_time": "2025-04-24T23:33:57.093320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the outputs for each set of aPhN2-SAs\n",
    "set_1_outputs = neuronal_outputs(set_1)\n",
    "set_2_outputs = neuronal_outputs(set_2)\n",
    "set_3_outputs = neuronal_outputs(set_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ed827c3c0193a",
   "metadata": {},
   "source": [
    "### Quantifying aPhN2-SA to aPhN2-SA communication\n",
    "# This creates a heatmap showing the number of synapses between each set of pharyngeal sensory axons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013a86204c62a7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:57.929008Z",
     "start_time": "2025-04-24T23:33:57.734056Z"
    }
   },
   "outputs": [],
   "source": [
    "def aphn1_sa_heatmap_matrix(outputs_list, sets_list):\n",
    "    \"\"\"\n",
    "    Given a list of DataFrames representing the outputs of each aPhN2-SA set\n",
    "    (e.g., set_1_outputs, set_2_outputs, etc.) and a list of DataFrames of the\n",
    "    actual sets (e.g., set_1, set_2, ...), return an NxN matrix where\n",
    "    matrix[i, j] is the sum of 'syn_count' from set i to set j.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs_list : list of pd.DataFrame\n",
    "        [set_1_outputs, set_2_outputs, set_3_outputs, ...]\n",
    "    sets_list : list of pd.DataFrame\n",
    "        [set_1, set_2, set_3, ...]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An NxN integer matrix with syn_count sums.\n",
    "    \"\"\"\n",
    "    n = len(sets_list)\n",
    "    # Initialize an NxN matrix of zeros\n",
    "    syn_matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    # Loop over each set's outputs (the \"upstream\" side)\n",
    "    for i, out_df in enumerate(outputs_list):\n",
    "        # For each possible downstream set\n",
    "        for j, set_df in enumerate(sets_list):\n",
    "            merged_df = pd.merge(\n",
    "                out_df,\n",
    "                set_df,\n",
    "                left_on='post_root_id',  # from the outputs side\n",
    "                right_on='root_id',      # from the set's root IDs\n",
    "                how='inner'\n",
    "            )\n",
    "            # Sum all syn_count\n",
    "            syn_matrix[i, j] = merged_df['syn_count'].sum()\n",
    "\n",
    "    return syn_matrix\n",
    "\n",
    "\n",
    "# We assume you already have:\n",
    "# set_1, set_2, set_3\n",
    "# set_1_outputs, set_2_outputs, set_3_outputs\n",
    "\n",
    "all_sets = [set_1, set_2, set_3]\n",
    "all_outputs = [set_1_outputs, set_2_outputs, set_3_outputs]\n",
    "\n",
    "\n",
    "# Create the matrix of syn_counts\n",
    "syn_matrix = aphn1_sa_heatmap_matrix(all_outputs, all_sets)\n",
    "\n",
    "# Provide labels for rows (upstream sets) and columns (downstream sets)\n",
    "row_labels = ['set_1', 'set_2', 'set_3']\n",
    "col_labels = ['set_1', 'set_2', 'set_3']\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(6,5))  # optional figure size\n",
    "sns.heatmap(\n",
    "    syn_matrix,\n",
    "    cmap='viridis',\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    xticklabels=col_labels,\n",
    "    yticklabels=row_labels,\n",
    "    square=True,\n",
    "    vmin=0\n",
    ")\n",
    "plt.title(\"Set-to-Set Synapses (aPhN2-SAs)\")\n",
    "plt.ylabel(\"Upstream Set\")\n",
    "plt.xlabel(\"Downstream Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8469ebb57ef92",
   "metadata": {},
   "source": [
    "## 2. SECOND ORDER NEURON ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d44f97b2086ddb6",
   "metadata": {},
   "source": [
    "## 2a. Identify 2Ns and their connections and organize the data\n",
    "\n",
    "### Define and run a function to identify second order neurons (2Ns)\n",
    "- neuron must be downstream of aPhN2-SAs\n",
    "- minimum of 5 synapses between the two neurons\n",
    "- we will filter out aPhN2-SAs from the list later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f96012359a711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:57.998576Z",
     "start_time": "2025-04-24T23:33:57.994001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function\n",
    "def second_order(aph1_sa, set_label):\n",
    "    \"\"\"\n",
    "    Given a set of pharyngeal sensory axons (aPhN2-SAs) with a 'root_id' column,\n",
    "    this function extracts the connectivity (filtered for syn_count >= 5) and\n",
    "    aggregates it to yield second-order outputs.\n",
    "\n",
    "    Parameters:\n",
    "        aph1_sa : pd.DataFrame\n",
    "            DataFrame containing a column 'root_id' for the aPhN2-SAs.\n",
    "        set_label : str\n",
    "            A label (e.g., 'set_1', 'set_2', etc.) used to annotate the output columns.\n",
    "\n",
    "    Returns:\n",
    "        connectivity : pd.DataFrame\n",
    "            DataFrame containing the filtered connectivity data.\n",
    "        second_orders : pd.DataFrame\n",
    "            Aggregated connectivity DataFrame with:\n",
    "              - 'root_id': post-synaptic neuron ID,\n",
    "              - 'upstream_<set_label>_aPhN2_SAs': count of unique upstream aPhN2-SAs,\n",
    "              - '<set_label>_syn_count': total synapse count,\n",
    "              - 'const': constant column for OLS regression.\n",
    "    \"\"\"\n",
    "    # Get connectivity from aPhN2-SAs\n",
    "    connectivity = pd.merge(\n",
    "        aph1_sa['root_id'],\n",
    "        connections[['pre_root_id', 'post_root_id', 'neuropil', 'syn_count', 'nt_type']],\n",
    "        left_on='root_id',\n",
    "        right_on='pre_root_id',\n",
    "        how='inner'\n",
    "    ).query(\"syn_count >= 5\")\n",
    "\n",
    "    connectivity = connectivity.drop(columns='root_id')\n",
    "\n",
    "    # Aggregate connectivity to obtain second-order outputs\n",
    "    second_orders = connectivity.groupby(\"post_root_id\").agg({\n",
    "        'pre_root_id': 'nunique',\n",
    "        'syn_count': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    second_orders.columns = ['root_id', 'upstream_' + set_label + '_aPhN2_SAs', set_label + '_syn_count']\n",
    "    second_orders['const'] = 1  # For OLS regression later\n",
    "\n",
    "    return connectivity, second_orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eceb2c005e6be81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:58.699899Z",
     "start_time": "2025-04-24T23:33:58.062559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run function to get second-order connectivity and list of second-order outputs (2Ns) for each set\n",
    "set_1_second_order_connectivity, set_1_2Ns = second_order(set_1, 'set_1')\n",
    "set_2_second_order_connectivity, set_2_2Ns = second_order(set_2, 'set_2')\n",
    "set_3_second_order_connectivity, set_3_2Ns = second_order(set_3, 'set_3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79551a15b92a260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:58.748104Z",
     "start_time": "2025-04-24T23:33:58.733592Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at set_6_2Ns as an example\n",
    "set_1_2Ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72765733c9af63c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:33:58.880563Z",
     "start_time": "2025-04-24T23:33:58.876830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking how many 2Ns we have for each set\n",
    "print(\"Set_1 2Ns:\", np.unique(set_1_2Ns.root_id.values).shape)\n",
    "print(\"Set_2 2Ns:\", np.unique(set_2_2Ns.root_id.values).shape)\n",
    "print(\"Set_3 2Ns:\", np.unique(set_3_2Ns.root_id.values).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdae73496d2adc",
   "metadata": {},
   "source": [
    "### Getting more information about 2Ns from flywire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e4c71418c928b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:01.723042Z",
     "start_time": "2025-04-24T23:34:01.699876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select and merge rows from neurons_data for each set of 2Ns\n",
    "# 'neurons_data' contains columns like nt_type, side, input_synapses, output_synapses\n",
    "set_1_2Ns = pd.merge(set_1_2Ns, neurons_data, on='root_id', how='inner')\n",
    "set_2_2Ns = pd.merge(set_2_2Ns, neurons_data, on='root_id', how='inner')\n",
    "set_3_2Ns = pd.merge(set_3_2Ns, neurons_data, on='root_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28c35431341799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:01.916060Z",
     "start_time": "2025-04-24T23:34:01.912185Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Checking that we didn't drop neurons with the merge\n",
    "print(\"Set_1 2Ns (after merge):\", np.unique(set_1_2Ns.root_id.values).shape)\n",
    "print(\"Set_2 2Ns (after merge):\", np.unique(set_2_2Ns.root_id.values).shape)\n",
    "print(\"Set_3 2Ns (after merge):\", np.unique(set_3_2Ns.root_id.values).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294f6bbee8562c39",
   "metadata": {},
   "source": [
    "### Remove aPhN2-SAs from 2N lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d45497d2b6fbce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:02.127926Z",
     "start_time": "2025-04-24T23:34:02.116879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all sets into a single DataFrame\n",
    "all_sets = pd.concat([set_1, set_2, set_3], axis=0)\n",
    "\n",
    "# Drop 2Ns that are actually in the pharyngeal sensory axon sets\n",
    "set_1_2Ns = set_1_2Ns[~set_1_2Ns['root_id'].isin(all_sets['root_id'])]\n",
    "set_2_2Ns = set_2_2Ns[~set_2_2Ns['root_id'].isin(all_sets['root_id'])]\n",
    "set_3_2Ns = set_3_2Ns[~set_3_2Ns['root_id'].isin(all_sets['root_id'])]\n",
    "\n",
    "# Drop all second-order connections where post-synaptic neurons are also in the sets\n",
    "set_1_second_order_connectivity = set_1_second_order_connectivity[\n",
    "    ~set_1_second_order_connectivity['post_root_id'].isin(all_sets['root_id'])\n",
    "]\n",
    "set_2_second_order_connectivity = set_2_second_order_connectivity[\n",
    "    ~set_2_second_order_connectivity['post_root_id'].isin(all_sets['root_id'])\n",
    "]\n",
    "set_3_second_order_connectivity = set_3_second_order_connectivity[\n",
    "    ~set_3_second_order_connectivity['post_root_id'].isin(all_sets['root_id'])\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffe50c634af3ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:02.335043Z",
     "start_time": "2025-04-24T23:34:02.331190Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1) Check how many 2Ns are left for each set\n",
    "print(\"Set_1 2Ns:\", np.unique(set_1_2Ns.root_id.values).shape)\n",
    "print(\"Set_2 2Ns:\", np.unique(set_2_2Ns.root_id.values).shape)\n",
    "print(\"Set_3 2Ns:\", np.unique(set_3_2Ns.root_id.values).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26c86ceeb61cee",
   "metadata": {},
   "source": [
    "### Add the data from the model simulations to the 2N dataframes\n",
    "\n",
    "This will be necessary to analyze activated vs. non-activated 2Ns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbd36fffd6f850",
   "metadata": {},
   "source": [
    "First, import the data and add the activation status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d9fa076bc6c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:02.484678Z",
     "start_time": "2025-04-24T23:34:02.475994Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4) Make a DataFrame with data for all sets\n",
    "all_2N = pd.concat([\n",
    "    set_1_2Ns,\n",
    "    set_2_2Ns,\n",
    "    set_3_2Ns,\n",
    "])\n",
    "\n",
    "# The columns to merge and sum up depend on how you named them in the 'second_order' step\n",
    "# Here, we assume they're named 'set_1_syn_count', 'set_2_syn_count', etc.\n",
    "cols_merge_syn = [\n",
    "    'set_1_syn_count', 'set_2_syn_count', 'set_3_syn_count'\n",
    "]\n",
    "all_2N = all_2N.assign(\n",
    "    total_syn_count=all_2N[cols_merge_syn].sum(axis=1)\n",
    ").drop(cols_merge_syn, axis=1)\n",
    "\n",
    "# Similarly for the upstream columns, if you named them 'upstream_set_1_aPhN2_SAs', etc.\n",
    "cols_upstream = [\n",
    "    'upstream_set_1_aPhN2_SAs', 'upstream_set_2_aPhN2_SAs', 'upstream_set_3_aPhN2_SAs'\n",
    "]\n",
    "all_2N = all_2N.assign(\n",
    "    total_upstream=all_2N[cols_upstream].sum(axis=1)\n",
    ").drop(cols_upstream, axis=1)\n",
    "\n",
    "# all_2N now contains a combined table of second-order neurons with:\n",
    "#   - Activation status at each rate, for each set\n",
    "#   - Summed total syn_count across all sets\n",
    "#   - Summed total of upstream sets\n",
    "#   - A 'label' column that indicates which set the neuron was originally associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e624aa83a5295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:02.643528Z",
     "start_time": "2025-04-24T23:34:02.631628Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the new data frame\n",
    "all_2N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5339405e55c7c30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:02.828383Z",
     "start_time": "2025-04-24T23:34:02.825269Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3e91bc3029ad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:05.606484Z",
     "start_time": "2025-04-24T23:34:05.604466Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c5945071bcaea7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:05.939478Z",
     "start_time": "2025-04-24T23:34:05.903513Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, get classifications for 2Ns\n",
    "# This uses the \"classification_other\" dataset for the first time, which has classes/superclasses for all neurons\n",
    "\n",
    "# get classifications for 2Ns\n",
    "# For each set, merge the unique 2N root_ids with the classification_other dataset.\n",
    "set_1_2Ns_classified = pd.merge(pd.DataFrame({'root_id': set_1_2Ns.root_id.unique()}), classification_other,\n",
    "                                on='root_id')\n",
    "set_2_2Ns_classified = pd.merge(pd.DataFrame({'root_id': set_2_2Ns.root_id.unique()}), classification_other,\n",
    "                                on='root_id')\n",
    "set_3_2Ns_classified = pd.merge(pd.DataFrame({'root_id': set_3_2Ns.root_id.unique()}), classification_other,\n",
    "                                on='root_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3213ba1bf5bd8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:06.060406Z",
     "start_time": "2025-04-24T23:34:06.042394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get unique superclass categories from the classification_other dataset\n",
    "categories = classification_other.super_class.dropna().unique()\n",
    "print(\"Superclasses found:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517b1d5907e3b9d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:08.444807Z",
     "start_time": "2025-04-24T23:34:08.441533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reorder the superclasses in the desired order.\n",
    "categories = ['sensory', 'ascending', 'central',\n",
    "              'descending', 'motor', 'endocrine',\n",
    "              'optic', 'visual_projection', 'visual_centrifugal']\n",
    "print(\"Ordered superclasses:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7953dee56e163",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:08.840752Z",
     "start_time": "2025-04-24T23:34:08.592838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot graph of superclasses for 2Ns\n",
    "\n",
    "# We assume you have set_1_2Ns_classified, set_2_2Ns_classified, etc.\n",
    "# which contain a 'super_class' column from classification_other.\n",
    "\n",
    "datasets = [\n",
    "    set_1_2Ns_classified,\n",
    "    set_2_2Ns_classified,\n",
    "    set_3_2Ns_classified,\n",
    "]\n",
    "\n",
    "# Count superclasses for each dataset\n",
    "# .dropna() avoids counts on NaN\n",
    "datasets_counts = [\n",
    "    ds['super_class'].dropna().value_counts() for ds in datasets\n",
    "]\n",
    "\n",
    "# Suppose we have 9 superclasses in a desired order\n",
    "categories = [\n",
    "    'sensory',\n",
    "    'ascending',\n",
    "    'central',\n",
    "    'descending',\n",
    "    'motor',\n",
    "    'endocrine',\n",
    "    'optic',\n",
    "    'visual_projection',\n",
    "    'visual_centrifugal'\n",
    "]\n",
    "\n",
    "# array_plot will hold one array per superclass\n",
    "# each array has length = number of sets (6), representing the count in that set\n",
    "array_plot = []\n",
    "for superclass in categories:\n",
    "    row_counts = []\n",
    "    for ds_count in datasets_counts:\n",
    "        # If the superclass is not found in a given set, we push 0\n",
    "        row_counts.append(ds_count.get(superclass, 0))\n",
    "    array_plot.append(np.array(row_counts))\n",
    "\n",
    "# array_plot is now a list of 9 np.arrays, each of length 6.\n",
    "# Convert it to a NumPy array of shape (9, 6) if convenient\n",
    "array_plot = np.array(array_plot)\n",
    "\n",
    "# Set labels for the sets, e.g. \"Set 1\", \"Set 2\", ...\n",
    "set_labels = [\"Set 1\", \"Set 2\", \"Set 3\"]\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# We'll create 6 bars (one per set), each stacked with 9 layers (one per superclass).\n",
    "bar_positions = np.arange(len(set_labels)) + 1  # e.g. [1,2,3,4,5,6]\n",
    "bar_width = 0.6\n",
    "\n",
    "# Define a color palette for the 9 superclasses\n",
    "palette = [\n",
    "    '#dc143c',    # sensory\n",
    "    '#ffa500',    # ascending\n",
    "    'green',      # central\n",
    "    '#069af3',    # descending\n",
    "    '#0000ff',    # motor\n",
    "    '#9a0eea',    # endocrine\n",
    "    '#c79fef',    # optic\n",
    "    '#ffc0cb',    # visual_projection\n",
    "    '#ff81c0'     # visual_centrifugal\n",
    "]\n",
    "\n",
    "# The first row of array_plot will be plotted directly;\n",
    "# subsequent rows get stacked on top.\n",
    "ax1 = ax.bar(\n",
    "    bar_positions,\n",
    "    array_plot[0],  # counts for first superclass\n",
    "    width=bar_width,\n",
    "    color=palette[0]\n",
    ")\n",
    "stack_totals = array_plot[0].copy()  # track the cumulative heights\n",
    "\n",
    "# For each of the remaining superclasses (rows 1..8)\n",
    "for i in range(1, len(categories)):\n",
    "    ax_i = ax.bar(\n",
    "        bar_positions,\n",
    "        array_plot[i],\n",
    "        bottom=stack_totals,\n",
    "        width=bar_width,\n",
    "        color=palette[i]\n",
    "    )\n",
    "    stack_totals += array_plot[i]\n",
    "\n",
    "ax.set_title('Superclasses of 2Ns', fontsize=16)\n",
    "ax.set_ylabel('# 2Ns', fontsize=16)\n",
    "\n",
    "# Create a legend with the categories\n",
    "ax.legend(\n",
    "    categories,\n",
    "    loc='upper left',\n",
    "    bbox_to_anchor=(1, 1.1),\n",
    "    frameon=False,\n",
    "    fontsize=13\n",
    ")\n",
    "\n",
    "plt.xticks(bar_positions, set_labels, fontsize=7)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Optionally adjust y-limit if needed\n",
    "plt.ylim(0, np.max(stack_totals)*1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc657eb0052b9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:10.632516Z",
     "start_time": "2025-04-24T23:34:10.628167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert raw counts to proportions (column-wise)\n",
    "# array_plot has shape (9,6) => 9 superclasses, 6 sets\n",
    "col_sums = array_plot.sum(axis=0)  # sum over rows => shape (6,)\n",
    "array_plot_prop = (array_plot / col_sums) * 100.0  # percentage\n",
    "\n",
    "print(\"Proportions (as %):\")\n",
    "print(array_plot_prop)\n",
    "\n",
    "# Then you can plot array_plot_prop the same way you plotted array_plot,\n",
    "# just changing the bar heights and the y-axis label to \"% 2Ns\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c25b2029babd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:10.748991Z",
     "start_time": "2025-04-24T23:34:10.743802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Once you have array_plot (counts) or array_plot_prop (percentages),\n",
    "# you can look at them directly:\n",
    "\n",
    "print(\"array_plot (raw counts)\\n\", array_plot)\n",
    "print(\"\\narray_plot_prop (percent):\\n\", array_plot_prop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990358fb9e27a432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:12.686820Z",
     "start_time": "2025-04-24T23:34:12.682611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get values for proportion of each type\n",
    "array_plot/np.sum(array_plot, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ca44f7385491a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:12.873598Z",
     "start_time": "2025-04-24T23:34:12.857549Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: using your already computed array_plot and set_labels, and the ordered categories\n",
    "categories = ['sensory', 'ascending', 'central', 'descending', 'motor',\n",
    "              'endocrine', 'optic', 'visual_projection', 'visual_centrifugal']\n",
    "set_labels = [\"Set 1\", \"Set 2\", \"Set 3\"]\n",
    "\n",
    "# Create a DataFrame with rows as superclasses and columns as datasets\n",
    "df_counts = pd.DataFrame(array_plot, index=categories, columns=set_labels)\n",
    "\n",
    "# (Optional) Reset index so that the superclass names become a column in the CSV\n",
    "df_counts = df_counts.reset_index().rename(columns={'index': 'super_class'})\n",
    "\n",
    "# Save to CSV file\n",
    "df_counts.to_csv(\"datasets_counts.csv\", index=False)\n",
    "print(\"CSV table saved as 'datasets_counts.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50a1d565135b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:17.236956Z",
     "start_time": "2025-04-24T23:34:17.214702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge set_1_outputs with classification_other to add the super_class for each output neuron.\n",
    "# (Assumes set_1_outputs has a column 'post_root_id' and classification_other has 'root_id' and 'super_class')\n",
    "set_1_outputs_with_class = pd.merge(\n",
    "    set_1_outputs,\n",
    "    classification_other[['root_id', 'super_class']],\n",
    "    left_on='post_root_id',\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the extra 'root_id' column that came from the merge\n",
    "set_1_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# (Optional) Rename the super_class column for clarity\n",
    "set_1_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# Save the result as a CSV file\n",
    "set_1_outputs_with_class.to_csv(\"set_1_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_1_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70cec1fba1e80a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:17.688986Z",
     "start_time": "2025-04-24T23:34:17.372001Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Use the original table from Option 1 and treat its post IDs as new pre IDs.\n",
    "# ------------------------------------------------------------------------------\n",
    "# We assume 'set_1_outputs' is the DataFrame from Option 1 with columns:\n",
    "#    pre_root_id, post_root_id, neuropil, syn_count, nt_type, location_of_connection, etc.\n",
    "#\n",
    "# Create a DataFrame of unique neurons that were outputs (i.e. post ids) in the first round:\n",
    "new_pre_neurons = pd.DataFrame({'root_id': set_1_outputs['post_root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Get the downstream outputs for these new pre neurons.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Use the neuronal_outputs function (which expects a DataFrame with a column 'root_id')\n",
    "# to find the downstream connections for these neurons.\n",
    "new_outputs = neuronal_outputs(new_pre_neurons)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with classification info so that each connection includes\n",
    "# the super_class of the output (i.e. new post) neuron.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class = pd.merge(\n",
    "    new_outputs,\n",
    "    classification_other[['root_id', 'super_class']],  # classification table for output neurons\n",
    "    left_on='post_root_id',  # new outputs: these are the downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'root_id' column that came from the merge\n",
    "new_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# Rename the column for clarity\n",
    "new_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as CSV.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class.to_csv(\"set_1_hop_1_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_1_hop_1_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5700be1aafa6c20",
   "metadata": {},
   "source": [
    "i now want to make a sankey plot of this data where we see on axis 1 set 1 axis 2 the super classes and axis 3 as super classes. for context here is a head of one of the csvs (pre_root_id\tpost_root_id\tneuropil\tsyn_count\tnt_type\tlocation_of_connection\toutput_super_class\n",
    "720575940617034713\t720575940628071211\tPRW\t5\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940633548128\tFLA_R\t7\tSER\tlocal\tascending\n",
    "720575940617034713\t720575940621662332\tFLA_R\t8\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940630672938\tFLA_R\t8\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940630672938\tPRW\t16\tACH\tlocal\tcentral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe4e6a63477c710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:19.890561Z",
     "start_time": "2025-04-24T23:34:19.554419Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Load the previous CSV file and extract the unique post ids\n",
    "# ------------------------------------------------------------------------------\n",
    "prev_outputs = pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\")\n",
    "# Here, the post ids from the previous file will now serve as the pre ids.\n",
    "new_pre_neurons_2 = pd.DataFrame({'root_id': prev_outputs['post_root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Find downstream connections for these new pre neurons using your neuronal_outputs function\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2 = neuronal_outputs(new_pre_neurons_2)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with the classification information to add super_class for each new downstream neuron\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2_with_class = pd.merge(\n",
    "    new_outputs_2,\n",
    "    classification_other[['root_id', 'super_class']],\n",
    "    left_on='post_root_id',  # these are the new downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the extra 'root_id' column that came from the merge\n",
    "new_outputs_2_with_class.drop(columns=['root_id'], inplace=True)\n",
    "# Rename the super_class column for clarity\n",
    "new_outputs_2_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as a CSV file\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2_with_class.to_csv(\"set_1_hop_2_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_1_hop_2_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc97e54f43d09c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:22.021343Z",
     "start_time": "2025-04-24T23:34:21.767020Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Create a DataFrame of unique neurons from the set_1 2Ns.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Here, we assume that \"set_1_2Ns\" is your DataFrame of second-order neurons from set 1,\n",
    "# which was generated earlier in your pipeline.\n",
    "new_pre_neurons = pd.DataFrame({'root_id': set_1_2Ns['root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Get the downstream outputs for these new pre neurons (i.e. the 3NS).\n",
    "# ------------------------------------------------------------------------------\n",
    "# Use the neuronal_outputs function (which expects a DataFrame with a 'root_id' column)\n",
    "# to extract all downstream connections for these neurons.\n",
    "new_outputs = neuronal_outputs(new_pre_neurons)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with classification information so that each connection includes\n",
    "# the super_class of the output (i.e. new post) neuron.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class = pd.merge(\n",
    "    new_outputs,\n",
    "    classification_other[['root_id', 'super_class']],  # Classification table for output neurons\n",
    "    left_on='post_root_id',  # New outputs: these are the downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'root_id' column that came from the merge\n",
    "new_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# Rename the column for clarity\n",
    "new_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as CSV.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class.to_csv(\"set_1_hop_3_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_1_hop_3_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b907d2cfaedce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:24.438717Z",
     "start_time": "2025-04-24T23:34:24.075059Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the first round data: set 1 outputs (first-order)\n",
    "df1 = pd.read_csv(\"set_1_opt_conns_superclass.csv\")\n",
    "# Load the second round data: outputs where we treat the first round outputs as new pre neurons\n",
    "df2 = pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\")\n",
    "\n",
    "# --- Build flows from \"Set 1\" to first-level super classes ---\n",
    "# Group df1 by the first-level output super_class and count connections\n",
    "flow1 = df1.groupby('output_super_class').size().reset_index(name='count')\n",
    "\n",
    "# --- Build flows from first-level to second-level super classes ---\n",
    "# Merge the two dataframes by linking the first round's post_root_id (the output neuron)\n",
    "# with the second round's pre_root_id (the same neuron now acting as input)\n",
    "merged = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id', suffixes=('_first', '_second'))\n",
    "\n",
    "# Group by first-level and second-level super classes and count connections\n",
    "flow2 = merged.groupby(['output_super_class_first', 'output_super_class_second']).size().reset_index(name='count')\n",
    "\n",
    "# --- Define Sankey nodes ---\n",
    "# In this diagram, we want three columns:\n",
    "# Column 1: \"Set 1\" (a single node)\n",
    "# Column 2: Unique first-level super classes from df1\n",
    "# Column 3: Unique second-level super classes from the merged table\n",
    "nodes = []\n",
    "nodes.append(\"Set 1\")  # Column 1\n",
    "\n",
    "first_super_nodes = flow1['output_super_class'].unique().tolist()\n",
    "nodes.extend(first_super_nodes)  # Column 2\n",
    "\n",
    "second_super_nodes = flow2['output_super_class_second'].unique().tolist()\n",
    "nodes.extend(second_super_nodes)  # Column 3\n",
    "\n",
    "# Create a mapping from node label to node index for building links\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# --- Create Sankey link data ---\n",
    "\n",
    "# Flow 1: from \"Set 1\" (source) to each first-level super_class node\n",
    "source1 = []\n",
    "target1 = []\n",
    "value1 = []\n",
    "for _, row in flow1.iterrows():\n",
    "    source1.append(node_index[\"Set 1\"])  # all originate from \"Set 1\"\n",
    "    target1.append(node_index[row['output_super_class']])\n",
    "    value1.append(row['count'])\n",
    "\n",
    "# Flow 2: from first-level super class to second-level super class\n",
    "source2 = []\n",
    "target2 = []\n",
    "value2 = []\n",
    "for _, row in flow2.iterrows():\n",
    "    source2.append(node_index[row['output_super_class_first']])\n",
    "    target2.append(node_index[row['output_super_class_second']])\n",
    "    value2.append(row['count'])\n",
    "\n",
    "# Combine flows from both steps\n",
    "source = source1 + source2\n",
    "target = target1 + target2\n",
    "value  = value1  + value2\n",
    "\n",
    "# --- Build and display the Sankey diagram ---\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),\n",
    "        label = nodes,\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = source,\n",
    "        target = target,\n",
    "        value = value\n",
    "    ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram: Set 1 to First and Second Level Super Classes\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e9bf6a8e5b2e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:26.781Z",
     "start_time": "2025-04-24T23:34:26.661212Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_1_opt_conns_superclass.csv\")\n",
    "df2 = pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\")\n",
    "df3 = pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows using vectorized operations\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 1\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class').size().reset_index(name='count')\n",
    "flow1['source'] = \"Set 1\"  # all connections originate from \"Set 1\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "# Merge df1 and df2 using the connection where first round's post_root_id becomes df2's pre_root_id.\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second']).size().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "# Merge df2 and df3 similarly (df2.post_root_id becomes df3.pre_root_id).\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third']).size().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Define nodes for the Sankey diagram\n",
    "# ---------------------------\n",
    "# We define four groups:\n",
    "# Column 1: \"Set 1\" (a single node)\n",
    "# Column 2: Unique first-level super classes from df1\n",
    "# Column 3: Unique second-level super classes from the merged flows (from flow2 and flow3)\n",
    "# Column 4: Unique third-level super classes from flow3\n",
    "nodes = []\n",
    "nodes.append(\"Set 1\")\n",
    "first_nodes = flow1['output_super_class'].unique().tolist()\n",
    "nodes.extend(first_nodes)\n",
    "second_nodes = pd.concat([flow2['output_super_class_second'], flow3['output_super_class_second']]).unique().tolist()\n",
    "nodes.extend(second_nodes)\n",
    "third_nodes = flow3['output_super_class_third'].unique().tolist()\n",
    "nodes.extend(third_nodes)\n",
    "\n",
    "# Create a mapping from node label to node index\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Vectorized mapping to get link indices\n",
    "# ---------------------------\n",
    "flow1['source_idx'] = node_index[\"Set 1\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].map(node_index)\n",
    "\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].map(node_index)\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].map(node_index)\n",
    "\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].map(node_index)\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].map(node_index)\n",
    "\n",
    "# Combine flows into link lists for the Sankey diagram\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute incoming and outgoing values for each node\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    total = max(incoming, outgoing)\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Determine hop counts for a target super class (e.g., \"motor\")\n",
    "# ---------------------------\n",
    "target_class = \"motor\"\n",
    "hop1_motor = flow1[flow1['output_super_class'] == target_class]['count'].sum()\n",
    "hop2_motor = flow2[flow2['output_super_class_second'] == target_class]['count'].sum()\n",
    "hop3_motor = flow3[flow3['output_super_class_third'] == target_class]['count'].sum()\n",
    "\n",
    "print(f\"Connections reaching '{target_class}' neurons at 1 hop (direct):\", hop1_motor)\n",
    "print(f\"Connections reaching '{target_class}' neurons at 2 hops:\", hop2_motor)\n",
    "print(f\"Connections reaching '{target_class}' neurons at 3 hops:\", hop3_motor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ae74bc30d5905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:28.755411Z",
     "start_time": "2025-04-24T23:34:28.752569Z"
    }
   },
   "outputs": [],
   "source": [
    "#fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9cca15c356397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:28.958913Z",
     "start_time": "2025-04-24T23:34:28.845106Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files and filter out weak connections (< 5 synapses)\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_1_opt_conns_superclass.csv\")\n",
    "df1 = df1[df1['syn_count'] >= 5]  # only keep connections with at least 5 synapses\n",
    "\n",
    "df2 = pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\")\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\")\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows (using summed synapse counts)\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 1\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class')['syn_count'].sum().reset_index(name='count')\n",
    "flow1['source'] = \"Set 1\"  # all connections originate from \"Set 1\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "# Sum the synapse counts from df2 (now in 'syn_count_second') for each pair.\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second'])['syn_count_second'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "# Sum the synapse counts from df3 (now in 'syn_count_third') for each pair.\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third'])['syn_count_third'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Build node lists directly from the data sources for each hop.\n",
    "# This ensures that nodes appear in the column corresponding to their data source.\n",
    "# ---------------------------\n",
    "col1 = [\"Set 1\"]  # Column 1: the source\n",
    "col2 = [\"1: \" + label for label in sorted(df1['output_super_class'].unique())]  # Column 2 from df1\n",
    "col3 = [\"2: \" + label for label in sorted(df2['output_super_class'].unique())]  # Column 3 from df2\n",
    "col4 = [\"3: \" + label for label in sorted(df3['output_super_class'].unique())]  # Column 4 from df3\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Update flows to use the new, prefixed node labels.\n",
    "# ---------------------------\n",
    "# Flow 1: Source is \"Set 1\"; target from df1 gets prefix \"1: \"\n",
    "flow1['source_idx'] = node_index[\"Set 1\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].apply(lambda x: node_index[\"1: \" + x])\n",
    "\n",
    "# Flow 2: Source is from df1 (first hop, prefix \"1: \") and target is from df2 (second hop, prefix \"2: \")\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].apply(lambda x: node_index[\"1: \" + x])\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "\n",
    "# Flow 3: Source is from df2 (second hop, prefix \"2: \") and target is from df3 (third hop, prefix \"3: \")\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].apply(lambda x: node_index[\"3: \" + x])\n",
    "\n",
    "# Combine flows into link lists.\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute custom hover info for nodes.\n",
    "# For each node, we sum incoming and outgoing flows based on the link lists.\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    # For nodes that both receive and send flows, Plotly by default uses max(incoming, outgoing)\n",
    "    # Here we display both values and the \"total\" (the max of the two)\n",
    "    total = incoming if incoming >= outgoing else outgoing\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define fixed x positions so that the diagram is laid out in a linear, successive fashion.\n",
    "# Column 1 (\"Set 1\") at x=0.0, Column 2 (first-level nodes) at x=0.33,\n",
    "# Column 3 (second-level nodes) at x=0.66, Column 4 (third-level nodes) at x=1.0.\n",
    "# ---------------------------\n",
    "x_positions = []\n",
    "x_positions += [0.0] * len(col1)\n",
    "x_positions += [0.33] * len(col2)\n",
    "x_positions += [0.66] * len(col3)\n",
    "x_positions += [1.0] * len(col4)\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text for nodes.\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement=\"freeform\",  # you can use \"fixed\" or \"freeform\" as desired\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    "fig.update_layout(title_text=\"Linear Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feba041f12b916e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:31.087370Z",
     "start_time": "2025-04-24T23:34:30.953689Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import colorsys\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions for colors\n",
    "# ---------------------------\n",
    "def get_color(i, n):\n",
    "    \"\"\"Generate a distinct RGB color for node index i among n nodes.\"\"\"\n",
    "    hue = i / n  # evenly spaced hue\n",
    "    # Using moderate saturation and high brightness for vivid colors\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, 0.6, 0.9)\n",
    "    r, g, b = int(r * 255), int(g * 255), int(b * 255)\n",
    "    return f\"rgb({r},{g},{b})\"\n",
    "\n",
    "def make_rgba(rgb_str, alpha=0.5):\n",
    "    \"\"\"Convert an rgb string (e.g., 'rgb(31,119,180)') to an rgba string with given alpha.\"\"\"\n",
    "    rgb_values = rgb_str.strip(\"rgb(\").strip(\")\").split(\",\")\n",
    "    return f\"rgba({rgb_values[0].strip()},{rgb_values[1].strip()},{rgb_values[2].strip()},{alpha})\"\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files and filter out weak connections (< 5 synapses)\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_1_opt_conns_superclass.csv\")\n",
    "df1 = df1[df1['syn_count'] >= 5]  # only keep connections with at least 5 synapses\n",
    "\n",
    "df2 = pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\")\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\")\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows (using summed synapse counts)\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 1\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class')['syn_count'].sum().reset_index(name='count')\n",
    "flow1['source'] = \"Set 1\"  # all connections originate from \"Set 1\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second'])['syn_count_second'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third'])['syn_count_third'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Build node lists directly from the data sources for each hop.\n",
    "# ---------------------------\n",
    "# Define columns:\n",
    "# Column 1: \"Set 1\"\n",
    "# Column 2: Unique first-level super classes from df1 (prefixed for clarity)\n",
    "# Column 3: Unique second-level super classes from df2 (prefixed)\n",
    "# Column 4: Unique third-level super classes from df3 (prefixed)\n",
    "col1 = [\"Set 1\"]\n",
    "col2 = [\"1: \" + label for label in sorted(df1['output_super_class'].unique())]\n",
    "col3 = [\"2: \" + label for label in sorted(df2['output_super_class'].unique())]\n",
    "col4 = [\"3: \" + label for label in sorted(df3['output_super_class'].unique())]\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Assign each node a unique color.\n",
    "# ---------------------------\n",
    "n_nodes = len(nodes)\n",
    "node_colors = [get_color(i, n_nodes) for i in range(n_nodes)]\n",
    "\n",
    "# ---------------------------\n",
    "# Update flows to use the new, prefixed node labels.\n",
    "# ---------------------------\n",
    "# Flow 1: Source is \"Set 1\"; target from df1 gets prefix \"1: \"\n",
    "flow1['source_idx'] = node_index[\"Set 1\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].apply(lambda x: node_index[\"1: \" + x])\n",
    "\n",
    "# Flow 2: Source is from df1 (first hop, prefix \"1: \") and target is from df2 (prefix \"2: \")\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].apply(lambda x: node_index[\"1: \" + x])\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "\n",
    "# Flow 3: Source is from df2 (prefix \"2: \") and target is from df3 (prefix \"3: \")\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].apply(lambda x: node_index[\"3: \" + x])\n",
    "\n",
    "# Combine flows into link lists.\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute custom hover info for nodes.\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    total = incoming if incoming >= outgoing else outgoing\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build link colors based on the source node's unique color (with 50% opacity).\n",
    "# ---------------------------\n",
    "link_colors = []\n",
    "for s in source:\n",
    "    base_color = node_colors[s]  # use the source node's color\n",
    "    link_colors.append(make_rgba(base_color, 0.5))\n",
    "\n",
    "# ---------------------------\n",
    "# Define fixed x positions so that the diagram is laid out in a linear, successive fashion.\n",
    "# ---------------------------\n",
    "x_positions = []\n",
    "x_positions += [0.0] * len(col1)\n",
    "x_positions += [0.33] * len(col2)\n",
    "x_positions += [0.66] * len(col3)\n",
    "x_positions += [1.0] * len(col4)\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text and link colors.\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement=\"freeform\",\n",
    "    node=dict(\n",
    "        pad=80,\n",
    "        thickness=50,\n",
    "        line=dict(color=\"black\", width=0.9),\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        color=node_colors,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=link_colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Linear Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91891a1fe8053c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:33.064019Z",
     "start_time": "2025-04-24T23:34:33.056295Z"
    }
   },
   "outputs": [],
   "source": [
    "flow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79f3923180b085f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:35.017928Z",
     "start_time": "2025-04-24T23:34:34.946438Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load connectivity CSV files\n",
    "df1 = pd.read_csv(\"set_1_opt_conns_superclass.csv\")\n",
    "df2 = pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\")\n",
    "df3 = pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "target_class = \"motor\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1-hop: Direct connections from Set 1 to a motor neuron.\n",
    "# ---------------------------\n",
    "df1_motor = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "df1_motor = df1_motor.assign(hop=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 2-hop: Chain from Set 1  Neuron2  Neuron3 where the second hop ends in a motor neuron.\n",
    "# ---------------------------\n",
    "df12 = pd.merge(\n",
    "    df1[['pre_root_id', 'post_root_id']],\n",
    "    df2,\n",
    "    left_on='post_root_id',\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df1', '_df2')\n",
    ")\n",
    "# In df12, the starting neuron from Set 1 is now in 'pre_root_id' from df1,\n",
    "# which has been renamed automatically to 'pre_root_id' (or to 'pre_root_id_df1' if needed).\n",
    "# Let's check for our purposes:\n",
    "if 'pre_root_id_df1' in df12.columns:\n",
    "    start_col = 'pre_root_id_df1'\n",
    "else:\n",
    "    start_col = 'pre_root_id'\n",
    "\n",
    "df12_motor = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "df12_motor = df12_motor.rename(columns={start_col: 'pre_root_id'})\n",
    "df12_motor = df12_motor.assign(hop=2)\n",
    "\n",
    "# ---------------------------\n",
    "# 3-hop: Chain from Set 1  Neuron2  Neuron3  Neuron4 where the third hop ends in a motor neuron.\n",
    "# ---------------------------\n",
    "df123 = pd.merge(\n",
    "    df12,\n",
    "    df3,\n",
    "    left_on='post_root_id_df2',  # the output from the second leg from df2\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df2', '_df3')\n",
    ")\n",
    "# After merging, the df3 column 'output_super_class' becomes 'output_super_class_df3'\n",
    "df123_motor = df123[df123['output_super_class_df3'] == target_class][[start_col]].drop_duplicates()\n",
    "df123_motor = df123_motor.rename(columns={start_col: 'pre_root_id'})\n",
    "df123_motor = df123_motor.assign(hop=3)\n",
    "\n",
    "# ---------------------------\n",
    "# Combine and determine the minimum hop count per starting neuron.\n",
    "# ---------------------------\n",
    "df_hops = pd.concat([df1_motor, df12_motor, df123_motor], ignore_index=True)\n",
    "min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "\n",
    "print(min_hops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e11aefb12150c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:38.636107Z",
     "start_time": "2025-04-24T23:34:38.627481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for the hops\n",
    "mean_hops = min_hops['hop'].mean()\n",
    "median_hops = min_hops['hop'].median()\n",
    "std_hops = min_hops['hop'].std()\n",
    "min_hop = min_hops['hop'].min()\n",
    "max_hop = min_hops['hop'].max()\n",
    "count = min_hops['hop'].count()\n",
    "\n",
    "print(\"Descriptive statistics for hops to reach a motor neuron:\")\n",
    "print(f\"Number of neurons: {count}\")\n",
    "print(f\"Mean hops: {mean_hops:.2f}\")\n",
    "print(f\"Median hops: {median_hops}\")\n",
    "print(f\"Standard Deviation: {std_hops:.2f}\")\n",
    "print(f\"Minimum hops: {min_hop}\")\n",
    "print(f\"Maximum hops: {max_hop}\")\n",
    "\n",
    "# Alternatively, you can use describe() to get a summary:\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(min_hops['hop'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79912b9d919203b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:39.603718Z",
     "start_time": "2025-04-24T23:34:38.848668Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Ensure 'hop' is numeric and get max_hop as a Python int.\n",
    "min_hops['hop'] = pd.to_numeric(min_hops['hop'])\n",
    "max_hop = int(min_hops['hop'].max())\n",
    "\n",
    "# --- Histogram ---\n",
    "# Set nbins to max_hop + 1 so that each integer hop has its own bin.\n",
    "fig_hist = px.histogram(\n",
    "    min_hops,\n",
    "    x=\"hop\",\n",
    "    nbins=max_hop + 1,\n",
    "    title=\"Distribution of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\", \"count\": \"Number of Neurons\"},\n",
    "    range_x=[0.5, max_hop + 0.5]\n",
    ")\n",
    "fig_hist.update_xaxes(dtick=1)\n",
    "fig_hist.show()\n",
    "\n",
    "# --- Box Plot ---\n",
    "fig_box = px.box(\n",
    "    min_hops,\n",
    "    y=\"hop\",\n",
    "    title=\"Box Plot of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_box.show()\n",
    "\n",
    "# --- Violin Plot ---\n",
    "fig_violin = px.violin(\n",
    "    min_hops,\n",
    "    y=\"hop\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Violin Plot of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_violin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42acfedc92f647",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:42.167572Z",
     "start_time": "2025-04-24T23:34:42.165022Z"
    }
   },
   "outputs": [],
   "source": [
    "##fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b806e69de6e090ce",
   "metadata": {},
   "source": [
    "### Plot proportions of ascending and descending neurons\n",
    "\n",
    "This figure is not included in the paper as it is redundant with the superclass figure above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539e2695a3fe5f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:43.956598Z",
     "start_time": "2025-04-24T23:34:43.953911Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74441fb76b9aa05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:44.084761Z",
     "start_time": "2025-04-24T23:34:44.082765Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3036c5b8c1e5eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:45.941234Z",
     "start_time": "2025-04-24T23:34:45.911450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge set_2_outputs with classification_other to add the super_class for each output neuron.\n",
    "# (Assumes set_2_outputs has a column 'post_root_id' and classification_other has 'root_id' and 'super_class')\n",
    "set_2_outputs_with_class = pd.merge(\n",
    "    set_2_outputs,\n",
    "    classification_other[['root_id', 'super_class']],\n",
    "    left_on='post_root_id',\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the extra 'root_id' column that came from the merge\n",
    "set_2_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# (Optional) Rename the super_class column for clarity\n",
    "set_2_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# Save the result as a CSV file\n",
    "set_2_outputs_with_class.to_csv(\"set_2_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_2_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad472ffe4ac08a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:48.240174Z",
     "start_time": "2025-04-24T23:34:47.855029Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Use the original table from Option 1 and treat its post IDs as new pre IDs.\n",
    "# ------------------------------------------------------------------------------\n",
    "# We assume 'set_2_outputs' is the DataFrame from Option 1 with columns:\n",
    "#    pre_root_id, post_root_id, neuropil, syn_count, nt_type, location_of_connection, etc.\n",
    "#\n",
    "# Create a DataFrame of unique neurons that were outputs (i.e. post ids) in the first round:\n",
    "new_pre_neurons = pd.DataFrame({'root_id': set_2_outputs['post_root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Get the downstream outputs for these new pre neurons.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Use the neuronal_outputs function (which expects a DataFrame with a column 'root_id')\n",
    "# to find the downstream connections for these neurons.\n",
    "new_outputs = neuronal_outputs(new_pre_neurons)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with classification info so that each connection includes\n",
    "# the super_class of the output (i.e. new post) neuron.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class = pd.merge(\n",
    "    new_outputs,\n",
    "    classification_other[['root_id', 'super_class']],  # classification table for output neurons\n",
    "    left_on='post_root_id',  # new outputs: these are the downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'root_id' column that came from the merge\n",
    "new_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# Rename the column for clarity\n",
    "new_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as CSV.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class.to_csv(\"set_2_hop_1_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_2_hop_1_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd3d941498e2ea4",
   "metadata": {},
   "source": [
    "i now want to make a sankey plot of this data where we see on axis 1 Set 2 axis 2 the super classes and axis 3 as super classes. for context here is a head of one of the csvs (pre_root_id\tpost_root_id\tneuropil\tsyn_count\tnt_type\tlocation_of_connection\toutput_super_class\n",
    "720575940617034713\t720575940628071211\tPRW\t5\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940633548128\tFLA_R\t7\tSER\tlocal\tascending\n",
    "720575940617034713\t720575940621662332\tFLA_R\t8\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940630672938\tFLA_R\t8\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940630672938\tPRW\t16\tACH\tlocal\tcentral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a49da79600fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:50.471765Z",
     "start_time": "2025-04-24T23:34:50.050754Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Load the previous CSV file and extract the unique post ids\n",
    "# ------------------------------------------------------------------------------\n",
    "prev_outputs = pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\")\n",
    "# Here, the post ids from the previous file will now serve as the pre ids.\n",
    "new_pre_neurons_2 = pd.DataFrame({'root_id': prev_outputs['post_root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Find downstream connections for these new pre neurons using your neuronal_outputs function\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2 = neuronal_outputs(new_pre_neurons_2)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with the classification information to add super_class for each new downstream neuron\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2_with_class = pd.merge(\n",
    "    new_outputs_2,\n",
    "    classification_other[['root_id', 'super_class']],\n",
    "    left_on='post_root_id',  # these are the new downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the extra 'root_id' column that came from the merge\n",
    "new_outputs_2_with_class.drop(columns=['root_id'], inplace=True)\n",
    "# Rename the super_class column for clarity\n",
    "new_outputs_2_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as a CSV file\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2_with_class.to_csv(\"set_2_hop_2_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_2_hop_2_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ea79178220dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:52.424778Z",
     "start_time": "2025-04-24T23:34:52.162344Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Create a DataFrame of unique neurons from the set_2 2Ns.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Here, we assume that \"set_2_2Ns\" is your DataFrame of second-order neurons from Set 2,\n",
    "# which was generated earlier in your pipeline.\n",
    "new_pre_neurons = pd.DataFrame({'root_id': set_2_2Ns['root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Get the downstream outputs for these new pre neurons (i.e. the 3NS).\n",
    "# ------------------------------------------------------------------------------\n",
    "# Use the neuronal_outputs function (which expects a DataFrame with a 'root_id' column)\n",
    "# to extract all downstream connections for these neurons.\n",
    "new_outputs = neuronal_outputs(new_pre_neurons)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with classification information so that each connection includes\n",
    "# the super_class of the output (i.e. new post) neuron.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class = pd.merge(\n",
    "    new_outputs,\n",
    "    classification_other[['root_id', 'super_class']],  # Classification table for output neurons\n",
    "    left_on='post_root_id',  # New outputs: these are the downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'root_id' column that came from the merge\n",
    "new_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# Rename the column for clarity\n",
    "new_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as CSV.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class.to_csv(\"set_2_hop_3_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_2_hop_3_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50078f7fbbd9b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:54.377324Z",
     "start_time": "2025-04-24T23:34:54.285641Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the first round data: Set 2 outputs (first-order)\n",
    "df1 = pd.read_csv(\"set_2_opt_conns_superclass.csv\")\n",
    "# Load the second round data: outputs where we treat the first round outputs as new pre neurons\n",
    "df2 = pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\")\n",
    "\n",
    "# --- Build flows from \"Set 2\" to first-level super classes ---\n",
    "# Group df1 by the first-level output super_class and count connections\n",
    "flow1 = df1.groupby('output_super_class').size().reset_index(name='count')\n",
    "\n",
    "# --- Build flows from first-level to second-level super classes ---\n",
    "# Merge the two dataframes by linking the first round's post_root_id (the output neuron)\n",
    "# with the second round's pre_root_id (the same neuron now acting as input)\n",
    "merged = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id', suffixes=('_first', '_second'))\n",
    "\n",
    "# Group by first-level and second-level super classes and count connections\n",
    "flow2 = merged.groupby(['output_super_class_first', 'output_super_class_second']).size().reset_index(name='count')\n",
    "\n",
    "# --- Define Sankey nodes ---\n",
    "# In this diagram, we want three columns:\n",
    "# Column 1: \"Set 2\" (a single node)\n",
    "# Column 2: Unique first-level super classes from df1\n",
    "# Column 3: Unique second-level super classes from the merged table\n",
    "nodes = []\n",
    "nodes.append(\"Set 2\")  # Column 1\n",
    "\n",
    "first_super_nodes = flow1['output_super_class'].unique().tolist()\n",
    "nodes.extend(first_super_nodes)  # Column 2\n",
    "\n",
    "second_super_nodes = flow2['output_super_class_second'].unique().tolist()\n",
    "nodes.extend(second_super_nodes)  # Column 3\n",
    "\n",
    "# Create a mapping from node label to node index for building links\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# --- Create Sankey link data ---\n",
    "\n",
    "# Flow 1: from \"Set 2\" (source) to each first-level super_class node\n",
    "source1 = []\n",
    "target1 = []\n",
    "value1 = []\n",
    "for _, row in flow1.iterrows():\n",
    "    source1.append(node_index[\"Set 2\"])  # all originate from \"Set 2\"\n",
    "    target1.append(node_index[row['output_super_class']])\n",
    "    value1.append(row['count'])\n",
    "\n",
    "# Flow 2: from first-level super class to second-level super class\n",
    "source2 = []\n",
    "target2 = []\n",
    "value2 = []\n",
    "for _, row in flow2.iterrows():\n",
    "    source2.append(node_index[row['output_super_class_first']])\n",
    "    target2.append(node_index[row['output_super_class_second']])\n",
    "    value2.append(row['count'])\n",
    "\n",
    "# Combine flows from both steps\n",
    "source = source1 + source2\n",
    "target = target1 + target2\n",
    "value  = value1  + value2\n",
    "\n",
    "# --- Build and display the Sankey diagram ---\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),\n",
    "        label = nodes,\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = source,\n",
    "        target = target,\n",
    "        value = value\n",
    "    ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram: Set 2 to First and Second Level Super Classes\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095d374eb25bc18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:56.907677Z",
     "start_time": "2025-04-24T23:34:56.515891Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_2_opt_conns_superclass.csv\")\n",
    "df2 = pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\")\n",
    "df3 = pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows using vectorized operations\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 2\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class').size().reset_index(name='count')\n",
    "flow1['source'] = \"Set 2\"  # all connections originate from \"Set 2\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "# Merge df1 and df2 using the connection where first round's post_root_id becomes df2's pre_root_id.\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second']).size().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "# Merge df2 and df3 similarly (df2.post_root_id becomes df3.pre_root_id).\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third']).size().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Define nodes for the Sankey diagram\n",
    "# ---------------------------\n",
    "# We define four groups:\n",
    "# Column 1: \"Set 2\" (a single node)\n",
    "# Column 2: Unique first-level super classes from df1\n",
    "# Column 3: Unique second-level super classes from the merged flows (from flow2 and flow3)\n",
    "# Column 4: Unique third-level super classes from flow3\n",
    "nodes = []\n",
    "nodes.append(\"Set 2\")\n",
    "first_nodes = flow1['output_super_class'].unique().tolist()\n",
    "nodes.extend(first_nodes)\n",
    "second_nodes = pd.concat([flow2['output_super_class_second'], flow3['output_super_class_second']]).unique().tolist()\n",
    "nodes.extend(second_nodes)\n",
    "third_nodes = flow3['output_super_class_third'].unique().tolist()\n",
    "nodes.extend(third_nodes)\n",
    "\n",
    "# Create a mapping from node label to node index\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Vectorized mapping to get link indices\n",
    "# ---------------------------\n",
    "flow1['source_idx'] = node_index[\"Set 2\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].map(node_index)\n",
    "\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].map(node_index)\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].map(node_index)\n",
    "\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].map(node_index)\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].map(node_index)\n",
    "\n",
    "# Combine flows into link lists for the Sankey diagram\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute incoming and outgoing values for each node\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    total = max(incoming, outgoing)\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Determine hop counts for a target super class (e.g., \"motor\")\n",
    "# ---------------------------\n",
    "target_class = \"motor\"\n",
    "hop1_motor = flow1[flow1['output_super_class'] == target_class]['count'].sum()\n",
    "hop2_motor = flow2[flow2['output_super_class_second'] == target_class]['count'].sum()\n",
    "hop3_motor = flow3[flow3['output_super_class_third'] == target_class]['count'].sum()\n",
    "\n",
    "print(f\"Connections reaching '{target_class}' neurons at 1 hop (direct):\", hop1_motor)\n",
    "print(f\"Connections reaching '{target_class}' neurons at 2 hops:\", hop2_motor)\n",
    "print(f\"Connections reaching '{target_class}' neurons at 3 hops:\", hop3_motor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceca7f3cf48ab76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:34:58.673199Z",
     "start_time": "2025-04-24T23:34:58.670685Z"
    }
   },
   "outputs": [],
   "source": [
    "#fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ecb3b5335e85c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:00.886938Z",
     "start_time": "2025-04-24T23:35:00.525199Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files and filter out weak connections (< 5 synapses)\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_2_opt_conns_superclass.csv\")\n",
    "df1 = df1[df1['syn_count'] >= 5]  # only keep connections with at least 5 synapses\n",
    "\n",
    "df2 = pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\")\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\")\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows (using summed synapse counts)\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 2\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class')['syn_count'].sum().reset_index(name='count')\n",
    "flow1['source'] = \"Set 2\"  # all connections originate from \"Set 2\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "# Sum the synapse counts from df2 (now in 'syn_count_second') for each pair.\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second'])['syn_count_second'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "# Sum the synapse counts from df3 (now in 'syn_count_third') for each pair.\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third'])['syn_count_third'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Build node lists directly from the data sources for each hop.\n",
    "# This ensures that nodes appear in the column corresponding to their data source.\n",
    "# ---------------------------\n",
    "col1 = [\"Set 2\"]  # Column 1: the source\n",
    "col2 = [\"1: \" + label for label in sorted(df1['output_super_class'].unique())]  # Column 2 from df1\n",
    "col3 = [\"2: \" + label for label in sorted(df2['output_super_class'].unique())]  # Column 3 from df2\n",
    "col4 = [\"3: \" + label for label in sorted(df3['output_super_class'].unique())]  # Column 4 from df3\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Update flows to use the new, prefixed node labels.\n",
    "# ---------------------------\n",
    "# Flow 1: Source is \"Set 2\"; target from df1 gets prefix \"1: \"\n",
    "flow1['source_idx'] = node_index[\"Set 2\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].apply(lambda x: node_index[\"1: \" + x])\n",
    "\n",
    "# Flow 2: Source is from df1 (first hop, prefix \"1: \") and target is from df2 (second hop, prefix \"2: \")\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].apply(lambda x: node_index[\"1: \" + x])\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "\n",
    "# Flow 3: Source is from df2 (second hop, prefix \"2: \") and target is from df3 (third hop, prefix \"3: \")\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].apply(lambda x: node_index[\"3: \" + x])\n",
    "\n",
    "# Combine flows into link lists.\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute custom hover info for nodes.\n",
    "# For each node, we sum incoming and outgoing flows based on the link lists.\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    # For nodes that both receive and send flows, Plotly by default uses max(incoming, outgoing)\n",
    "    # Here we display both values and the \"total\" (the max of the two)\n",
    "    total = incoming if incoming >= outgoing else outgoing\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define fixed x positions so that the diagram is laid out in a linear, successive fashion.\n",
    "# Column 1 (\"Set 2\") at x=0.0, Column 2 (first-level nodes) at x=0.33,\n",
    "# Column 3 (second-level nodes) at x=0.66, Column 4 (third-level nodes) at x=1.0.\n",
    "# ---------------------------\n",
    "x_positions = []\n",
    "x_positions += [0.0] * len(col1)\n",
    "x_positions += [0.33] * len(col2)\n",
    "x_positions += [0.66] * len(col3)\n",
    "x_positions += [1.0] * len(col4)\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text for nodes.\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement=\"freeform\",  # you can use \"fixed\" or \"freeform\" as desired\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    "fig.update_layout(title_text=\"Linear Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0e44d0f06f912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:02.854410Z",
     "start_time": "2025-04-24T23:35:02.506231Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import colorsys\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions for colors\n",
    "# ---------------------------\n",
    "def get_color(i, n):\n",
    "    \"\"\"Generate a distinct RGB color for node index i among n nodes.\"\"\"\n",
    "    hue = i / n  # evenly spaced hue\n",
    "    # Using moderate saturation and high brightness for vivid colors\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, 0.6, 0.9)\n",
    "    r, g, b = int(r * 255), int(g * 255), int(b * 255)\n",
    "    return f\"rgb({r},{g},{b})\"\n",
    "\n",
    "def make_rgba(rgb_str, alpha=0.5):\n",
    "    \"\"\"Convert an rgb string (e.g., 'rgb(31,119,180)') to an rgba string with given alpha.\"\"\"\n",
    "    rgb_values = rgb_str.strip(\"rgb(\").strip(\")\").split(\",\")\n",
    "    return f\"rgba({rgb_values[0].strip()},{rgb_values[1].strip()},{rgb_values[2].strip()},{alpha})\"\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files and filter out weak connections (< 5 synapses)\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_2_opt_conns_superclass.csv\")\n",
    "df1 = df1[df1['syn_count'] >= 5]  # only keep connections with at least 5 synapses\n",
    "\n",
    "df2 = pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\")\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\")\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows (using summed synapse counts)\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 2\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class')['syn_count'].sum().reset_index(name='count')\n",
    "flow1['source'] = \"Set 2\"  # all connections originate from \"Set 2\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second'])['syn_count_second'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third'])['syn_count_third'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Build node lists directly from the data sources for each hop.\n",
    "# ---------------------------\n",
    "# Define columns:\n",
    "# Column 1: \"Set 2\"\n",
    "# Column 2: Unique first-level super classes from df1 (prefixed for clarity)\n",
    "# Column 3: Unique second-level super classes from df2 (prefixed)\n",
    "# Column 4: Unique third-level super classes from df3 (prefixed)\n",
    "col1 = [\"Set 2\"]\n",
    "col2 = [\"1: \" + label for label in sorted(df1['output_super_class'].unique())]\n",
    "col3 = [\"2: \" + label for label in sorted(df2['output_super_class'].unique())]\n",
    "col4 = [\"3: \" + label for label in sorted(df3['output_super_class'].unique())]\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Assign each node a unique color.\n",
    "# ---------------------------\n",
    "n_nodes = len(nodes)\n",
    "node_colors = [get_color(i, n_nodes) for i in range(n_nodes)]\n",
    "\n",
    "# ---------------------------\n",
    "# Update flows to use the new, prefixed node labels.\n",
    "# ---------------------------\n",
    "# Flow 1: Source is \"Set 2\"; target from df1 gets prefix \"1: \"\n",
    "flow1['source_idx'] = node_index[\"Set 2\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].apply(lambda x: node_index[\"1: \" + x])\n",
    "\n",
    "# Flow 2: Source is from df1 (first hop, prefix \"1: \") and target is from df2 (prefix \"2: \")\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].apply(lambda x: node_index[\"1: \" + x])\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "\n",
    "# Flow 3: Source is from df2 (prefix \"2: \") and target is from df3 (prefix \"3: \")\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].apply(lambda x: node_index[\"3: \" + x])\n",
    "\n",
    "# Combine flows into link lists.\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute custom hover info for nodes.\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    total = incoming if incoming >= outgoing else outgoing\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build link colors based on the source node's unique color (with 50% opacity).\n",
    "# ---------------------------\n",
    "link_colors = []\n",
    "for s in source:\n",
    "    base_color = node_colors[s]  # use the source node's color\n",
    "    link_colors.append(make_rgba(base_color, 0.5))\n",
    "\n",
    "# ---------------------------\n",
    "# Define fixed x positions so that the diagram is laid out in a linear, successive fashion.\n",
    "# ---------------------------\n",
    "x_positions = []\n",
    "x_positions += [0.0] * len(col1)\n",
    "x_positions += [0.33] * len(col2)\n",
    "x_positions += [0.66] * len(col3)\n",
    "x_positions += [1.0] * len(col4)\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text and link colors.\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement=\"freeform\",\n",
    "    node=dict(\n",
    "        pad=80,\n",
    "        thickness=50,\n",
    "        line=dict(color=\"black\", width=0.9),\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        color=node_colors,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=link_colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Linear Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cda08ca935cd10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:04.704688Z",
     "start_time": "2025-04-24T23:35:04.698524Z"
    }
   },
   "outputs": [],
   "source": [
    "flow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551962c8d2010e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:05.956767Z",
     "start_time": "2025-04-24T23:35:04.848874Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load connectivity CSV files\n",
    "df1 = pd.read_csv(\"set_2_opt_conns_superclass.csv\")\n",
    "df2 = pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\")\n",
    "df3 = pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "target_class = \"motor\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1-hop: Direct connections from Set 2 to a motor neuron.\n",
    "# ---------------------------\n",
    "df1_motor = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "df1_motor = df1_motor.assign(hop=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 2-hop: Chain from Set 2  Neuron2  Neuron3 where the second hop ends in a motor neuron.\n",
    "# ---------------------------\n",
    "df12 = pd.merge(\n",
    "    df1[['pre_root_id', 'post_root_id']],\n",
    "    df2,\n",
    "    left_on='post_root_id',\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df1', '_df2')\n",
    ")\n",
    "# In df12, the starting neuron from Set 2 is now in 'pre_root_id' from df1,\n",
    "# which has been renamed automatically to 'pre_root_id' (or to 'pre_root_id_df1' if needed).\n",
    "# Let's check for our purposes:\n",
    "if 'pre_root_id_df1' in df12.columns:\n",
    "    start_col = 'pre_root_id_df1'\n",
    "else:\n",
    "    start_col = 'pre_root_id'\n",
    "\n",
    "df12_motor = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "df12_motor = df12_motor.rename(columns={start_col: 'pre_root_id'})\n",
    "df12_motor = df12_motor.assign(hop=2)\n",
    "\n",
    "# ---------------------------\n",
    "# 3-hop: Chain from Set 2  Neuron2  Neuron3  Neuron4 where the third hop ends in a motor neuron.\n",
    "# ---------------------------\n",
    "df123 = pd.merge(\n",
    "    df12,\n",
    "    df3,\n",
    "    left_on='post_root_id_df2',  # the output from the second leg from df2\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df2', '_df3')\n",
    ")\n",
    "# After merging, the df3 column 'output_super_class' becomes 'output_super_class_df3'\n",
    "df123_motor = df123[df123['output_super_class_df3'] == target_class][[start_col]].drop_duplicates()\n",
    "df123_motor = df123_motor.rename(columns={start_col: 'pre_root_id'})\n",
    "df123_motor = df123_motor.assign(hop=3)\n",
    "\n",
    "# ---------------------------\n",
    "# Combine and determine the minimum hop count per starting neuron.\n",
    "# ---------------------------\n",
    "df_hops = pd.concat([df1_motor, df12_motor, df123_motor], ignore_index=True)\n",
    "min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "\n",
    "print(min_hops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6b977ba25d32ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:07.618250Z",
     "start_time": "2025-04-24T23:35:07.607500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for the hops\n",
    "mean_hops = min_hops['hop'].mean()\n",
    "median_hops = min_hops['hop'].median()\n",
    "std_hops = min_hops['hop'].std()\n",
    "min_hop = min_hops['hop'].min()\n",
    "max_hop = min_hops['hop'].max()\n",
    "count = min_hops['hop'].count()\n",
    "\n",
    "print(\"Descriptive statistics for hops to reach a motor neuron:\")\n",
    "print(f\"Number of neurons: {count}\")\n",
    "print(f\"Mean hops: {mean_hops:.2f}\")\n",
    "print(f\"Median hops: {median_hops}\")\n",
    "print(f\"Standard Deviation: {std_hops:.2f}\")\n",
    "print(f\"Minimum hops: {min_hop}\")\n",
    "print(f\"Maximum hops: {max_hop}\")\n",
    "\n",
    "# Alternatively, you can use describe() to get a summary:\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(min_hops['hop'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6b9f9d4a2bef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:10.042492Z",
     "start_time": "2025-04-24T23:35:09.765687Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Ensure 'hop' is numeric and get max_hop as a Python int.\n",
    "min_hops['hop'] = pd.to_numeric(min_hops['hop'])\n",
    "max_hop = int(min_hops['hop'].max())\n",
    "\n",
    "# --- Histogram ---\n",
    "# Set nbins to max_hop + 1 so that each integer hop has its own bin.\n",
    "fig_hist = px.histogram(\n",
    "    min_hops,\n",
    "    x=\"hop\",\n",
    "    nbins=max_hop + 1,\n",
    "    title=\"Distribution of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\", \"count\": \"Number of Neurons\"},\n",
    "    range_x=[0.5, max_hop + 0.5]\n",
    ")\n",
    "fig_hist.update_xaxes(dtick=1)\n",
    "fig_hist.show()\n",
    "\n",
    "# --- Box Plot ---\n",
    "fig_box = px.box(\n",
    "    min_hops,\n",
    "    y=\"hop\",\n",
    "    title=\"Box Plot of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_box.show()\n",
    "\n",
    "# --- Violin Plot ---\n",
    "fig_violin = px.violin(\n",
    "    min_hops,\n",
    "    y=\"hop\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Violin Plot of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_violin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240acc73d83c9bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:13.313751Z",
     "start_time": "2025-04-24T23:35:13.311407Z"
    }
   },
   "outputs": [],
   "source": [
    "##fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102df7ebedafad62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:13.467615Z",
     "start_time": "2025-04-24T23:35:13.465502Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1035f4f0f4312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:15.334938Z",
     "start_time": "2025-04-24T23:35:15.332237Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756dda5295b5ca50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:15.505436Z",
     "start_time": "2025-04-24T23:35:15.478469Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge set_3_outputs with classification_other to add the super_class for each output neuron.\n",
    "# (Assumes set_3_outputs has a column 'post_root_id' and classification_other has 'root_id' and 'super_class')\n",
    "set_3_outputs_with_class = pd.merge(\n",
    "    set_3_outputs,\n",
    "    classification_other[['root_id', 'super_class']],\n",
    "    left_on='post_root_id',\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the extra 'root_id' column that came from the merge\n",
    "set_3_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# (Optional) Rename the super_class column for clarity\n",
    "set_3_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# Save the result as a CSV file\n",
    "set_3_outputs_with_class.to_csv(\"set_3_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_3_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e61eb2429015f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:17.642844Z",
     "start_time": "2025-04-24T23:35:17.381686Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Use the original table from Option 1 and treat its post IDs as new pre IDs.\n",
    "# ------------------------------------------------------------------------------\n",
    "# We assume 'set_3_outputs' is the DataFrame from Option 1 with columns:\n",
    "#    pre_root_id, post_root_id, neuropil, syn_count, nt_type, location_of_connection, etc.\n",
    "#\n",
    "# Create a DataFrame of unique neurons that were outputs (i.e. post ids) in the first round:\n",
    "new_pre_neurons = pd.DataFrame({'root_id': set_3_outputs['post_root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Get the downstream outputs for these new pre neurons.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Use the neuronal_outputs function (which expects a DataFrame with a column 'root_id')\n",
    "# to find the downstream connections for these neurons.\n",
    "new_outputs = neuronal_outputs(new_pre_neurons)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with classification info so that each connection includes\n",
    "# the super_class of the output (i.e. new post) neuron.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class = pd.merge(\n",
    "    new_outputs,\n",
    "    classification_other[['root_id', 'super_class']],  # classification table for output neurons\n",
    "    left_on='post_root_id',  # new outputs: these are the downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'root_id' column that came from the merge\n",
    "new_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# Rename the column for clarity\n",
    "new_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as CSV.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class.to_csv(\"set_3_hop_1_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_3_hop_1_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8d6fd3ef488ac",
   "metadata": {},
   "source": [
    "i now want to make a sankey plot of this data where we see on axis 1 Set 3 axis 2 the super classes and axis 3 as super classes. for context here is a head of one of the csvs (pre_root_id\tpost_root_id\tneuropil\tsyn_count\tnt_type\tlocation_of_connection\toutput_super_class\n",
    "720575940617034713\t720575940628071211\tPRW\t5\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940633548128\tFLA_R\t7\tSER\tlocal\tascending\n",
    "720575940617034713\t720575940621662332\tFLA_R\t8\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940630672938\tFLA_R\t8\tSER\tlocal\tcentral\n",
    "720575940617034713\t720575940630672938\tPRW\t16\tACH\tlocal\tcentral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155ee91041b5906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:19.672648Z",
     "start_time": "2025-04-24T23:35:19.354861Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Load the previous CSV file and extract the unique post ids\n",
    "# ------------------------------------------------------------------------------\n",
    "prev_outputs = pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "# Here, the post ids from the previous file will now serve as the pre ids.\n",
    "new_pre_neurons_2 = pd.DataFrame({'root_id': prev_outputs['post_root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Find downstream connections for these new pre neurons using your neuronal_outputs function\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2 = neuronal_outputs(new_pre_neurons_2)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with the classification information to add super_class for each new downstream neuron\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2_with_class = pd.merge(\n",
    "    new_outputs_2,\n",
    "    classification_other[['root_id', 'super_class']],\n",
    "    left_on='post_root_id',  # these are the new downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove the extra 'root_id' column that came from the merge\n",
    "new_outputs_2_with_class.drop(columns=['root_id'], inplace=True)\n",
    "# Rename the super_class column for clarity\n",
    "new_outputs_2_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as a CSV file\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_2_with_class.to_csv(\"set_3_hop_2_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_3_hop_2_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bbb8cc16c44ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:21.672637Z",
     "start_time": "2025-04-24T23:35:21.436823Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Step 1: Create a DataFrame of unique neurons from the set_3 2Ns.\n",
    "# ------------------------------------------------------------------------------\n",
    "# Here, we assume that \"set_3_2Ns\" is your DataFrame of second-order neurons from Set 3,\n",
    "# which was generated earlier in your pipeline.\n",
    "new_pre_neurons = pd.DataFrame({'root_id': set_3_2Ns['root_id'].unique()})\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 2: Get the downstream outputs for these new pre neurons (i.e. the 3NS).\n",
    "# ------------------------------------------------------------------------------\n",
    "# Use the neuronal_outputs function (which expects a DataFrame with a 'root_id' column)\n",
    "# to extract all downstream connections for these neurons.\n",
    "new_outputs = neuronal_outputs(new_pre_neurons)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 3: Merge the new outputs with classification information so that each connection includes\n",
    "# the super_class of the output (i.e. new post) neuron.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class = pd.merge(\n",
    "    new_outputs,\n",
    "    classification_other[['root_id', 'super_class']],  # Classification table for output neurons\n",
    "    left_on='post_root_id',  # New outputs: these are the downstream neurons\n",
    "    right_on='root_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra 'root_id' column that came from the merge\n",
    "new_outputs_with_class.drop(columns=['root_id'], inplace=True)\n",
    "\n",
    "# Rename the column for clarity\n",
    "new_outputs_with_class.rename(columns={'super_class': 'output_super_class'}, inplace=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Step 4: Save the new table as CSV.\n",
    "# ------------------------------------------------------------------------------\n",
    "new_outputs_with_class.to_csv(\"set_3_hop_3_opt_conns_superclass.csv\", index=False)\n",
    "print(\"CSV table saved as 'set_3_hop_3_opt_conns_superclass.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ffd5c79ab1d806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:23.606112Z",
     "start_time": "2025-04-24T23:35:23.528485Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Load the first round data: Set 3 outputs (first-order)\n",
    "df1 = pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "# Load the second round data: outputs where we treat the first round outputs as new pre neurons\n",
    "df2 = pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "\n",
    "# --- Build flows from \"Set 3\" to first-level super classes ---\n",
    "# Group df1 by the first-level output super_class and count connections\n",
    "flow1 = df1.groupby('output_super_class').size().reset_index(name='count')\n",
    "\n",
    "# --- Build flows from first-level to second-level super classes ---\n",
    "# Merge the two dataframes by linking the first round's post_root_id (the output neuron)\n",
    "# with the second round's pre_root_id (the same neuron now acting as input)\n",
    "merged = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id', suffixes=('_first', '_second'))\n",
    "\n",
    "# Group by first-level and second-level super classes and count connections\n",
    "flow2 = merged.groupby(['output_super_class_first', 'output_super_class_second']).size().reset_index(name='count')\n",
    "\n",
    "# --- Define Sankey nodes ---\n",
    "# In this diagram, we want three columns:\n",
    "# Column 1: \"Set 3\" (a single node)\n",
    "# Column 2: Unique first-level super classes from df1\n",
    "# Column 3: Unique second-level super classes from the merged table\n",
    "nodes = []\n",
    "nodes.append(\"Set 3\")  # Column 1\n",
    "\n",
    "first_super_nodes = flow1['output_super_class'].unique().tolist()\n",
    "nodes.extend(first_super_nodes)  # Column 2\n",
    "\n",
    "second_super_nodes = flow2['output_super_class_second'].unique().tolist()\n",
    "nodes.extend(second_super_nodes)  # Column 3\n",
    "\n",
    "# Create a mapping from node label to node index for building links\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# --- Create Sankey link data ---\n",
    "\n",
    "# Flow 1: from \"Set 3\" (source) to each first-level super_class node\n",
    "source1 = []\n",
    "target1 = []\n",
    "value1 = []\n",
    "for _, row in flow1.iterrows():\n",
    "    source1.append(node_index[\"Set 3\"])  # all originate from \"Set 3\"\n",
    "    target1.append(node_index[row['output_super_class']])\n",
    "    value1.append(row['count'])\n",
    "\n",
    "# Flow 2: from first-level super class to second-level super class\n",
    "source2 = []\n",
    "target2 = []\n",
    "value2 = []\n",
    "for _, row in flow2.iterrows():\n",
    "    source2.append(node_index[row['output_super_class_first']])\n",
    "    target2.append(node_index[row['output_super_class_second']])\n",
    "    value2.append(row['count'])\n",
    "\n",
    "# Combine flows from both steps\n",
    "source = source1 + source2\n",
    "target = target1 + target2\n",
    "value  = value1  + value2\n",
    "\n",
    "# --- Build and display the Sankey diagram ---\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "        pad = 15,\n",
    "        thickness = 20,\n",
    "        line = dict(color = \"black\", width = 0.5),\n",
    "        label = nodes,\n",
    "    ),\n",
    "    link = dict(\n",
    "        source = source,\n",
    "        target = target,\n",
    "        value = value\n",
    "    ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram: Set 3 to First and Second Level Super Classes\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d6916d346808f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:25.967545Z",
     "start_time": "2025-04-24T23:35:25.790583Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "df2 = pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "df3 = pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows using vectorized operations\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 3\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class').size().reset_index(name='count')\n",
    "flow1['source'] = \"Set 3\"  # all connections originate from \"Set 3\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "# Merge df1 and df2 using the connection where first round's post_root_id becomes df2's pre_root_id.\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second']).size().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "# Merge df2 and df3 similarly (df2.post_root_id becomes df3.pre_root_id).\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third']).size().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Define nodes for the Sankey diagram\n",
    "# ---------------------------\n",
    "# We define four groups:\n",
    "# Column 1: \"Set 3\" (a single node)\n",
    "# Column 2: Unique first-level super classes from df1\n",
    "# Column 3: Unique second-level super classes from the merged flows (from flow2 and flow3)\n",
    "# Column 4: Unique third-level super classes from flow3\n",
    "nodes = []\n",
    "nodes.append(\"Set 3\")\n",
    "first_nodes = flow1['output_super_class'].unique().tolist()\n",
    "nodes.extend(first_nodes)\n",
    "second_nodes = pd.concat([flow2['output_super_class_second'], flow3['output_super_class_second']]).unique().tolist()\n",
    "nodes.extend(second_nodes)\n",
    "third_nodes = flow3['output_super_class_third'].unique().tolist()\n",
    "nodes.extend(third_nodes)\n",
    "\n",
    "# Create a mapping from node label to node index\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Vectorized mapping to get link indices\n",
    "# ---------------------------\n",
    "flow1['source_idx'] = node_index[\"Set 3\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].map(node_index)\n",
    "\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].map(node_index)\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].map(node_index)\n",
    "\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].map(node_index)\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].map(node_index)\n",
    "\n",
    "# Combine flows into link lists for the Sankey diagram\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute incoming and outgoing values for each node\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    total = max(incoming, outgoing)\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Determine hop counts for a target super class (e.g., \"motor\")\n",
    "# ---------------------------\n",
    "target_class = \"motor\"\n",
    "hop1_motor = flow1[flow1['output_super_class'] == target_class]['count'].sum()\n",
    "hop2_motor = flow2[flow2['output_super_class_second'] == target_class]['count'].sum()\n",
    "hop3_motor = flow3[flow3['output_super_class_third'] == target_class]['count'].sum()\n",
    "\n",
    "print(f\"Connections reaching '{target_class}' neurons at 1 hop (direct):\", hop1_motor)\n",
    "print(f\"Connections reaching '{target_class}' neurons at 2 hops:\", hop2_motor)\n",
    "print(f\"Connections reaching '{target_class}' neurons at 3 hops:\", hop3_motor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114660d3a6eac0b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:27.908965Z",
     "start_time": "2025-04-24T23:35:27.905290Z"
    }
   },
   "outputs": [],
   "source": [
    "#fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a485bf90e7813",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:28.192587Z",
     "start_time": "2025-04-24T23:35:28.019917Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files and filter out weak connections (< 5 synapses)\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "df1 = df1[df1['syn_count'] >= 5]  # only keep connections with at least 5 synapses\n",
    "\n",
    "df2 = pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows (using summed synapse counts)\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 3\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class')['syn_count'].sum().reset_index(name='count')\n",
    "flow1['source'] = \"Set 3\"  # all connections originate from \"Set 3\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "# Sum the synapse counts from df2 (now in 'syn_count_second') for each pair.\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second'])['syn_count_second'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "# Sum the synapse counts from df3 (now in 'syn_count_third') for each pair.\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third'])['syn_count_third'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Build node lists directly from the data sources for each hop.\n",
    "# This ensures that nodes appear in the column corresponding to their data source.\n",
    "# ---------------------------\n",
    "col1 = [\"Set 3\"]  # Column 1: the source\n",
    "col2 = [\"1: \" + label for label in sorted(df1['output_super_class'].unique())]  # Column 2 from df1\n",
    "col3 = [\"2: \" + label for label in sorted(df2['output_super_class'].unique())]  # Column 3 from df2\n",
    "col4 = [\"3: \" + label for label in sorted(df3['output_super_class'].unique())]  # Column 4 from df3\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Update flows to use the new, prefixed node labels.\n",
    "# ---------------------------\n",
    "# Flow 1: Source is \"Set 3\"; target from df1 gets prefix \"1: \"\n",
    "flow1['source_idx'] = node_index[\"Set 3\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].apply(lambda x: node_index[\"1: \" + x])\n",
    "\n",
    "# Flow 2: Source is from df1 (first hop, prefix \"1: \") and target is from df2 (second hop, prefix \"2: \")\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].apply(lambda x: node_index[\"1: \" + x])\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "\n",
    "# Flow 3: Source is from df2 (second hop, prefix \"2: \") and target is from df3 (third hop, prefix \"3: \")\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].apply(lambda x: node_index[\"3: \" + x])\n",
    "\n",
    "# Combine flows into link lists.\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute custom hover info for nodes.\n",
    "# For each node, we sum incoming and outgoing flows based on the link lists.\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    # For nodes that both receive and send flows, Plotly by default uses max(incoming, outgoing)\n",
    "    # Here we display both values and the \"total\" (the max of the two)\n",
    "    total = incoming if incoming >= outgoing else outgoing\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Define fixed x positions so that the diagram is laid out in a linear, successive fashion.\n",
    "# Column 1 (\"Set 3\") at x=0.0, Column 2 (first-level nodes) at x=0.33,\n",
    "# Column 3 (second-level nodes) at x=0.66, Column 4 (third-level nodes) at x=1.0.\n",
    "# ---------------------------\n",
    "x_positions = []\n",
    "x_positions += [0.0] * len(col1)\n",
    "x_positions += [0.33] * len(col2)\n",
    "x_positions += [0.66] * len(col3)\n",
    "x_positions += [1.0] * len(col4)\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text for nodes.\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement=\"freeform\",  # you can use \"fixed\" or \"freeform\" as desired\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value\n",
    "    )\n",
    ")])\n",
    "fig.update_layout(title_text=\"Linear Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c2569773ef2a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:30.194784Z",
     "start_time": "2025-04-24T23:35:30.015071Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import colorsys\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions for colors\n",
    "# ---------------------------\n",
    "def get_color(i, n):\n",
    "    \"\"\"Generate a distinct RGB color for node index i among n nodes.\"\"\"\n",
    "    hue = i / n  # evenly spaced hue\n",
    "    # Using moderate saturation and high brightness for vivid colors\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, 0.6, 0.9)\n",
    "    r, g, b = int(r * 255), int(g * 255), int(b * 255)\n",
    "    return f\"rgb({r},{g},{b})\"\n",
    "\n",
    "def make_rgba(rgb_str, alpha=0.5):\n",
    "    \"\"\"Convert an rgb string (e.g., 'rgb(31,119,180)') to an rgba string with given alpha.\"\"\"\n",
    "    rgb_values = rgb_str.strip(\"rgb(\").strip(\")\").split(\",\")\n",
    "    return f\"rgba({rgb_values[0].strip()},{rgb_values[1].strip()},{rgb_values[2].strip()},{alpha})\"\n",
    "\n",
    "# ---------------------------\n",
    "# Load CSV files and filter out weak connections (< 5 synapses)\n",
    "# ---------------------------\n",
    "df1 = pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "df1 = df1[df1['syn_count'] >= 5]  # only keep connections with at least 5 synapses\n",
    "\n",
    "df2 = pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# Build flows (using summed synapse counts)\n",
    "# ---------------------------\n",
    "# Flow 1: From \"Set 3\" to first-level super classes (from df1)\n",
    "flow1 = df1.groupby('output_super_class')['syn_count'].sum().reset_index(name='count')\n",
    "flow1['source'] = \"Set 3\"  # all connections originate from \"Set 3\"\n",
    "\n",
    "# Flow 2: From first-level to second-level super classes\n",
    "merged1 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_first', '_second'))\n",
    "flow2 = merged1.groupby(['output_super_class_first', 'output_super_class_second'])['syn_count_second'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# Flow 3: From second-level to third-level super classes\n",
    "merged2 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_second', '_third'))\n",
    "flow3 = merged2.groupby(['output_super_class_second', 'output_super_class_third'])['syn_count_third'] \\\n",
    "    .sum().reset_index(name='count')\n",
    "\n",
    "# ---------------------------\n",
    "# Build node lists directly from the data sources for each hop.\n",
    "# ---------------------------\n",
    "# Define columns:\n",
    "# Column 1: \"Set 3\"\n",
    "# Column 2: Unique first-level super classes from df1 (prefixed for clarity)\n",
    "# Column 3: Unique second-level super classes from df2 (prefixed)\n",
    "# Column 4: Unique third-level super classes from df3 (prefixed)\n",
    "col1 = [\"Set 3\"]\n",
    "col2 = [\"1: \" + label for label in sorted(df1['output_super_class'].unique())]\n",
    "col3 = [\"2: \" + label for label in sorted(df2['output_super_class'].unique())]\n",
    "col4 = [\"3: \" + label for label in sorted(df3['output_super_class'].unique())]\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# Assign each node a unique color.\n",
    "# ---------------------------\n",
    "n_nodes = len(nodes)\n",
    "node_colors = [get_color(i, n_nodes) for i in range(n_nodes)]\n",
    "\n",
    "# ---------------------------\n",
    "# Update flows to use the new, prefixed node labels.\n",
    "# ---------------------------\n",
    "# Flow 1: Source is \"Set 3\"; target from df1 gets prefix \"1: \"\n",
    "flow1['source_idx'] = node_index[\"Set 3\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].apply(lambda x: node_index[\"1: \" + x])\n",
    "\n",
    "# Flow 2: Source is from df1 (first hop, prefix \"1: \") and target is from df2 (prefix \"2: \")\n",
    "flow2['source_idx'] = flow2['output_super_class_first'].apply(lambda x: node_index[\"1: \" + x])\n",
    "flow2['target_idx'] = flow2['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "\n",
    "# Flow 3: Source is from df2 (prefix \"2: \") and target is from df3 (prefix \"3: \")\n",
    "flow3['source_idx'] = flow3['output_super_class_second'].apply(lambda x: node_index[\"2: \" + x])\n",
    "flow3['target_idx'] = flow3['output_super_class_third'].apply(lambda x: node_index[\"3: \" + x])\n",
    "\n",
    "# Combine flows into link lists.\n",
    "source = pd.concat([flow1['source_idx'], flow2['source_idx'], flow3['source_idx']]).tolist()\n",
    "target = pd.concat([flow1['target_idx'], flow2['target_idx'], flow3['target_idx']]).tolist()\n",
    "value  = pd.concat([flow1['count'], flow2['count'], flow3['count']]).tolist()\n",
    "\n",
    "# ---------------------------\n",
    "# Compute custom hover info for nodes.\n",
    "# ---------------------------\n",
    "node_incoming = {node: 0 for node in nodes}\n",
    "node_outgoing = {node: 0 for node in nodes}\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_outgoing[nodes[s]] += v\n",
    "    node_incoming[nodes[t]] += v\n",
    "\n",
    "customdata = []\n",
    "for node in nodes:\n",
    "    incoming = node_incoming[node]\n",
    "    outgoing = node_outgoing[node]\n",
    "    total = incoming if incoming >= outgoing else outgoing\n",
    "    customdata.append(f\"Incoming: {incoming}<br>Outgoing: {outgoing}<br>Total: {total}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build link colors based on the source node's unique color (with 50% opacity).\n",
    "# ---------------------------\n",
    "link_colors = []\n",
    "for s in source:\n",
    "    base_color = node_colors[s]  # use the source node's color\n",
    "    link_colors.append(make_rgba(base_color, 0.5))\n",
    "\n",
    "# ---------------------------\n",
    "# Define fixed x positions so that the diagram is laid out in a linear, successive fashion.\n",
    "# ---------------------------\n",
    "x_positions = []\n",
    "x_positions += [0.0] * len(col1)\n",
    "x_positions += [0.33] * len(col2)\n",
    "x_positions += [0.66] * len(col3)\n",
    "x_positions += [1.0] * len(col4)\n",
    "\n",
    "# ---------------------------\n",
    "# Build and display the Sankey diagram with custom hover text and link colors.\n",
    "# ---------------------------\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    arrangement=\"freeform\",\n",
    "    node=dict(\n",
    "        pad=80,\n",
    "        thickness=50,\n",
    "        line=dict(color=\"black\", width=0.9),\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        color=node_colors,\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=link_colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(title_text=\"Linear Sankey Diagram: Neural Pathway Hops\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13d671b3c93ba78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:31.928783Z",
     "start_time": "2025-04-24T23:35:31.921407Z"
    }
   },
   "outputs": [],
   "source": [
    "flow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294e4cae6548ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:32.435289Z",
     "start_time": "2025-04-24T23:35:32.068491Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load connectivity CSV files\n",
    "df1 = pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "df2 = pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "df3 = pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "target_class = \"motor\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1-hop: Direct connections from Set 3 to a motor neuron.\n",
    "# ---------------------------\n",
    "df1_motor = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "df1_motor = df1_motor.assign(hop=1)\n",
    "\n",
    "# ---------------------------\n",
    "# 2-hop: Chain from Set 3  Neuron2  Neuron3 where the second hop ends in a motor neuron.\n",
    "# ---------------------------\n",
    "df12 = pd.merge(\n",
    "    df1[['pre_root_id', 'post_root_id']],\n",
    "    df2,\n",
    "    left_on='post_root_id',\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df1', '_df2')\n",
    ")\n",
    "# In df12, the starting neuron from Set 3 is now in 'pre_root_id' from df1,\n",
    "# which has been renamed automatically to 'pre_root_id' (or to 'pre_root_id_df1' if needed).\n",
    "# Let's check for our purposes:\n",
    "if 'pre_root_id_df1' in df12.columns:\n",
    "    start_col = 'pre_root_id_df1'\n",
    "else:\n",
    "    start_col = 'pre_root_id'\n",
    "\n",
    "df12_motor = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "df12_motor = df12_motor.rename(columns={start_col: 'pre_root_id'})\n",
    "df12_motor = df12_motor.assign(hop=2)\n",
    "\n",
    "# ---------------------------\n",
    "# 3-hop: Chain from Set 3  Neuron2  Neuron3  Neuron4 where the third hop ends in a motor neuron.\n",
    "# ---------------------------\n",
    "df123 = pd.merge(\n",
    "    df12,\n",
    "    df3,\n",
    "    left_on='post_root_id_df2',  # the output from the second leg from df2\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df2', '_df3')\n",
    ")\n",
    "# After merging, the df3 column 'output_super_class' becomes 'output_super_class_df3'\n",
    "df123_motor = df123[df123['output_super_class_df3'] == target_class][[start_col]].drop_duplicates()\n",
    "df123_motor = df123_motor.rename(columns={start_col: 'pre_root_id'})\n",
    "df123_motor = df123_motor.assign(hop=3)\n",
    "\n",
    "# ---------------------------\n",
    "# Combine and determine the minimum hop count per starting neuron.\n",
    "# ---------------------------\n",
    "df_hops = pd.concat([df1_motor, df12_motor, df123_motor], ignore_index=True)\n",
    "min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "\n",
    "print(min_hops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56f273fba1c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:36.703694Z",
     "start_time": "2025-04-24T23:35:36.696044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics for the hops\n",
    "mean_hops = min_hops['hop'].mean()\n",
    "median_hops = min_hops['hop'].median()\n",
    "std_hops = min_hops['hop'].std()\n",
    "min_hop = min_hops['hop'].min()\n",
    "max_hop = min_hops['hop'].max()\n",
    "count = min_hops['hop'].count()\n",
    "\n",
    "print(\"Descriptive statistics for hops to reach a motor neuron:\")\n",
    "print(f\"Number of neurons: {count}\")\n",
    "print(f\"Mean hops: {mean_hops:.2f}\")\n",
    "print(f\"Median hops: {median_hops}\")\n",
    "print(f\"Standard Deviation: {std_hops:.2f}\")\n",
    "print(f\"Minimum hops: {min_hop}\")\n",
    "print(f\"Maximum hops: {max_hop}\")\n",
    "\n",
    "# Alternatively, you can use describe() to get a summary:\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(min_hops['hop'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620383842617da19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:37.099990Z",
     "start_time": "2025-04-24T23:35:36.858154Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Ensure 'hop' is numeric and get max_hop as a Python int.\n",
    "min_hops['hop'] = pd.to_numeric(min_hops['hop'])\n",
    "max_hop = int(min_hops['hop'].max())\n",
    "\n",
    "# --- Histogram ---\n",
    "# Set nbins to max_hop + 1 so that each integer hop has its own bin.\n",
    "fig_hist = px.histogram(\n",
    "    min_hops,\n",
    "    x=\"hop\",\n",
    "    nbins=max_hop + 1,\n",
    "    title=\"Distribution of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\", \"count\": \"Number of Neurons\"},\n",
    "    range_x=[0.5, max_hop + 0.5]\n",
    ")\n",
    "fig_hist.update_xaxes(dtick=1)\n",
    "fig_hist.show()\n",
    "\n",
    "# --- Box Plot ---\n",
    "fig_box = px.box(\n",
    "    min_hops,\n",
    "    y=\"hop\",\n",
    "    title=\"Box Plot of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_box.show()\n",
    "\n",
    "# --- Violin Plot ---\n",
    "fig_violin = px.violin(\n",
    "    min_hops,\n",
    "    y=\"hop\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Violin Plot of Minimum Hops to Reach a Motor Neuron\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_violin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b50053e8eb2059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:39.463803Z",
     "start_time": "2025-04-24T23:35:39.461181Z"
    }
   },
   "outputs": [],
   "source": [
    "##fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21487800a6ef41e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:41.295323Z",
     "start_time": "2025-04-24T23:35:41.292796Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b600364692666cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:35:42.369692Z",
     "start_time": "2025-04-24T23:35:41.424384Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_endocrine_hops(set_prefix, target_class=\"endocrine\"):\n",
    "    \"\"\"\n",
    "    Analyzes the minimum number of hops needed for a neuron from a given set\n",
    "    to ultimately reach an endocrine neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    set_prefix : str\n",
    "        The prefix to use for the file names, e.g. \"set_1\" or \"set_2\", etc.\n",
    "    target_class : str, optional\n",
    "        The target output super_class (default is \"endocrine\")\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    min_hops : pd.DataFrame\n",
    "        DataFrame with each starting neuron (pre_root_id) and the minimum hop count.\n",
    "    \"\"\"\n",
    "    # Load first-round (direct) connections from the specified set.\n",
    "    df1 = pd.read_csv(f\"{set_prefix}_opt_conns_superclass.csv\")\n",
    "    # Load second-round connections (first hop applied, now new pre neurons are the previous post ids).\n",
    "    df2 = pd.read_csv(f\"{set_prefix}_hop_1_opt_conns_superclass.csv\")\n",
    "    # Load third-round connections.\n",
    "    df3 = pd.read_csv(f\"{set_prefix}_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1-Hop: Direct connections from set_X to an endocrine neuron.\n",
    "    # -------------------------------------------------------------------------\n",
    "    df1_target = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    df1_target = df1_target.assign(hop=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2-Hop: Set_X  Neuron (from df1)  Endocrine neuron (from df2).\n",
    "    # Merge df1 and df2 on: previous post_root_id becomes new pre_root_id.\n",
    "    # -------------------------------------------------------------------------\n",
    "    df12 = pd.merge(\n",
    "        df1[['pre_root_id', 'post_root_id']],\n",
    "        df2,\n",
    "        left_on='post_root_id',\n",
    "        right_on='pre_root_id',  # new pre ids come from the df1 outputs\n",
    "        suffixes=('_df1', '_df2')\n",
    "    )\n",
    "    # In df12, we want to keep only those chains that eventually end in an endocrine neuron.\n",
    "    df12_target = df12[df12['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    # We assume the starting neuron is the one from df1 (it appears in the original 'pre_root_id')\n",
    "    df12_target = df12_target.rename(columns={'pre_root_id': 'pre_root_id'})\n",
    "    df12_target = df12_target.assign(hop=2)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3-Hop: Set_X  Neuron (df1)  Neuron (df2)  Endocrine neuron (from df3).\n",
    "    # Merge the chain from df12 with df3.\n",
    "    # -------------------------------------------------------------------------\n",
    "    # We expect that in the merge of df12 with df3, the connection from the second hop\n",
    "    # is used (often automatically renamed; adjust the key if needed).\n",
    "    merge_key = 'post_root_id_df2' if 'post_root_id_df2' in df12.columns else 'post_root_id'\n",
    "\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=merge_key,\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df2', '_df3')\n",
    "    )\n",
    "    # Filter for chains where df3 shows the target class.\n",
    "    if 'output_super_class_df3' in df123.columns:\n",
    "        df123_target = df123[df123['output_super_class_df3'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    else:\n",
    "        df123_target = df123[df123['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    df123_target = df123_target.rename(columns={'pre_root_id': 'pre_root_id'})\n",
    "    df123_target = df123_target.assign(hop=3)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine the results from all hops and choose the minimum hop per starting neuron.\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_hops = pd.concat([df1_target, df12_target, df123_target], ignore_index=True)\n",
    "    min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "\n",
    "    return min_hops\n",
    "\n",
    "# Define the target class to look for (\"endocrine\")\n",
    "target_class = \"endocrine\"\n",
    "# List the sets (adjust if you have a different naming scheme)\n",
    "set_numbers = [1, 2, 3]\n",
    "\n",
    "# Dictionary to store results for each set\n",
    "results = {}\n",
    "\n",
    "for set_num in set_numbers:\n",
    "    set_prefix = f\"set_{set_num}\"\n",
    "    print(f\"Analyzing endocrine hops for {set_prefix} ...\")\n",
    "    min_hops = analyze_endocrine_hops(set_prefix, target_class=target_class)\n",
    "    results[set_prefix] = min_hops\n",
    "\n",
    "    # Print descriptive statistics\n",
    "    print(f\"Descriptive statistics for {set_prefix}:\")\n",
    "    print(min_hops['hop'].describe())\n",
    "\n",
    "    # Plot histogram of hop counts\n",
    "    max_hop = int(min_hops['hop'].max())\n",
    "    fig_hist = px.histogram(\n",
    "        min_hops,\n",
    "        x=\"hop\",\n",
    "        nbins=max_hop + 1,\n",
    "        title=f\"Distribution of Minimum Hops to Reach an Endocrine Neuron ({set_prefix})\",\n",
    "        labels={\"hop\": \"Minimum Hops\", \"count\": \"Number of Neurons\"}\n",
    "    )\n",
    "    fig_hist.update_xaxes(dtick=1)\n",
    "    fig_hist.show()\n",
    "\n",
    "# (Optional) You can combine results from all sets or save the results to CSV files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa194ba9237102a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:40.724290Z",
     "start_time": "2025-04-24T23:46:40.715314Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge df1 and df2 to build 2-hop chains.\n",
    "df12 = pd.merge(\n",
    "    df1[['pre_root_id', 'post_root_id']],\n",
    "    df2,\n",
    "    left_on='post_root_id',\n",
    "    right_on='pre_root_id',\n",
    "    suffixes=('_df1', '_df2')\n",
    ")\n",
    "\n",
    "# Determine which column holds the starting neuron\n",
    "start_col = 'pre_root_id_df1' if 'pre_root_id_df1' in df12.columns else 'pre_root_id'\n",
    "\n",
    "# Then filter for chains whose output is endocrine.\n",
    "df12_target = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "df12_target = df12_target.rename(columns={start_col: 'pre_root_id'})\n",
    "df12_target = df12_target.assign(hop=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc385b841422f175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:41.035678Z",
     "start_time": "2025-04-24T23:46:40.825067Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_endocrine_hops(set_prefix, target_class=\"endocrine\"):\n",
    "    \"\"\"\n",
    "    Analyzes the minimum number of hops needed for a neuron from a given set\n",
    "    to ultimately reach an endocrine neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    set_prefix : str\n",
    "        The prefix to use for the file names (e.g., \"set_1\").\n",
    "    target_class : str, optional\n",
    "        The target output super_class (default is \"endocrine\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    min_hops : pd.DataFrame\n",
    "        DataFrame with each starting neuron (pre_root_id) and the minimum hop count.\n",
    "    \"\"\"\n",
    "    # Load the CSV files (modify paths as needed)\n",
    "    df1 = pd.read_csv(f\"{set_prefix}_opt_conns_superclass.csv\")\n",
    "    df2 = pd.read_csv(f\"{set_prefix}_hop_1_opt_conns_superclass.csv\")\n",
    "    df3 = pd.read_csv(f\"{set_prefix}_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1-Hop: Direct connections from the set to an endocrine neuron.\n",
    "    # -------------------------------------------------------------------------\n",
    "    df1_target = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    df1_target = df1_target.assign(hop=1)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2-Hop: Merge df1 and df2 (chain: set neuron -> intermediate neuron -> output).\n",
    "    # -------------------------------------------------------------------------\n",
    "    df12 = pd.merge(\n",
    "        df1[['pre_root_id', 'post_root_id']],\n",
    "        df2,\n",
    "        left_on='post_root_id',\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df1', '_df2')\n",
    "    )\n",
    "    # Because both df1 and df2 have a column named \"pre_root_id\", the left one is renamed.\n",
    "    start_col = 'pre_root_id_df1' if 'pre_root_id_df1' in df12.columns else 'pre_root_id'\n",
    "    df12_target = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "    df12_target = df12_target.rename(columns={start_col: 'pre_root_id'})\n",
    "    df12_target = df12_target.assign(hop=2)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3-Hop: Merge the 2-hop chains (df12) with df3.\n",
    "    # -------------------------------------------------------------------------\n",
    "    # For the merge key, use the second hops output. Depending on your dataframe,\n",
    "    # the column name might be automatically suffixed.\n",
    "    merge_key = 'post_root_id_df2' if 'post_root_id_df2' in df12.columns else 'post_root_id'\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=merge_key,\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df2', '_df3')\n",
    "    )\n",
    "    # In df123, check if the output column from df3 was renamed:\n",
    "    out_col = 'output_super_class_df3' if 'output_super_class_df3' in df123.columns else 'output_super_class'\n",
    "    df123_target = df123[df123[out_col] == target_class][[start_col]].drop_duplicates()\n",
    "    df123_target = df123_target.rename(columns={start_col: 'pre_root_id'})\n",
    "    df123_target = df123_target.assign(hop=3)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine and select the minimum hop count per starting neuron.\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_hops = pd.concat([df1_target, df12_target, df123_target], ignore_index=True)\n",
    "    min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "\n",
    "    return min_hops\n",
    "\n",
    "# Example usage for set_1:\n",
    "set_prefix = \"set_1\"\n",
    "target_class = \"endocrine\"\n",
    "\n",
    "min_hops_set1 = analyze_endocrine_hops(set_prefix, target_class=target_class)\n",
    "\n",
    "# Display descriptive statistics:\n",
    "print(f\"Descriptive statistics for {set_prefix} (endocrine hops):\")\n",
    "print(min_hops_set1['hop'].describe())\n",
    "\n",
    "# Plot a histogram:\n",
    "max_hop = int(min_hops_set1['hop'].max())\n",
    "fig_hist = px.histogram(\n",
    "    min_hops_set1,\n",
    "    x=\"hop\",\n",
    "    nbins=max_hop + 1,\n",
    "    title=f\"Distribution of Minimum Hops to Reach an Endocrine Neuron ({set_prefix})\",\n",
    "    labels={\"hop\": \"Minimum Hops\", \"count\": \"Number of Neurons\"}\n",
    ")\n",
    "fig_hist.update_xaxes(dtick=1)\n",
    "fig_hist.show()\n",
    "\n",
    "# Plot a violin plot:\n",
    "fig_violin = px.violin(\n",
    "    min_hops_set1,\n",
    "    y=\"hop\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=f\"Violin Plot of Minimum Hops to Reach an Endocrine Neuron ({set_prefix})\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_violin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326a903a6b132f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:43.216726Z",
     "start_time": "2025-04-24T23:46:41.538027Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_endocrine_hops(set_prefix, target_class=\"endocrine\"):\n",
    "    \"\"\"\n",
    "    Analyzes the minimum number of hops needed for a neuron from a given set\n",
    "    to ultimately reach an endocrine neuron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    set_prefix : str\n",
    "        The prefix for the file names (e.g., \"set_1\").\n",
    "    target_class : str, optional\n",
    "        The target output super_class (default is \"endocrine\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    min_hops : pd.DataFrame\n",
    "        DataFrame with each starting neuron (pre_root_id) and the minimum hop count.\n",
    "    \"\"\"\n",
    "    # Load CSV files (adjust paths if necessary)\n",
    "    df1 = pd.read_csv(f\"{set_prefix}_opt_conns_superclass.csv\")\n",
    "    df2 = pd.read_csv(f\"{set_prefix}_hop_1_opt_conns_superclass.csv\")\n",
    "    df3 = pd.read_csv(f\"{set_prefix}_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "    # --- 1-Hop ---\n",
    "    # Direct connections: keep rows where output_super_class equals the target_class.\n",
    "    df1_target = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    df1_target = df1_target.assign(hop=1)\n",
    "\n",
    "    # --- 2-Hop ---\n",
    "    # Merge df1 (starting connections) with df2 on df1.post_root_id == df2.pre_root_id.\n",
    "    df12 = pd.merge(\n",
    "        df1[['pre_root_id', 'post_root_id']],\n",
    "        df2,\n",
    "        left_on='post_root_id',\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df1', '_df2')\n",
    "    )\n",
    "    # The starting neuron from df1 may be automatically renamed.\n",
    "    start_col = 'pre_root_id_df1' if 'pre_root_id_df1' in df12.columns else 'pre_root_id'\n",
    "    df12_target = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "    df12_target = df12_target.rename(columns={start_col: 'pre_root_id'})\n",
    "    df12_target = df12_target.assign(hop=2)\n",
    "\n",
    "    # --- 3-Hop ---\n",
    "    # Merge the 2-hop chains with df3. Use the second hop's output column.\n",
    "    merge_key = 'post_root_id_df2' if 'post_root_id_df2' in df12.columns else 'post_root_id'\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=merge_key,\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df2', '_df3')\n",
    "    )\n",
    "    # Depending on the merge, the output from df3 might be renamed.\n",
    "    out_col = 'output_super_class_df3' if 'output_super_class_df3' in df123.columns else 'output_super_class'\n",
    "    df123_target = df123[df123[out_col] == target_class][[start_col]].drop_duplicates()\n",
    "    df123_target = df123_target.rename(columns={start_col: 'pre_root_id'})\n",
    "    df123_target = df123_target.assign(hop=3)\n",
    "\n",
    "    # --- Combine and compute the minimum hops ---\n",
    "    df_hops = pd.concat([df1_target, df12_target, df123_target], ignore_index=True)\n",
    "    min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "\n",
    "    return min_hops\n",
    "\n",
    "# Loop over sets 1 to 6 and perform the analysis:\n",
    "target_class = \"endocrine\"\n",
    "results = {}\n",
    "\n",
    "for set_num in range(1, 4):\n",
    "    set_prefix = f\"set_{set_num}\"\n",
    "    print(f\"\\nAnalyzing endocrine hops for {set_prefix} ...\")\n",
    "\n",
    "    # Compute minimum hops to endocrine neuron for current set\n",
    "    min_hops = analyze_endocrine_hops(set_prefix, target_class=target_class)\n",
    "    results[set_prefix] = min_hops\n",
    "\n",
    "    # Print descriptive statistics\n",
    "    print(f\"Descriptive statistics for {set_prefix} (endocrine hops):\")\n",
    "    print(min_hops['hop'].describe())\n",
    "\n",
    "    # Plot histogram for the set\n",
    "    max_hop = int(min_hops['hop'].max())\n",
    "    fig_hist = px.histogram(\n",
    "        min_hops,\n",
    "        x=\"hop\",\n",
    "        nbins=max_hop + 1,\n",
    "        title=f\"Distribution of Minimum Hops to Reach an Endocrine Neuron ({set_prefix})\",\n",
    "        labels={\"hop\": \"Minimum Hops\", \"count\": \"Number of Neurons\"}\n",
    "    )\n",
    "    fig_hist.update_xaxes(dtick=1)\n",
    "    fig_hist.show()\n",
    "\n",
    "    # Plot violin plot for the set\n",
    "    fig_violin = px.violin(\n",
    "        min_hops,\n",
    "        y=\"hop\",\n",
    "        box=True,\n",
    "        points=\"all\",\n",
    "        title=f\"Violin Plot of Minimum Hops to Reach an Endocrine Neuron ({set_prefix})\",\n",
    "        labels={\"hop\": \"Minimum Hops\"}\n",
    "    )\n",
    "    fig_violin.show()\n",
    "\n",
    "# (Optional) Combine all sets for overall analysis if needed:\n",
    "all_results = pd.concat([results[sp] for sp in results], keys=results.keys(), names=[\"set\", \"index\"]).reset_index()\n",
    "print(\"Combined results for all sets:\")\n",
    "print(all_results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ad10aea8045fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:46.146855Z",
     "start_time": "2025-04-24T23:46:45.903576Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Suppose you already have the results stored in a dictionary called results,\n",
    "# with keys 'set_1', 'set_2', ..., 'set_6'. For example:\n",
    "# results = {\n",
    "#     \"set_1\": min_hops_set1,\n",
    "#     \"set_2\": min_hops_set2,\n",
    "#     ...\n",
    "#     \"set_6\": min_hops_set6\n",
    "# }\n",
    "#\n",
    "# If you used our previous function:\n",
    "#    min_hops = analyze_endocrine_hops(set_prefix, target_class=\"endocrine\")\n",
    "# and stored each in the dictionary 'results', you now can combine them.\n",
    "\n",
    "# --- Combine the results from sets 1 through 6 ---\n",
    "all_results = []\n",
    "for s in range(1, 4):\n",
    "    set_prefix = f\"set_{s}\"\n",
    "    # Assuming each results[set_prefix] is a DataFrame with 'pre_root_id' and 'hop'\n",
    "    df_temp = results[set_prefix].copy()\n",
    "    df_temp[\"set\"] = set_prefix\n",
    "    all_results.append(df_temp)\n",
    "\n",
    "combined_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# --- Create a stacked bar chart ---\n",
    "# Create a frequency table: for each set, count number of neurons with each hop value.\n",
    "freq_df = combined_df.groupby([\"set\", \"hop\"]).size().reset_index(name=\"count\")\n",
    "# Plot the stacked bar chart\n",
    "fig_bar = px.bar(\n",
    "    freq_df,\n",
    "    x=\"set\",\n",
    "    y=\"count\",\n",
    "    color=\"hop\",\n",
    "    barmode=\"stack\",\n",
    "    title=\"Stacked Bar Chart of Minimum Endocrine Hops by Set\",\n",
    "    labels={\"set\": \"Set\", \"count\": \"Number of Neurons\", \"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_bar.show()\n",
    "\n",
    "# --- Create combined violin subplots ---\n",
    "# You can use facet_col to create subplots by set.\n",
    "fig_violin = px.violin(\n",
    "    combined_df,\n",
    "    y=\"hop\",\n",
    "    color=\"set\",\n",
    "    facet_col=\"set\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Violin Plots of Minimum Endocrine Hops by Set\",\n",
    "    labels={\"hop\": \"Minimum Hops\"}\n",
    ")\n",
    "fig_violin.update_layout(showlegend=False)\n",
    "fig_violin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317bda3fbc383c5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:49.459611Z",
     "start_time": "2025-04-24T23:46:48.055885Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_target_hops(set_prefix, target_class=\"motor\"):\n",
    "    \"\"\"\n",
    "    Computes the minimum number of hops needed for neurons from a given aPhN2SA set\n",
    "    to eventually reach a neuron whose output super_class equals target_class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    set_prefix : str\n",
    "        The prefix for the file names (e.g., \"set_1\").\n",
    "    target_class : str, optional\n",
    "        The target output super_class (for motor, use \"motor\"; for endocrine, use \"endocrine\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    min_hops : pd.DataFrame\n",
    "        DataFrame with each starting neuron (pre_root_id) and its minimum hop count.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    # Load the three connectivity files (modify paths if necessary)\n",
    "    df1 = pd.read_csv(f\"{set_prefix}_opt_conns_superclass.csv\")\n",
    "    df2 = pd.read_csv(f\"{set_prefix}_hop_1_opt_conns_superclass.csv\")\n",
    "    df3 = pd.read_csv(f\"{set_prefix}_hop_2_opt_conns_superclass.csv\")\n",
    "\n",
    "    # --- 1-Hop: Direct connections ---\n",
    "    df1_target = df1[df1['output_super_class'] == target_class][['pre_root_id']].drop_duplicates()\n",
    "    df1_target = df1_target.assign(hop=1)\n",
    "\n",
    "    # --- 2-Hop: Merge df1 and df2 ---\n",
    "    df12 = pd.merge(\n",
    "        df1[['pre_root_id', 'post_root_id']],\n",
    "        df2,\n",
    "        left_on='post_root_id',\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df1', '_df2')\n",
    "    )\n",
    "    # The starting neuron from df1 may get renamed automatically\n",
    "    start_col = 'pre_root_id_df1' if 'pre_root_id_df1' in df12.columns else 'pre_root_id'\n",
    "    df12_target = df12[df12['output_super_class'] == target_class][[start_col]].drop_duplicates()\n",
    "    df12_target = df12_target.rename(columns={start_col: 'pre_root_id'})\n",
    "    df12_target = df12_target.assign(hop=2)\n",
    "\n",
    "    # --- 3-Hop: Merge df12 with df3 ---\n",
    "    merge_key = 'post_root_id_df2' if 'post_root_id_df2' in df12.columns else 'post_root_id'\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=merge_key,\n",
    "        right_on='pre_root_id',\n",
    "        suffixes=('_df2', '_df3')\n",
    "    )\n",
    "    out_col = 'output_super_class_df3' if 'output_super_class_df3' in df123.columns else 'output_super_class'\n",
    "    df123_target = df123[df123[out_col] == target_class][[start_col]].drop_duplicates()\n",
    "    df123_target = df123_target.rename(columns={start_col: 'pre_root_id'})\n",
    "    df123_target = df123_target.assign(hop=3)\n",
    "\n",
    "    # --- Combine and compute minimum hops per starting neuron ---\n",
    "    df_hops = pd.concat([df1_target, df12_target, df123_target], ignore_index=True)\n",
    "    min_hops = df_hops.groupby('pre_root_id', as_index=False)['hop'].min()\n",
    "    return min_hops\n",
    "\n",
    "\n",
    "# (Make sure that the connectivity CSV files are in the working directory.)\n",
    "min_hops_motor_set1 = analyze_target_hops(\"set_1\", target_class=\"motor\")\n",
    "min_hops_motor_set2 = analyze_target_hops(\"set_2\", target_class=\"motor\")\n",
    "min_hops_motor_set3 = analyze_target_hops(\"set_3\", target_class=\"motor\")\n",
    "\n",
    "results_motor = {\n",
    "    \"set_1\": min_hops_motor_set1,\n",
    "    \"set_2\": min_hops_motor_set2,\n",
    "    \"set_3\": min_hops_motor_set3\n",
    "}\n",
    "\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Combine results from all six sets\n",
    "all_results_motor = []\n",
    "for s in range(1, 4):\n",
    "    set_prefix = f\"set_{s}\"\n",
    "    df_temp = results_motor[set_prefix].copy()  # each DataFrame should contain 'pre_root_id' and 'hop'\n",
    "    df_temp[\"set\"] = set_prefix\n",
    "    all_results_motor.append(df_temp)\n",
    "\n",
    "combined_motor = pd.concat(all_results_motor, ignore_index=True)\n",
    "# Convert hop values to string so they are treated as discrete categories\n",
    "combined_motor['hop'] = combined_motor['hop'].astype(str)\n",
    "combined_motor['set'] = pd.Categorical(combined_motor['set'], categories=[\"set_1\", \"set_2\", \"set_3\"], ordered=True)\n",
    "\n",
    "# --- Stacked Bar Chart ---\n",
    "# Create a frequency table counting the number of neurons per set per hop value.\n",
    "freq_motor = combined_motor.groupby([\"set\", \"hop\"]).size().reset_index(name=\"count\")\n",
    "# Define a color mapping for motor hop categories (1, 2, and 3 hops).\n",
    "motor_color_map = {\n",
    "    \"1\": \"red\",\n",
    "    \"2\": \"yellow\",\n",
    "    \"3\": \"blue\"\n",
    "}\n",
    "fig_bar_motor = px.bar(\n",
    "    freq_motor,\n",
    "    x=\"set\",\n",
    "    y=\"count\",\n",
    "    color=\"hop\",\n",
    "    barmode=\"stack\",\n",
    "    title=\"Stacked Bar Chart of Minimum Motor Hops by aPhN2SA Set\",\n",
    "    labels={\"set\": \"aPhN2SA Set\", \"count\": \"Number of Neurons\", \"hop\": \"Minimum Motor Hops\"},\n",
    "    color_discrete_map=motor_color_map\n",
    ")\n",
    "fig_bar_motor.show()\n",
    "\n",
    "# --- Combined Violin Plot ---\n",
    "# Create violin plots faceted by set to show the distribution of hop values.\n",
    "fig_violin_motor = px.violin(\n",
    "    combined_motor,\n",
    "    y=\"hop\",\n",
    "    color=\"set\",\n",
    "    facet_col=\"set\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Violin Plots of Minimum Motor Hops by aPhN2SA Set\",\n",
    "    labels={\"hop\": \"Minimum Motor Hops\"}\n",
    ")\n",
    "fig_violin_motor.update_layout(showlegend=False)\n",
    "fig_violin_motor.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb2d20e84ae5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:51.240860Z",
     "start_time": "2025-04-24T23:46:50.144188Z"
    }
   },
   "outputs": [],
   "source": [
    "min_hops_endocrine_set1 = analyze_target_hops(\"set_1\", target_class=\"endocrine\")\n",
    "min_hops_endocrine_set2 = analyze_target_hops(\"set_2\", target_class=\"endocrine\")\n",
    "min_hops_endocrine_set3 = analyze_target_hops(\"set_3\", target_class=\"endocrine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a26cac9c3c54e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:52.267993Z",
     "start_time": "2025-04-24T23:46:51.974818Z"
    }
   },
   "outputs": [],
   "source": [
    "results_endocrine = {\n",
    "    \"set_1\": min_hops_endocrine_set1,  # DataFrame with columns 'pre_root_id' and 'hop'\n",
    "    \"set_2\": min_hops_endocrine_set2,\n",
    "    \"set_3\": min_hops_endocrine_set3,\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "all_results_endocrine = []\n",
    "for s in range(1, 4):\n",
    "    set_prefix = f\"set_{s}\"\n",
    "    df_temp = results_endocrine[set_prefix].copy()\n",
    "    df_temp[\"set\"] = set_prefix\n",
    "    all_results_endocrine.append(df_temp)\n",
    "\n",
    "combined_endocrine = pd.concat(all_results_endocrine, ignore_index=True)\n",
    "\n",
    "# Convert hop values to string so that Plotly treats them as discrete\n",
    "combined_endocrine['hop'] = combined_endocrine['hop'].astype(str)\n",
    "# Force the sets to be ordered as set_1, set_2, ... set_6\n",
    "combined_endocrine['set'] = pd.Categorical(combined_endocrine['set'],\n",
    "                                           categories=[\"set_1\", \"set_2\", \"set_3\"],\n",
    "                                           ordered=True)\n",
    "\n",
    "# --- Create a stacked bar chart for endocrine hops ---\n",
    "freq_endocrine = combined_endocrine.groupby([\"set\", \"hop\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Define a discrete color mapping for endocrine hop categories.\n",
    "endocrine_color_map = {\n",
    "    \"1\": \"red\",\n",
    "    \"2\": \"yellow\",\n",
    "    \"3\": \"blue\"\n",
    "}\n",
    "\n",
    "fig_bar_endocrine = px.bar(\n",
    "    freq_endocrine,\n",
    "    x=\"set\",\n",
    "    y=\"count\",\n",
    "    color=\"hop\",\n",
    "    barmode=\"stack\",\n",
    "    title=\"Stacked Bar Chart of Minimum Endocrine Hops by aPhN2SA Set\",\n",
    "    labels={\"set\": \"aPhN2SA Set\", \"count\": \"Number of Neurons\", \"hop\": \"Minimum Endocrine Hops\"},\n",
    "    color_discrete_map=endocrine_color_map,\n",
    "    category_orders={\"set\": [\"set_1\", \"set_2\", \"set_3\"]}\n",
    ")\n",
    "fig_bar_endocrine.show()\n",
    "\n",
    "# --- Create combined violin subplots for endocrine hops ---\n",
    "fig_violin_endocrine = px.violin(\n",
    "    combined_endocrine,\n",
    "    y=\"hop\",\n",
    "    color=\"set\",\n",
    "    facet_col=\"set\",\n",
    "    box=True,\n",
    "    points=\"all\",\n",
    "    title=\"Violin Plots of Minimum Endocrine Hops by aPhN2SA Set\",\n",
    "    labels={\"hop\": \"Minimum Endocrine Hops\"}\n",
    ")\n",
    "fig_violin_endocrine.update_layout(showlegend=False)\n",
    "fig_violin_endocrine.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bde3ea8f0c8ed73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:54.070833Z",
     "start_time": "2025-04-24T23:46:54.068636Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7460e2c03093dd61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:55.833075Z",
     "start_time": "2025-04-24T23:46:55.830873Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f05bd92711c73b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:46:56.032560Z",
     "start_time": "2025-04-24T23:46:55.997461Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- after you've built & reindexed freq_motor as you already have ---\n",
    "all_sets = list(total_counts.keys())\n",
    "all_hops = ['1','2','3','>3']\n",
    "\n",
    "# force categorical ordering\n",
    "freq_motor['hop'] = pd.Categorical(freq_motor['hop'], categories=all_hops, ordered=True)\n",
    "\n",
    "# --- define your motor_color_map (note '>3' is grey) ---\n",
    "motor_color_map = {\n",
    "    '1': 'red',\n",
    "    '2': 'yellow',\n",
    "    '3': 'blue',\n",
    "    '>3': 'grey'\n",
    "}\n",
    "\n",
    "# --- your existing px.bar(...) call ---\n",
    "fig_bar_motor = px.bar(\n",
    "    freq_motor,\n",
    "    x=\"set\",\n",
    "    y=\"count\",\n",
    "    color=\"hop\",\n",
    "    barmode=\"stack\",\n",
    "    category_orders={\"set\": all_sets, \"hop\": all_hops},\n",
    "    color_discrete_map=motor_color_map,\n",
    "    title=\"Stacked Bar Chart of Minimum Motor Hops by aPhN2SA Set\",\n",
    "    labels={\"set\":\"aPhN2SA Set\",\"count\":\"# Neurons\",\"hop\":\"Hops to Motor\"}\n",
    ")\n",
    "fig_bar_motor.update_layout(xaxis_title=None)\n",
    "\n",
    "# --- now force a dummy grey >3 trace in the legend if it isnt already there ---\n",
    "if not any(t.name == \">3\" for t in fig_bar_motor.data):\n",
    "    fig_bar_motor.add_trace(\n",
    "        go.Bar(\n",
    "            x=[all_sets[0]],      # just needs one xvalue\n",
    "            y=[0],                # zero height\n",
    "            name=\">3\",\n",
    "            marker_color=\"grey\",\n",
    "            showlegend=True,\n",
    "            visible=\"legendonly\"  # keeps it out of the actual bars\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig_bar_motor.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf861da4f834b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T23:47:07.140251Z",
     "start_time": "2025-04-24T23:47:07.027574Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# --- 1) compute total aPhN2SAs per set (same as motor) ---\n",
    "total_counts = {\n",
    "    'set_1': set_1['root_id'].nunique(),\n",
    "    'set_2': set_2['root_id'].nunique(),\n",
    "    'set_3': set_3['root_id'].nunique()\n",
    "}\n",
    "\n",
    "# --- 2) build your existing freq table for endocrine ---\n",
    "freq_endocrine = (\n",
    "    combined_endocrine\n",
    "    .groupby([\"set\",\"hop\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# --- 3) insert the >3 category per set ---\n",
    "extra = []\n",
    "for s, total in total_counts.items():\n",
    "    reached = freq_endocrine.loc[freq_endocrine['set']==s, 'count'].sum()\n",
    "    unreached = total - reached\n",
    "    if unreached > 0:\n",
    "        extra.append({'set': s, 'hop': '>3', 'count': unreached})\n",
    "if extra:\n",
    "    freq_endocrine = pd.concat([freq_endocrine, pd.DataFrame(extra)], ignore_index=True)\n",
    "\n",
    "# --- 3b) **force** every sethop combination to exist (fill missing with 0) ---\n",
    "all_sets = list(total_counts.keys())\n",
    "all_hops = ['1','2','3','>3']\n",
    "idx = pd.MultiIndex.from_product([all_sets, all_hops], names=['set','hop'])\n",
    "freq_endocrine = (\n",
    "    freq_endocrine\n",
    "    .set_index(['set','hop'])\n",
    "    .reindex(idx, fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# --- 4) force ordering and define colors ---\n",
    "freq_endocrine['hop'] = pd.Categorical(\n",
    "    freq_endocrine['hop'],\n",
    "    categories=all_hops,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "endocrine_color_map = {\n",
    "    '1': 'red',\n",
    "    '2': 'yellow',\n",
    "    '3': 'blue',\n",
    "    '>3': 'grey'\n",
    "}\n",
    "\n",
    "# --- 5) plot the stacked bar ---\n",
    "fig_bar_endocrine = px.bar(\n",
    "    freq_endocrine,\n",
    "    x=\"set\",\n",
    "    y=\"count\",\n",
    "    color=\"hop\",\n",
    "    barmode=\"stack\",\n",
    "    category_orders={\"set\": all_sets, \"hop\": all_hops},\n",
    "    color_discrete_map=endocrine_color_map,\n",
    "    title=\"Stacked Bar Chart of Minimum Endocrine Hops by aPhN2SA Set\",\n",
    "    labels={\"set\":\"aPhN2SA Set\",\"count\":\"# Neurons\",\"hop\":\"Hops to Endocrine\"}\n",
    ")\n",
    "\n",
    "fig_bar_endocrine.update_layout(xaxis_title=None)\n",
    "fig_bar_endocrine.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422a7b85-e998-44b6-8f9f-1b7f3c38f9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aeb9ba-bc12-4fb2-a8d7-70002c35fd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42390ad7-49bc-463b-9e2d-da76bf2c47fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82dea8-655f-4729-acf2-100503bfbc52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46785a1-0812-42ed-9f44-dc697acf648b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e462f67-89fa-4fbb-a9d0-e7e830771681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f2fd607ac364c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T00:27:26.998902Z",
     "start_time": "2025-04-25T00:27:25.712997Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import colorsys\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions for colors\n",
    "# ---------------------------\n",
    "def get_color(i, n):\n",
    "    \"\"\"Generate a distinct RGB color for node index i among n nodes.\"\"\"\n",
    "    hue = i / n\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, 0.6, 0.9)\n",
    "    return f\"rgb({int(r*255)},{int(g*255)},{int(b*255)})\"\n",
    "\n",
    "def make_rgba(rgb_str, alpha=0.5):\n",
    "    \"\"\"Convert 'rgb(r,g,b)' to 'rgba(r,g,b,alpha)'.\"\"\"\n",
    "    r, g, b = rgb_str.lstrip(\"rgb(\").rstrip(\")\").split(\",\")\n",
    "    return f\"rgba({r},{g},{b},{alpha})\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load & concatenate all three sets at each hop level\n",
    "# ---------------------------\n",
    "df1 = pd.concat([\n",
    "    pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df1 = df1[df1['syn_count'] >= 5]\n",
    "\n",
    "df2 = pd.concat([\n",
    "    pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.concat([\n",
    "    pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Build flows (summing syn_count)\n",
    "# ---------------------------\n",
    "flow1 = (\n",
    "    df1.groupby('output_super_class')['syn_count']\n",
    "       .sum().reset_index(name='count')\n",
    ")\n",
    "flow1['source'] = \"All Sets\"\n",
    "\n",
    "m12 = pd.merge(df1, df2,\n",
    "               left_on='post_root_id', right_on='pre_root_id',\n",
    "               suffixes=('_1','_2'))\n",
    "flow2 = (\n",
    "    m12.groupby(['output_super_class_1','output_super_class_2'])\n",
    "       ['syn_count_2'].sum().reset_index(name='count')\n",
    ")\n",
    "\n",
    "m23 = pd.merge(df2, df3,\n",
    "               left_on='post_root_id', right_on='pre_root_id',\n",
    "               suffixes=('_2','_3'))\n",
    "flow3 = (\n",
    "    m23.groupby(['output_super_class_2','output_super_class_3'])\n",
    "       ['syn_count_3'].sum().reset_index(name='count')\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Build node lists for each column\n",
    "# ---------------------------\n",
    "col1 = [\"All Sets\"]\n",
    "col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Assign colors\n",
    "# ---------------------------\n",
    "n_nodes = len(nodes)\n",
    "node_colors = [get_color(i, n_nodes) for i in range(n_nodes)]\n",
    "link_colors = []\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Map flows to indices\n",
    "# ---------------------------\n",
    "# Flow1  01\n",
    "flow1['source_idx'] = node_index[\"All Sets\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'].map(lambda x: node_index[f\"1: {x}\"])\n",
    "# Flow2  12\n",
    "flow2['source_idx'] = flow2['output_super_class_1'].map(lambda x: node_index[f\"1: {x}\"])\n",
    "flow2['target_idx'] = flow2['output_super_class_2'].map(lambda x: node_index[f\"2: {x}\"])\n",
    "# Flow3  23\n",
    "flow3['source_idx'] = flow3['output_super_class_2'].map(lambda x: node_index[f\"2: {x}\"])\n",
    "flow3['target_idx'] = flow3['output_super_class_3'].map(lambda x: node_index[f\"3: {x}\"])\n",
    "\n",
    "source = (\n",
    "    flow1['source_idx'].tolist() +\n",
    "    flow2['source_idx'].tolist() +\n",
    "    flow3['source_idx'].tolist()\n",
    ")\n",
    "target = (\n",
    "    flow1['target_idx'].tolist() +\n",
    "    flow2['target_idx'].tolist() +\n",
    "    flow3['target_idx'].tolist()\n",
    ")\n",
    "value = (\n",
    "    flow1['count'].tolist() +\n",
    "    flow2['count'].tolist() +\n",
    "    flow3['count'].tolist()\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Build link colors\n",
    "# ---------------------------\n",
    "for s in source:\n",
    "    link_colors.append(make_rgba(node_colors[s], 0.5))\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Compute hover stats\n",
    "# ---------------------------\n",
    "node_in  = dict.fromkeys(nodes, 0)\n",
    "node_out = dict.fromkeys(nodes, 0)\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_out[nodes[s]] += v\n",
    "    node_in [nodes[t]] += v\n",
    "\n",
    "customdata = [\n",
    "    f\"Incoming: {node_in[n]}<br>\"\n",
    "    f\"Outgoing: {node_out[n]}<br>\"\n",
    "    f\"Total: {max(node_in[n], node_out[n])}\"\n",
    "    for n in nodes\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Define fixed xpositions\n",
    "# ---------------------------\n",
    "x_positions = (\n",
    "    [0.0]*len(col1) +\n",
    "    [0.33]*len(col2) +\n",
    "    [0.66]*len(col3) +\n",
    "    [1.0] *len(col4)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Draw the Sankey\n",
    "# ---------------------------\n",
    "fig = go.Figure(go.Sankey(\n",
    "    arrangement=\"snap\",\n",
    "    node=dict(\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        color=node_colors,\n",
    "        pad=20,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=link_colors\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Combined aPhN2-SA Sets: 3-Hop Superclass Sankey\",\n",
    "    font_size=12\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6dbeba-7e6f-494a-b990-7246aaf267bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e1e2354ba6318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T00:17:09.062285Z",
     "start_time": "2025-04-25T00:17:09.060138Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f01218f3791c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T00:17:09.282937Z",
     "start_time": "2025-04-25T00:17:09.280818Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd9a569c42267a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T00:36:54.690010Z",
     "start_time": "2025-04-25T00:36:53.317118Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import colorsys\n",
    "\n",
    "# ---------------------------\n",
    "# Helper functions for colors\n",
    "# ---------------------------\n",
    "def get_color(i, n):\n",
    "    \"\"\"Generate a distinct RGB color for index i out of n using HSV.\"\"\"\n",
    "    hue = i / n\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, 0.6, 0.9)\n",
    "    return f\"rgb({int(r*255)},{int(g*255)},{int(b*255)})\"\n",
    "\n",
    "def make_rgba(rgb_str, alpha=0.5):\n",
    "    \"\"\"Convert 'rgb(r,g,b)' to 'rgba(r,g,b,alpha)'; fallback for non-rgb.\"\"\"\n",
    "    if not rgb_str.startswith(\"rgb\"):\n",
    "        # fallback to light grey\n",
    "        return f\"rgba(200,200,200,{alpha})\"\n",
    "    r, g, b = rgb_str.lstrip(\"rgb(\").rstrip(\")\").split(\",\")\n",
    "    return f\"rgba({r},{g},{b},{alpha})\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load & concatenate all three sets at each hop level\n",
    "# ---------------------------\n",
    "df1 = pd.concat([\n",
    "    pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df1 = df1[df1['syn_count'] >= 5]\n",
    "\n",
    "df2 = pd.concat([\n",
    "    pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.concat([\n",
    "    pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Build flows by summing syn_count\n",
    "# ---------------------------\n",
    "flow1 = (\n",
    "    df1.groupby('output_super_class')['syn_count']\n",
    "       .sum().reset_index(name='count')\n",
    ")\n",
    "flow1['source'] = \"All Sets\"\n",
    "\n",
    "m12 = pd.merge(df1, df2,\n",
    "               left_on='post_root_id', right_on='pre_root_id',\n",
    "               suffixes=('_1','_2'))\n",
    "flow2 = (\n",
    "    m12.groupby(['output_super_class_1','output_super_class_2'])\n",
    "       ['syn_count_2'].sum().reset_index(name='count')\n",
    ")\n",
    "\n",
    "m23 = pd.merge(df2, df3,\n",
    "               left_on='post_root_id', right_on='pre_root_id',\n",
    "               suffixes=('_2','_3'))\n",
    "flow3 = (\n",
    "    m23.groupby(['output_super_class_2','output_super_class_3'])\n",
    "       ['syn_count_3'].sum().reset_index(name='count')\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Node lists for each column\n",
    "# ---------------------------\n",
    "col1 = [\"All Sets\"]\n",
    "col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "\n",
    "nodes = col1 + col2 + col3 + col4\n",
    "node_index = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Assign unique color per superclass from Safe palette\n",
    "# ---------------------------\n",
    "palette = px.colors.qualitative.Safe\n",
    "all_classes = sorted({\n",
    "    *df1['output_super_class'].unique(),\n",
    "    *df2['output_super_class'].unique(),\n",
    "    *df3['output_super_class'].unique()\n",
    "})\n",
    "color_map = {cls: palette[i % len(palette)] for i, cls in enumerate(all_classes)}\n",
    "\n",
    "node_colors = []\n",
    "for label in nodes:\n",
    "    if label == \"All Sets\":\n",
    "        node_colors.append(\"lightgrey\")\n",
    "    else:\n",
    "        cls = label.split(\": \",1)[1]\n",
    "        node_colors.append(color_map[cls])\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Map flows to source/target indices\n",
    "# ---------------------------\n",
    "flow1['source_idx'] = node_index[\"All Sets\"]\n",
    "flow1['target_idx'] = flow1['output_super_class'] \\\n",
    "                      .map(lambda x: node_index[f\"1: {x}\"])\n",
    "\n",
    "flow2['source_idx'] = flow2['output_super_class_1'] \\\n",
    "                      .map(lambda x: node_index[f\"1: {x}\"])\n",
    "flow2['target_idx'] = flow2['output_super_class_2'] \\\n",
    "                      .map(lambda x: node_index[f\"2: {x}\"])\n",
    "\n",
    "flow3['source_idx'] = flow3['output_super_class_2'] \\\n",
    "                      .map(lambda x: node_index[f\"2: {x}\"])\n",
    "flow3['target_idx'] = flow3['output_super_class_3'] \\\n",
    "                      .map(lambda x: node_index[f\"3: {x}\"])\n",
    "\n",
    "source = (\n",
    "    flow1['source_idx'].tolist() +\n",
    "    flow2['source_idx'].tolist() +\n",
    "    flow3['source_idx'].tolist()\n",
    ")\n",
    "target = (\n",
    "    flow1['target_idx'].tolist() +\n",
    "    flow2['target_idx'].tolist() +\n",
    "    flow3['target_idx'].tolist()\n",
    ")\n",
    "value = (\n",
    "    flow1['count'].tolist() +\n",
    "    flow2['count'].tolist() +\n",
    "    flow3['count'].tolist()\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Build link colors (50% alpha from node color)\n",
    "# ---------------------------\n",
    "link_colors = [make_rgba(node_colors[s], 0.5) for s in source]\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Compute hover info\n",
    "# ---------------------------\n",
    "node_in  = dict.fromkeys(nodes, 0)\n",
    "node_out = dict.fromkeys(nodes, 0)\n",
    "for s, t, v in zip(source, target, value):\n",
    "    node_out[nodes[s]] += v\n",
    "    node_in [nodes[t]] += v\n",
    "\n",
    "customdata = [\n",
    "    f\"Incoming: {node_in[n]}<br>Outgoing: {node_out[n]}<br>\"\n",
    "    f\"Total: {max(node_in[n], node_out[n])}\"\n",
    "    for n in nodes\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Define x-positions for columns\n",
    "# ---------------------------\n",
    "x_positions = (\n",
    "    [0.0]*len(col1) +\n",
    "    [0.33]*len(col2) +\n",
    "    [0.66]*len(col3) +\n",
    "    [1.0] *len(col4)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 9) Draw the Sankey with arrangement=\"snap\"\n",
    "# ---------------------------\n",
    "fig = go.Figure(go.Sankey(\n",
    "    arrangement=\"snap\",\n",
    "    node=dict(\n",
    "        label=nodes,\n",
    "        x=x_positions,\n",
    "        color=node_colors,\n",
    "        pad=20,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        customdata=customdata,\n",
    "        hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source,\n",
    "        target=target,\n",
    "        value=value,\n",
    "        color=link_colors\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"aPhN2-SA Sankey Diagram\",\n",
    "    font_size=20\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761c695-1ac5-4a55-9d38-7006c05e7755",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as PNG\n",
    "fig.write_image(\"aphn2_sa_sankey.png\", width=1200, height=800, scale=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fbae85211aedd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T00:38:58.088532Z",
     "start_time": "2025-04-25T00:38:58.021542Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Load & concatenate all three sets at each hop level\n",
    "df1 = pd.concat([\n",
    "    pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df1 = df1[df1['syn_count'] >= 5]\n",
    "\n",
    "df2 = pd.concat([\n",
    "    pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df2 = df2[df2['syn_count'] >= 5]\n",
    "\n",
    "df3 = pd.concat([\n",
    "    pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "    pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\")\n",
    "], ignore_index=True)\n",
    "df3 = df3[df3['syn_count'] >= 5]\n",
    "\n",
    "# 2) Build the full list of superclasses\n",
    "all_classes = sorted(\n",
    "    set(df1.output_super_class)\n",
    "  | set(df2.output_super_class)\n",
    "  | set(df3.output_super_class)\n",
    ")\n",
    "\n",
    "# 3) Build the three heatmap matrices\n",
    "\n",
    "# Hop 0: \"All Sets\"  superclass\n",
    "flow0 = df1.groupby('output_super_class')['syn_count'] \\\n",
    "           .sum() \\\n",
    "           .reindex(all_classes, fill_value=0)\n",
    "mat0 = pd.DataFrame([flow0.values],\n",
    "                    index=[\"All Sets\"],\n",
    "                    columns=all_classes)\n",
    "\n",
    "# Hop 1: superclass  superclass\n",
    "m12 = pd.merge(\n",
    "    df1[['post_root_id','output_super_class','syn_count']],\n",
    "    df2[['pre_root_id','output_super_class','syn_count']],\n",
    "    left_on='post_root_id', right_on='pre_root_id',\n",
    "    suffixes=('_from','_to')\n",
    ")\n",
    "mat1 = ( m12\n",
    "    .groupby(['output_super_class_from','output_super_class_to'])['syn_count_to']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(index=all_classes, columns=all_classes, fill_value=0)\n",
    ")\n",
    "\n",
    "# Hop 2: superclass  superclass\n",
    "m23 = pd.merge(\n",
    "    df2[['post_root_id','output_super_class','syn_count']],\n",
    "    df3[['pre_root_id','output_super_class','syn_count']],\n",
    "    left_on='post_root_id', right_on='pre_root_id',\n",
    "    suffixes=('_from','_to')\n",
    ")\n",
    "mat2 = ( m23\n",
    "    .groupby(['output_super_class_from','output_super_class_to'])['syn_count_to']\n",
    "    .sum()\n",
    "    .unstack(fill_value=0)\n",
    "    .reindex(index=all_classes, columns=all_classes, fill_value=0)\n",
    ")\n",
    "\n",
    "# 4) Plotting\n",
    "sns.set_theme(style='white')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Hop 0 heatmap\n",
    "vmax0 = mat0.values.max()\n",
    "sns.heatmap(\n",
    "    mat0,\n",
    "    cmap='turbo',\n",
    "    annot=False,\n",
    "    square=True,\n",
    "    vmin=0, vmax=vmax0,\n",
    "    cbar_kws={'label':'synapse count'},\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Hop 0: All Sets  Superclass\")\n",
    "axes[0].set_ylabel(\"Source\")\n",
    "axes[0].set_xlabel(\"Target Superclass\")\n",
    "\n",
    "# Hop 1 heatmap\n",
    "vmax1 = mat1.values.max()\n",
    "sns.heatmap(\n",
    "    mat1,\n",
    "    cmap='turbo',\n",
    "    annot=False,\n",
    "    square=True,\n",
    "    vmin=0, vmax=vmax1,\n",
    "    cbar_kws={'label':'synapse count'},\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Hop 1: Superclass  Superclass\")\n",
    "axes[1].set_ylabel(\"Source Superclass\")\n",
    "axes[1].set_xlabel(\"Target Superclass\")\n",
    "\n",
    "# Hop 2 heatmap\n",
    "vmax2 = mat2.values.max()\n",
    "sns.heatmap(\n",
    "    mat2,\n",
    "    cmap='turbo',\n",
    "    annot=False,\n",
    "    square=True,\n",
    "    vmin=0, vmax=vmax2,\n",
    "    cbar_kws={'label':'synapse count'},\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Hop 2: Superclass  Superclass\")\n",
    "axes[2].set_ylabel(\"Source Superclass\")\n",
    "axes[2].set_xlabel(\"Target Superclass\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e466d455ffbcc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14386af-08d6-48ec-8b2b-68fe6163622e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "def analyze_target_hops_combined(target_class=\"motor\"):\n",
    "    \"\"\"\n",
    "    Loads 0-, 1-, and 2-hop CSVs for sets 13, concatenates them,\n",
    "    then finds for each pre_root_id the minimum hop (1, 2, or 3)\n",
    "    at which output_super_class == target_class.\n",
    "    Returns a DataFrame with ['pre_root_id','hop'].\n",
    "    \"\"\"\n",
    "    # 1) 0-hop (direct) across all sets\n",
    "    df1 = pd.concat([\n",
    "        pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 2) 1-hop outputs\n",
    "    df2 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 3) 2-hop outputs\n",
    "    df3 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # --- hop 1 hits ---\n",
    "    df1_t = (\n",
    "        df1[df1[\"output_super_class\"] == target_class]\n",
    "           [[\"pre_root_id\"]]\n",
    "           .drop_duplicates()\n",
    "           .assign(hop=1)\n",
    "    )\n",
    "\n",
    "    # --- hop 2 hits ---\n",
    "    df12 = pd.merge(\n",
    "        df1[[\"pre_root_id\",\"post_root_id\"]],\n",
    "        df2,\n",
    "        left_on=\"post_root_id\",\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_1\",\"_2\")\n",
    "    )\n",
    "    start_col = \"pre_root_id_1\" if \"pre_root_id_1\" in df12 else \"pre_root_id\"\n",
    "    df12_t = (\n",
    "        df12[df12[\"output_super_class\"] == target_class]\n",
    "            [[start_col]]\n",
    "            .drop_duplicates()\n",
    "            .rename(columns={start_col:\"pre_root_id\"})\n",
    "            .assign(hop=2)\n",
    "    )\n",
    "\n",
    "    # --- hop 3 hits ---\n",
    "    post2 = \"post_root_id_2\" if \"post_root_id_2\" in df12 else \"post_root_id\"\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=post2,\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_2\",\"_3\")\n",
    "    )\n",
    "    out3 = \"output_super_class_3\" if \"output_super_class_3\" in df123 else \"output_super_class\"\n",
    "    df123_t = (\n",
    "        df123[df123[out3] == target_class]\n",
    "             [[start_col]]\n",
    "             .drop_duplicates()\n",
    "             .rename(columns={start_col:\"pre_root_id\"})\n",
    "             .assign(hop=3)\n",
    "    )\n",
    "\n",
    "    # --- combine and pick minimum hop per neuron ---\n",
    "    all_hits = pd.concat([df1_t, df12_t, df123_t], ignore_index=True)\n",
    "    min_hops = all_hits.groupby(\"pre_root_id\", as_index=False)[\"hop\"].min()\n",
    "    return min_hops\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # compute minimum hops to motor\n",
    "    min_h = analyze_target_hops_combined(\"motor\")\n",
    "\n",
    "    # turn hop into string for discrete coloring\n",
    "    min_h[\"hop_str\"] = min_h[\"hop\"].astype(int).astype(str)\n",
    "\n",
    "    # --- stacked bar chart: one bar, segments = hop1,2,3 counts ---\n",
    "    freq = (\n",
    "        min_h\n",
    "        .groupby(\"hop_str\")\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .rename(columns={\"hop_str\":\"hop\"})\n",
    "    )\n",
    "    # add a dummy x-axis so plotly will stack\n",
    "    freq[\"all_sets\"] = \"All Sets\"\n",
    "\n",
    "    fig_bar = px.bar(\n",
    "        freq,\n",
    "        x=\"all_sets\",\n",
    "        y=\"count\",\n",
    "        color=\"hop\",\n",
    "        barmode=\"stack\",\n",
    "        title=\"All-Sets Minimum Motor Hops (Stacked)\",\n",
    "        labels={\n",
    "            \"all_sets\": \"\",\n",
    "            \"count\": \"Number of Neurons\",\n",
    "            \"hop\": \"Hops to Motor\"\n",
    "        }\n",
    "    )\n",
    "    fig_bar.update_layout(xaxis={\"visible\": False})\n",
    "    fig_bar.show()\n",
    "\n",
    "    # --- violin plot of hop distribution ---\n",
    "    fig_violin = px.violin(\n",
    "        min_h,\n",
    "        y=\"hop_str\",\n",
    "        box=True,\n",
    "        points=\"all\",\n",
    "        title=\"Distribution of Minimum Motor Hops (All Sets)\",\n",
    "        labels={\"hop_str\":\"Hops to Motor\"}\n",
    "    )\n",
    "    fig_violin.update_traces(side=\"positive\")\n",
    "    fig_violin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9629cf5f-e700-475c-8529-7104344ec51d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_target_hops_combined(target_class=\"motor\"):\n",
    "    \"\"\"\n",
    "    Loads 0-, 1-, and 2-hop CSVs for sets 13, concatenates them,\n",
    "    then finds for each pre_root_id the minimum hop (1, 2, or 3)\n",
    "    at which output_super_class == target_class.\n",
    "    Returns a DataFrame with columns ['pre_root_id','hop'].\n",
    "    \"\"\"\n",
    "    # 0-hop (direct)\n",
    "    df1 = pd.concat([\n",
    "        pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 1-hop\n",
    "    df2 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 2-hop\n",
    "    df3 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # --- hop=1 hits ---\n",
    "    df1_t = (\n",
    "        df1[df1[\"output_super_class\"] == target_class]\n",
    "           [[\"pre_root_id\"]]\n",
    "           .drop_duplicates()\n",
    "           .assign(hop=1)\n",
    "    )\n",
    "\n",
    "    # --- hop=2 hits via df1df2 merge ---\n",
    "    df12 = pd.merge(\n",
    "        df1[[\"pre_root_id\",\"post_root_id\"]],\n",
    "        df2,\n",
    "        left_on=\"post_root_id\",\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_1\",\"_2\")\n",
    "    )\n",
    "    start_col = \"pre_root_id_1\" if \"pre_root_id_1\" in df12 else \"pre_root_id\"\n",
    "    df12_t = (\n",
    "        df12[df12[\"output_super_class\"] == target_class]\n",
    "            [[start_col]]\n",
    "            .drop_duplicates()\n",
    "            .rename(columns={start_col:\"pre_root_id\"})\n",
    "            .assign(hop=2)\n",
    "    )\n",
    "\n",
    "    # --- hop=3 hits via df12df3 merge ---\n",
    "    post2 = \"post_root_id_2\" if \"post_root_id_2\" in df12 else \"post_root_id\"\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=post2,\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_2\",\"_3\")\n",
    "    )\n",
    "    out3 = \"output_super_class_3\" if \"output_super_class_3\" in df123 else \"output_super_class\"\n",
    "    df123_t = (\n",
    "        df123[df123[out3] == target_class]\n",
    "             [[start_col]]\n",
    "             .drop_duplicates()\n",
    "             .rename(columns={start_col:\"pre_root_id\"})\n",
    "             .assign(hop=3)\n",
    "    )\n",
    "\n",
    "    # --- combine and take minimum hop per neuron ---\n",
    "    all_hits = pd.concat([df1_t, df12_t, df123_t], ignore_index=True)\n",
    "    min_hops = all_hits.groupby(\"pre_root_id\", as_index=False)[\"hop\"].min()\n",
    "    return min_hops\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Compute min hops to motor across all sets\n",
    "    min_h = analyze_target_hops_combined(\"motor\")\n",
    "    # For plotting purposes\n",
    "    min_h[\"hop_str\"] = min_h[\"hop\"].astype(int).astype(str)\n",
    "\n",
    "    # --- Prepare frequency table for stacked bar ---\n",
    "    freq = (\n",
    "        min_h[\"hop_str\"]\n",
    "        .value_counts()\n",
    "        .sort_index(key=lambda idx: idx.astype(int))\n",
    "        .rename_axis(\"hop\")\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "    # single x-position\n",
    "    freq[\"dummy\"] = \"All Sets\"\n",
    "\n",
    "    # --- Plot with seaborn/matplotlib ---\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, (ax_bar, ax_viol) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # 1) Stacked bar chart\n",
    "    bottom = np.zeros(1)\n",
    "    hops = freq[\"hop\"].tolist()\n",
    "    for hop in hops:\n",
    "        cnt = freq.loc[freq.hop == hop, \"count\"].values\n",
    "        ax_bar.bar(\n",
    "            x=[\"All Sets\"],\n",
    "            height=cnt,\n",
    "            bottom=bottom,\n",
    "            label=f\"{hop} hop\" + (\"s\" if hop != \"1\" else \"\")\n",
    "        )\n",
    "        bottom += cnt  # update bottom for next segment\n",
    "\n",
    "    ax_bar.set_ylabel(\"Number of Neurons\")\n",
    "    ax_bar.set_title(\"All-Sets Minimum Motor Hops (Stacked)\")\n",
    "    ax_bar.legend(title=\"Hops to Motor\")\n",
    "    ax_bar.set_xticks([])  # hide the single x-tick\n",
    "\n",
    "    # 2) Violin plot of hop distribution\n",
    "    sns.violinplot(\n",
    "        y=\"hop_str\",\n",
    "        data=min_h,\n",
    "        inner=\"box\",\n",
    "        ax=ax_viol,\n",
    "        \n",
    "\n",
    "    )\n",
    "\n",
    "    ax_bar.legend(\n",
    "        title=\"Hops to Motor\",\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.05, 1)\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "    ax_viol.set_ylabel(\"Hops to Motor\")\n",
    "    ax_viol.set_title(\"Distribution of Minimum Motor Hops\\n(All Sets)\")\n",
    "    ax_viol.invert_yaxis()\n",
    "\n",
    "    \n",
    "    # tighten but leave space on the right\n",
    "    plt.tight_layout(rect=[0,0,0.85,1])    # [left, bottom, right, top] in figure coords\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b533584f-7d6f-4a18-b0bd-73fec7ae5a45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_target_hops_combined(target_class=\"motor\"):\n",
    "    \"\"\"\n",
    "    Loads 0-, 1-, and 2-hop CSVs for sets 13, concatenates them,\n",
    "    then finds for each pre_root_id the minimum hop (1, 2, or 3)\n",
    "    at which output_super_class == target_class.\n",
    "    Returns a DataFrame with columns ['pre_root_id','hop'].\n",
    "    \"\"\"\n",
    "    # 0-hop (direct)\n",
    "    df1 = pd.concat([\n",
    "        pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 1-hop\n",
    "    df2 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 2-hop\n",
    "    df3 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # --- hop=1 hits ---\n",
    "    df1_t = (\n",
    "        df1[df1[\"output_super_class\"] == target_class]\n",
    "           [[\"pre_root_id\"]]\n",
    "           .drop_duplicates()\n",
    "           .assign(hop=1)\n",
    "    )\n",
    "\n",
    "    # --- hop=2 hits via df1df2 merge ---\n",
    "    df12 = pd.merge(\n",
    "        df1[[\"pre_root_id\",\"post_root_id\"]],\n",
    "        df2,\n",
    "        left_on=\"post_root_id\",\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_1\",\"_2\")\n",
    "    )\n",
    "    start_col = \"pre_root_id_1\" if \"pre_root_id_1\" in df12 else \"pre_root_id\"\n",
    "    df12_t = (\n",
    "        df12[df12[\"output_super_class\"] == target_class]\n",
    "            [[start_col]]\n",
    "            .drop_duplicates()\n",
    "            .rename(columns={start_col:\"pre_root_id\"})\n",
    "            .assign(hop=2)\n",
    "    )\n",
    "\n",
    "    # --- hop=3 hits via df12df3 merge ---\n",
    "    post2 = \"post_root_id_2\" if \"post_root_id_2\" in df12 else \"post_root_id\"\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=post2,\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_2\",\"_3\")\n",
    "    )\n",
    "    out3 = \"output_super_class_3\" if \"output_super_class_3\" in df123 else \"output_super_class\"\n",
    "    df123_t = (\n",
    "        df123[df123[out3] == target_class]\n",
    "             [[start_col]]\n",
    "             .drop_duplicates()\n",
    "             .rename(columns={start_col:\"pre_root_id\"})\n",
    "             .assign(hop=3)\n",
    "    )\n",
    "\n",
    "    # --- combine and take minimum hop per neuron ---\n",
    "    all_hits = pd.concat([df1_t, df12_t, df123_t], ignore_index=True)\n",
    "    min_hops = all_hits.groupby(\"pre_root_id\", as_index=False)[\"hop\"].min()\n",
    "    return min_hops\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Compute min hops to motor\n",
    "    min_h = analyze_target_hops_combined(\"motor\")\n",
    "\n",
    "    # 2) Stringlabel the hops\n",
    "    min_h[\"hop_str\"] = min_h[\"hop\"].astype(int).astype(str)\n",
    "\n",
    "    # 3) Detect any neurons that *never* hit motor in 13 hops, tag as \">3\"\n",
    "    all_pres = pd.concat([\n",
    "        pd.read_csv(\"set_1_opt_conns_superclass.csv\")[[\"pre_root_id\"]],\n",
    "        pd.read_csv(\"set_2_opt_conns_superclass.csv\")[[\"pre_root_id\"]],\n",
    "        pd.read_csv(\"set_3_opt_conns_superclass.csv\")[[\"pre_root_id\"]],\n",
    "    ], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    missed = all_pres.loc[~all_pres[\"pre_root_id\"].isin(min_h[\"pre_root_id\"]), [\"pre_root_id\"]]\n",
    "    if not missed.empty:\n",
    "        missed = missed.copy()\n",
    "        missed[\"hop_str\"] = \">3\"\n",
    "        # only keep the string column for consistency\n",
    "        min_h = pd.concat([min_h[[\"pre_root_id\",\"hop_str\"]], missed], ignore_index=True)\n",
    "\n",
    "    # 4) Build frequency table and *force* all four categorieseven if zero\n",
    "    categories = [\"1\",\"2\",\"3\",\">3\"]\n",
    "    freq = (\n",
    "        min_h[\"hop_str\"]\n",
    "             .value_counts()\n",
    "             .rename_axis(\"hop\")\n",
    "             .reset_index(name=\"count\")\n",
    "             .set_index(\"hop\")\n",
    "             .reindex(categories, fill_value=0)\n",
    "             .reset_index()\n",
    "    )\n",
    "    freq[\"dummy\"] = \"All Sets\"\n",
    "\n",
    "    # 5) Plot\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, (ax_bar) = plt.subplots(1, figsize=(12, 5))\n",
    "\n",
    "    # ---- Stacked bar chart ----\n",
    "    bottom = np.zeros(1)\n",
    "    for hop in categories:\n",
    "        cnt = freq.loc[freq.hop == hop, \"count\"].values\n",
    "        ax_bar.bar(\n",
    "            x=[\"All Sets\"],\n",
    "            height=cnt,\n",
    "            bottom=bottom,\n",
    "            label=f\"{hop} hop\" + (\"s\" if hop != \"1\" else \"\")\n",
    "        )\n",
    "        bottom += cnt\n",
    "\n",
    "    ax_bar.set_ylabel(\"Number of Neurons\")\n",
    "    ax_bar.set_title(\"aPhN2 Motor Hops\")\n",
    "    ax_bar.set_xticks([])  # hide the single x-tick\n",
    "    ax_bar.legend(\n",
    "        title=\"Hops to Motor\",\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.05, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 6) Layout tweak\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # leave room on the right for legend\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6077d-c567-47ae-a514-6e3e24b42731",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_target_hops_combined(target_class=\"endocrine\"):\n",
    "    \"\"\"\n",
    "    Loads 0-, 1-, and 2-hop CSVs for sets 13, concatenates them,\n",
    "    then finds for each pre_root_id the minimum hop (1, 2, or 3)\n",
    "    at which output_super_class == target_class.\n",
    "    Returns a DataFrame with columns ['pre_root_id','hop'].\n",
    "    \"\"\"\n",
    "    # 0-hop (direct)\n",
    "    df1 = pd.concat([\n",
    "        pd.read_csv(\"set_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 1-hop\n",
    "    df2 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_1_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_1_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # 2-hop\n",
    "    df3 = pd.concat([\n",
    "        pd.read_csv(\"set_1_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_2_hop_2_opt_conns_superclass.csv\"),\n",
    "        pd.read_csv(\"set_3_hop_2_opt_conns_superclass.csv\"),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # --- hop=1 hits ---\n",
    "    df1_t = (\n",
    "        df1[df1[\"output_super_class\"] == target_class]\n",
    "           [[\"pre_root_id\"]]\n",
    "           .drop_duplicates()\n",
    "           .assign(hop=1)\n",
    "    )\n",
    "\n",
    "    # --- hop=2 hits via df1df2 merge ---\n",
    "    df12 = pd.merge(\n",
    "        df1[[\"pre_root_id\",\"post_root_id\"]],\n",
    "        df2,\n",
    "        left_on=\"post_root_id\",\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_1\",\"_2\")\n",
    "    )\n",
    "    start_col = \"pre_root_id_1\" if \"pre_root_id_1\" in df12 else \"pre_root_id\"\n",
    "    df12_t = (\n",
    "        df12[df12[\"output_super_class\"] == target_class]\n",
    "            [[start_col]]\n",
    "            .drop_duplicates()\n",
    "            .rename(columns={start_col:\"pre_root_id\"})\n",
    "            .assign(hop=2)\n",
    "    )\n",
    "\n",
    "    # --- hop=3 hits via df12df3 merge ---\n",
    "    post2 = \"post_root_id_2\" if \"post_root_id_2\" in df12 else \"post_root_id\"\n",
    "    df123 = pd.merge(\n",
    "        df12,\n",
    "        df3,\n",
    "        left_on=post2,\n",
    "        right_on=\"pre_root_id\",\n",
    "        suffixes=(\"_2\",\"_3\")\n",
    "    )\n",
    "    out3 = \"output_super_class_3\" if \"output_super_class_3\" in df123 else \"output_super_class\"\n",
    "    df123_t = (\n",
    "        df123[df123[out3] == target_class]\n",
    "             [[start_col]]\n",
    "             .drop_duplicates()\n",
    "             .rename(columns={start_col:\"pre_root_id\"})\n",
    "             .assign(hop=3)\n",
    "    )\n",
    "\n",
    "    # --- combine and take minimum hop per neuron ---\n",
    "    all_hits = pd.concat([df1_t, df12_t, df123_t], ignore_index=True)\n",
    "    min_hops = all_hits.groupby(\"pre_root_id\", as_index=False)[\"hop\"].min()\n",
    "    return min_hops\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Compute min hops to endocrine\n",
    "    min_h = analyze_target_hops_combined(\"endocrine\")\n",
    "\n",
    "    # 2) Stringlabel the hops\n",
    "    min_h[\"hop_str\"] = min_h[\"hop\"].astype(int).astype(str)\n",
    "\n",
    "    # 3) Detect any neurons that *never* hit endocrine in 13 hops, tag as \">3\"\n",
    "    all_pres = pd.concat([\n",
    "        pd.read_csv(\"set_1_opt_conns_superclass.csv\")[[\"pre_root_id\"]],\n",
    "        pd.read_csv(\"set_2_opt_conns_superclass.csv\")[[\"pre_root_id\"]],\n",
    "        pd.read_csv(\"set_3_opt_conns_superclass.csv\")[[\"pre_root_id\"]],\n",
    "    ], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    missed = all_pres.loc[~all_pres[\"pre_root_id\"].isin(min_h[\"pre_root_id\"]), [\"pre_root_id\"]]\n",
    "    if not missed.empty:\n",
    "        missed = missed.copy()\n",
    "        missed[\"hop_str\"] = \">3\"\n",
    "        # only keep the string column for consistency\n",
    "        min_h = pd.concat([min_h[[\"pre_root_id\",\"hop_str\"]], missed], ignore_index=True)\n",
    "\n",
    "    # 4) Build frequency table and *force* all four categorieseven if zero\n",
    "    categories = [\"1\",\"2\",\"3\",\">3\"]\n",
    "    freq = (\n",
    "        min_h[\"hop_str\"]\n",
    "             .value_counts()\n",
    "             .rename_axis(\"hop\")\n",
    "             .reset_index(name=\"count\")\n",
    "             .set_index(\"hop\")\n",
    "             .reindex(categories, fill_value=0)\n",
    "             .reset_index()\n",
    "    )\n",
    "    freq[\"dummy\"] = \"All Sets\"\n",
    "\n",
    "    # 5) Plot\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, (ax_bar) = plt.subplots(1, figsize=(12, 5))\n",
    "\n",
    "    # ---- Stacked bar chart ----\n",
    "    bottom = np.zeros(1)\n",
    "    for hop in categories:\n",
    "        cnt = freq.loc[freq.hop == hop, \"count\"].values\n",
    "        ax_bar.bar(\n",
    "            x=[\"All Sets\"],\n",
    "            height=cnt,\n",
    "            bottom=bottom,\n",
    "            label=f\"{hop} hop\" + (\"s\" if hop != \"1\" else \"\")\n",
    "        )\n",
    "        bottom += cnt\n",
    "\n",
    "    ax_bar.set_ylabel(\"Number of Neurons\")\n",
    "    ax_bar.set_title(\"aPhN2 Endocrine Hops\")\n",
    "    ax_bar.set_xticks([])  # hide the single x-tick\n",
    "    ax_bar.legend(\n",
    "        title=\"Hops to Endocrine\",\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(1.05, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 6) Layout tweak\n",
    "    plt.tight_layout(rect=[0, 0, 0.85, 1])  # leave room on the right for legend\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f3d03-8028-4c50-b036-376972312bdb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build a single DataFrame with a hop column\n",
    "all_summaries = []\n",
    "for df, hop in [(df1,0),(df2,1),(df3,2)]:\n",
    "    s = (df.groupby(\"output_super_class\")[\"syn_count\"]\n",
    "           .sum()\n",
    "           .reset_index(name=\"syn_count\"))\n",
    "    s[\"hop\"] = f\"hop {hop}\"\n",
    "    s[\"pct\"] = 100 * s[\"syn_count\"] / s[\"syn_count\"].sum()\n",
    "    all_summaries.append(s)\n",
    "\n",
    "big = pd.concat(all_summaries, ignore_index=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    big,\n",
    "    x=\"output_super_class\",\n",
    "    y=\"pct\",\n",
    "    color=\"hop\",\n",
    "    barmode=\"group\",\n",
    "    title=\"Synapse % by Superclass Across Hops\",\n",
    "    labels={\"output_super_class\":\"Superclass\",\"pct\":\"% of synapses\"},\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=45)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a32a1-2b5f-42d4-918d-4c78ee2b2f71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# 1) Build the percent summary for each hop\n",
    "all_hops = []\n",
    "for df, hop in [(df1, \"All SetsSC\"), (df2, \"1st hopSC\"), (df3, \"2nd hopSC\")]:\n",
    "    s = (\n",
    "        df.groupby(\"output_super_class\")[\"syn_count\"]\n",
    "          .sum()\n",
    "          .reset_index(name=\"syn_count\")\n",
    "    )\n",
    "    total = s[\"syn_count\"].sum()\n",
    "    s[\"pct\"] = 100 * s[\"syn_count\"] / total\n",
    "    s[\"hop\"] = hop\n",
    "    all_hops.append(s)\n",
    "\n",
    "big = pd.concat(all_hops, ignore_index=True)\n",
    "\n",
    "# 2) Plot a stacked bar chart\n",
    "fig = px.bar(\n",
    "    big,\n",
    "    x=\"hop\",\n",
    "    y=\"pct\",\n",
    "    color=\"output_super_class\",\n",
    "    title=\"Stacked Bar: % of Synapses by Superclass Across Hops\",\n",
    "    labels={\n",
    "        \"hop\": \"Connection stage\",\n",
    "        \"pct\": \"% of synapses\",\n",
    "        \"output_super_class\": \"Superclass\"\n",
    "    },\n",
    "    category_orders={\"hop\": [\"All SetsSC\", \"1st hopSC\", \"2nd hopSC\"]}\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    xaxis_title=None,\n",
    "    yaxis_range=[0,100],\n",
    "    legend_title_text=\"Superclass\",\n",
    "    yaxis=dict(ticksuffix=\"%\")\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a6e6e-7848-4c5e-b257-7613d98dc2c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# 1) Build the rawcount summary for each hop\n",
    "all_hops = []\n",
    "for df, hop in [\n",
    "    (df1, \"All Sets  SC\"),\n",
    "    (df2, \"1st hop  SC\"),\n",
    "    (df3, \"2nd hop  SC\")\n",
    "]:\n",
    "    s = (\n",
    "        df.groupby(\"output_super_class\")[\"syn_count\"]\n",
    "          .sum()\n",
    "          .reset_index(name=\"syn_count\")\n",
    "    )\n",
    "    s[\"hop\"] = hop\n",
    "    all_hops.append(s)\n",
    "\n",
    "big = pd.concat(all_hops, ignore_index=True)\n",
    "\n",
    "# 2) Plot a stacked bar chart of raw counts\n",
    "fig = px.bar(\n",
    "    big,\n",
    "    x=\"hop\",\n",
    "    y=\"syn_count\",\n",
    "    color=\"output_super_class\",\n",
    "    title=\"Stacked Bar: Synapse Counts by Superclass Across Hops\",\n",
    "    labels={\n",
    "        \"hop\": \"Connection stage\",\n",
    "        \"syn_count\": \"Total synapses\",\n",
    "        \"output_super_class\": \"Superclass\"\n",
    "    },\n",
    "    category_orders={\"hop\": [\"All Sets  SC\", \"1st hop  SC\", \"2nd hop  SC\"]}\n",
    ")\n",
    "fig.update_layout(\n",
    "    barmode=\"stack\",\n",
    "    xaxis_title=None,\n",
    "    legend_title_text=\"Superclass\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a33de-7f10-4e0d-be68-b6e074918bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8860abaa-2ff9-4b93-b96a-b4ce8abdf437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
