{
 "cells": [
  {
   "cell_type": "code",
   "id": "c024e6bb9468e829",
   "metadata": {},
   "source": [
    "#from aPhN-SA_Activation import set_1\n",
    "#%pip install statsmodels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a59a8205b55b56a3",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.optimize import curve_fit\n",
    "from venn import venn\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "import statsmodels.api as sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed15e32705ec0c18",
   "metadata": {},
   "source": [
    "# Set seaborn theme to white\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "# set up matplot lib theme\n",
    "andy_theme = {'axes.grid': False,\n",
    "              'grid.linestyle': '--',\n",
    "              'legend.framealpha': 1,\n",
    "              'legend.facecolor': 'white',\n",
    "              'legend.shadow': False,\n",
    "              'legend.fontsize': 14,\n",
    "              'legend.title_fontsize': 14,\n",
    "              'font.sans-serif':'Helvetica',\n",
    "              'xtick.labelsize': 8,\n",
    "              'ytick.labelsize': 8,\n",
    "              'axes.labelsize': 12,\n",
    "              'axes.titlesize': 16,\n",
    "              'figure.dpi': 300}\n",
    "\n",
    "plt.rcParams.update(andy_theme)\n",
    "\n",
    "#Uncomment next 2 lines if matplotlib can not find Helvetica font\n",
    "#plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "#plt.rcParams['font.sans-serif'] = ['Arial']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a520e2bb55877a9",
   "metadata": {},
   "source": [
    "## 1. FIRST ORDER ANALYSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e79f124baa017",
   "metadata": {},
   "source": [
    "### Load the datasets with neurons and connections.\n",
    "\n",
    "* This script assumes that the **CSV** files (`.csv.gz`) and **aPhN-SAs lists** (`_new.csv` files) are in the same folder as this notebook or script.\n",
    "* These files include four CSVs containing manually curated  aPhN-SAs lists and four connectome datasets from FlyWire:\n",
    "  1. **`classification.csv.gz`**\n",
    "  2. **`connections.csv.gz`**\n",
    "  3. **`neuropil_synapse_table.csv.gz`**\n",
    "  4. **`neurons.csv.gz`**\n",
    "* **Axon lists** were curated manually as described in the paper.\n",
    "* **Connectome datasets** were downloaded from the FlyWire website using **snapshot 783** (previous snapshot 630).\n",
    "* We focus on putative sensory axons from the Drosophila **pharyngeal nerve** in this analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1a3180cd7597f975",
   "metadata": {},
   "source": [
    "# Connections dataset and additional data sets\n",
    "\n",
    "# Load the connections dataset\n",
    "# columns: pre_root_id, post_root_id, neuropil, syn_count, nt_type\n",
    "connections = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz')\n",
    "\n",
    "# Neuropil synapses\n",
    "# columns: root_id, input synapses, input partners, output synapses, output partners, etc\n",
    "# Keep only root_id, input syanapses, output synapses\n",
    "neuropil_synapse = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/neuropil_synapse_table.csv.gz')[['root_id', 'input synapses', 'output synapses']]\n",
    "\n",
    "# Rename with underscores\n",
    "neuropil_synapse.rename(columns={'input synapses': 'input_synapses','output synapses': 'output_synapses'}, inplace=True)\n",
    "\n",
    "# Load classification table\n",
    "# columns: root_id, flow, super_class, side, etc\n",
    "# Keep only root_id and side\n",
    "classification = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz')[['root_id', 'side']]\n",
    "classification_other = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz')[['root_id', 'super_class', 'class']]\n",
    "\n",
    "# Load data about each neuron\n",
    "# columns: root_id, group, nt_type, etc\n",
    "# Keep only root_id, nt_type\n",
    "neurons = pd.read_csv('/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/YaoLabUF/YaoLab/Drosophila_brain_model/neurons.csv.gz')[['root_id', 'nt_type']]\n",
    "\n",
    "# Merging additional data in one data set\n",
    "neurons_data = pd.merge(neurons, pd.merge(classification, neuropil_synapse, on='root_id',how= 'outer'), on='root_id',how='outer')\n",
    "\n",
    "# Load putative PSO lists\n",
    "set_1 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv')\n",
    "set_2 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv')\n",
    "set_3 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f4695523a0084cd",
   "metadata": {},
   "source": [
    "### Find downstream connections of aPhN-SAs\n",
    "- includes all neurons downstream of aPhN-SAs - we will filter out set-set connections later\n",
    "- minimum of 5 synapses between the two neurons"
   ]
  },
  {
   "cell_type": "code",
   "id": "6bec0168f53b084b",
   "metadata": {},
   "source": [
    "# Define function to get outputs of aPhN-SAs\n",
    "def neuronal_outputs(aph1_sa):\n",
    "    # Merge the aph1_sa DataFrame with the 'connections' data, filtering out any connections\n",
    "    # that have fewer than 5 synapses\n",
    "    connectivity = pd.merge(\n",
    "        aph1_sa['root_id'],\n",
    "        connections[['pre_root_id','post_root_id','neuropil','syn_count','nt_type']],\n",
    "        left_on='root_id',\n",
    "        right_on='pre_root_id',\n",
    "        how='inner'\n",
    "    ).query(\"syn_count >= 5\")\n",
    "\n",
    "    # Remove the temporary 'root_id' column that came from the aph1_sa DataFrame\n",
    "    connectivity = connectivity.drop(columns='root_id')\n",
    "\n",
    "    # Define function to categorize connection location\n",
    "    def projection(neuropil):\n",
    "        if neuropil in ['GNG', 'PRW', 'SAD', 'FLA_L', 'FLA_R', 'CAN']:  # Example SEZ-related regions\n",
    "            return 'local'\n",
    "        else:\n",
    "            return 'outside_SEZ'\n",
    "\n",
    "    # Apply the projection categorization to each row in 'connectivity'\n",
    "    connectivity['location_of_connection'] = connectivity['neuropil'].apply(projection)\n",
    "\n",
    "    return connectivity"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc53b6b7b6da31de",
   "metadata": {},
   "source": [
    "# Get the outputs for each set of aPhN-SAs\n",
    "set_1_outputs = neuronal_outputs(set_1)\n",
    "set_2_outputs = neuronal_outputs(set_2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5c7d7e4917a5fcb",
   "metadata": {},
   "source": [
    "#fig.show(renderer=\"browser\")\n",
    "#fig.write_html(\"sankey_diagram.html\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 0. LOAD ALL YOUR DATASETS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# connections: pre_root_id, post_root_id, neuropil, syn_count, nt_type\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "\n",
    "# neuropil synapse table: root_id, input synapses, output synapses\n",
    "neuropil_synapse = (\n",
    "    pd.read_csv(\n",
    "        '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "        'YaoLabUF/YaoLab/Drosophila_brain_model/neuropil_synapse_table.csv.gz'\n",
    "    )\n",
    "    [['root_id','input synapses','output synapses']]\n",
    "    .rename(columns={\n",
    "        'input synapses':'input_synapses',\n",
    "        'output synapses':'output_synapses'\n",
    "    })\n",
    ")\n",
    "\n",
    "# classification: root_id, side\n",
    "classification_side = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','side']]\n",
    "\n",
    "# classification_other: root_id, super_class, class\n",
    "classification_other = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# build super_class map\n",
    "super_map = classification_other.set_index('root_id')['super_class'].to_dict()\n",
    "\n",
    "# neurons table: root_id, nt_type\n",
    "neurons = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/neurons.csv.gz'\n",
    ")[['root_id','nt_type']]\n",
    "\n",
    "# merged neurons_data (if you need it downstream)\n",
    "neurons_data = pd.merge(\n",
    "    neurons,\n",
    "    pd.merge(classification_side, neuropil_synapse, on='root_id', how='outer'),\n",
    "    on='root_id',\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# your three PSO lists\n",
    "set_1 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv')\n",
    "set_2 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv')\n",
    "set_3 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv')\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 1. BUILD THRESHOLDED ADJACENCY LIST (≥5 synapses)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "edge_df = (\n",
    "    connections\n",
    "    .groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "    .agg({'syn_count':'sum'})\n",
    "    .query('syn_count >= 5')\n",
    "    [['pre_root_id','post_root_id']]\n",
    ")\n",
    "\n",
    "adj = defaultdict(set)\n",
    "for u, v in edge_df.values:\n",
    "    adj[int(u)].add(int(v))\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 2. DEFINE YOUR SETS (DCSO, aPhN1, aPhN2)\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "sets = {\n",
    "    'DCSO':  set_1['root_id'].astype(int),\n",
    "    'aPhN1': set_2['root_id'].astype(int),\n",
    "    'aPhN2': set_3['root_id'].astype(int),\n",
    "}\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 3. BFS HELPER TO COMPUTE MINIMAL HOPS TO A TARGET CLASS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "def compute_hops(src_ids, target_class):\n",
    "    hop_of = {}\n",
    "    for src in src_ids:\n",
    "        if src not in adj:\n",
    "            hop_of[src] = None\n",
    "            continue\n",
    "\n",
    "        visited = {src}\n",
    "        queue = deque([(src, 0)])\n",
    "        found = None\n",
    "\n",
    "        while queue and found is None:\n",
    "            node, dist = queue.popleft()\n",
    "            if dist >= 3:\n",
    "                continue\n",
    "            for nei in adj[node]:\n",
    "                if nei in visited:\n",
    "                    continue\n",
    "                visited.add(nei)\n",
    "                nd = dist + 1\n",
    "                if super_map.get(nei) == target_class:\n",
    "                    found = nd\n",
    "                    break\n",
    "                queue.append((nei, nd))\n",
    "\n",
    "        hop_of[src] = found\n",
    "\n",
    "    counts = {'1':0,'2':0,'3':0,'>3':0}\n",
    "    for h in hop_of.values():\n",
    "        if h in (1,2,3):\n",
    "            counts[str(h)] += 1\n",
    "        else:\n",
    "            counts['>3'] += 1\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'hop':  ['1','2','3','>3'],\n",
    "        'count':[counts['1'],counts['2'],counts['3'],counts['>3']]\n",
    "    })\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "# 4. RUN FOR “motor” AND “endocrine” & PLOT STACKED BARS\n",
    "# ───────────────────────────────────────────────────────────────────────────────\n",
    "custom_colors = [\n",
    "    '#EE7733',  # Vibrant Orange\n",
    "    '#009E73',  # Vivid Blue\n",
    "    '#33BBEE',  # Cyan\n",
    "    '#CC3311'   # Red\n",
    "]\n",
    "\n",
    "for target in ['motor','endocrine']:\n",
    "    all_df = []\n",
    "    for label, ids in sets.items():\n",
    "        df_h = compute_hops(ids, target)\n",
    "        df_h['set'] = label\n",
    "        all_df.append(df_h)\n",
    "\n",
    "    df_stack = pd.concat(all_df, ignore_index=True)\n",
    "    df_stack['hop'] = pd.Categorical(df_stack['hop'], ['1','2','3','>3'], ordered=True)\n",
    "    df_stack['set'] = pd.Categorical(df_stack['set'], list(sets.keys()), ordered=True)\n",
    "\n",
    "    pivot = df_stack.pivot(index='set', columns='hop', values='count').fillna(0)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    bottom = np.zeros(len(pivot), dtype=int)\n",
    "    for i, hop in enumerate(['1','2','3','>3']):\n",
    "        ax.bar(pivot.index, pivot[hop], bottom=bottom,\n",
    "               color=custom_colors[i],\n",
    "               label=f\"{hop} hop{'s' if hop!='1' else ''}\")\n",
    "        bottom += pivot[hop].values\n",
    "\n",
    "    ax.set_title(f\"Hops from PSO-SA Sets to {target.capitalize()} Neurons\", fontsize=20)\n",
    "    ax.set_xlabel(\"PSO-SA Set\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of Cells\", fontsize=14)\n",
    "    ax.tick_params(labelsize=12)\n",
    "    ax.legend(title=\"Hops\", frameon=False, prop={'size':12})\n",
    "\n",
    "        # ---- save as SVG ----\n",
    "    fig.set_size_inches(12, 8)\n",
    "    filename = f\"hops_to_{target}.svg\"\n",
    "    fig.savefig(filename, format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "a32d935f1fda4fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for target in ['motor','endocrine']:\n",
    "    # … all your plotting code …\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # define a filename based on the target\n",
    "    filename = f\"hops_to_{target}.svg\"\n",
    "    # save it out at 1200×800px\n",
    "    fig.set_size_inches(12, 8)\n",
    "    fig.savefig(filename, format='svg', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ],
   "id": "cba1675ad38ff1ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and super_class classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO sets (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv')\n",
    "set_2 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv')\n",
    "set_3 = pd.read_csv('/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv')\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(\n",
    "            columns={'root_id':'post_root_id','super_class':'output_super_class'}\n",
    "        ),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build and show a Sankey diagram with maximal vertical spacing\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # 1) build hops\n",
    "    df1 = build_hop_df(grn_df['root_id'],            connections, classification, min_syn)\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # 2) summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum().reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(df1, df2, left_on='post_root_id', right_on='pre_root_id', suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'])['syn_count_2']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(df2, df3, left_on='post_root_id', right_on='pre_root_id', suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'])['syn_count_3']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # 3) build node labels\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx   = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # 4) color mapping\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    all_classes = sorted(\n",
    "        set(df1['output_super_class']) |\n",
    "        set(df2['output_super_class']) |\n",
    "        set(df3['output_super_class'])\n",
    "    )\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = ['lightgrey' if n==title else color_map[n.split(': ',1)[1]] for n in nodes]\n",
    "\n",
    "    # 5) assemble links\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    def add_links(flow_df, src_col, tgt_col):\n",
    "        for _, r in flow_df.iterrows():\n",
    "            s = idx[r[src_col]]\n",
    "            t = idx[r[tgt_col]]\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(r['count'])\n",
    "            link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "\n",
    "    flow1 = flow1.rename(columns={'source':'src','output_super_class':'dst'})\n",
    "    flow1['dst'] = flow1['dst'].map(lambda c: f\"1: {c}\")\n",
    "    add_links(flow1, 'src','dst')\n",
    "\n",
    "    flow2 = flow2.rename(columns={'output_super_class_1':'src','output_super_class_2':'dst'})\n",
    "    flow2['src'] = flow2['src'].map(lambda c: f\"1: {c}\")\n",
    "    flow2['dst'] = flow2['dst'].map(lambda c: f\"2: {c}\")\n",
    "    add_links(flow2, 'src','dst')\n",
    "\n",
    "    flow3 = flow3.rename(columns={'output_super_class_2':'src','output_super_class_3':'dst'})\n",
    "    flow3['src'] = flow3['src'].map(lambda c: f\"2: {c}\")\n",
    "    flow3['dst'] = flow3['dst'].map(lambda c: f\"3: {c}\")\n",
    "    add_links(flow3, 'src','dst')\n",
    "\n",
    "    # 6) hover info\n",
    "    incoming = dict.fromkeys(nodes,0)\n",
    "    outgoing = dict.fromkeys(nodes,0)\n",
    "    for s,t,v in zip(source,target,value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # 7) x positions\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "\n",
    "    # 8) y positions: spread evenly down the column\n",
    "    y = []\n",
    "    for col in (col1, col2, col3, col4):\n",
    "        n = len(col)\n",
    "        if n == 1:\n",
    "            y.append(0.5)\n",
    "        else:\n",
    "            y.extend(np.linspace(0, 1, n))\n",
    "\n",
    "    # 9) draw Sankey with freeform arrangement\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=f\"{title}\", font_size=14)\n",
    "    fig.write_image(f\"{title.replace('/','_')}.svg\", width=1200, height=800, scale=2)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 10. Generate a Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "727684226e3507da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bf711beedba17e6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "de876e18e1e00127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build & show a 3-hop Sankey, but filter df2 by motor criteria\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # 1st hop\n",
    "    df1 = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "\n",
    "    # full 2nd & 3rd hops\n",
    "    df2_full = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # find 2nd-order roots that either are motor or feed into motor in 3rd hop\n",
    "    motor_pre_ids = df3_full.loc[df3_full['output_super_class']=='motor','pre_root_id'].unique()\n",
    "    df2_direct = df2_full[df2_full['output_super_class']=='motor']\n",
    "    df2_feed   = df2_full[df2_full['post_root_id'].isin(motor_pre_ids)]\n",
    "    df2 = pd.concat([df2_direct, df2_feed], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # recompute 3rd hop from the filtered df2\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows for each hop ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble nodes & index mapping ---\n",
    "    col1 = [title]\n",
    "    col2 = sorted(flow1['target'].unique())\n",
    "    col3 = sorted(flow2['target'].unique())\n",
    "    col4 = sorted(flow3['target'].unique())\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- assign colors via Safe palette ---\n",
    "    all_classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = ['lightgrey' if n==title else color_map[n.split(': ',1)[1]] for n in nodes]\n",
    "\n",
    "    # --- build Sankey link lists ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- compute hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- x-axis positions for the 4 columns ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x,\n",
    "            color=node_colors,\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate the filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "bc756681e41b5434",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fdb05d1589bb423c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build & show a 3-hop Sankey with vertical spacing\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # hop-1, hop-2, hop-3\n",
    "    df1 = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum().reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'])['syn_count_2']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'])['syn_count_3']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # assemble nodes\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx   = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # colors\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    all_classes = sorted({\n",
    "        *df1['output_super_class'],\n",
    "        *df2['output_super_class'],\n",
    "        *df3['output_super_class']\n",
    "    })\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else color_map[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # build links\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for _, r in flow1.iterrows():\n",
    "        s = idx[title]\n",
    "        t = idx[f\"1: {r['output_super_class']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "    for _, r in flow2.iterrows():\n",
    "        s = idx[f\"1: {r['output_super_class_1']}\"]\n",
    "        t = idx[f\"2: {r['output_super_class_2']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "    for _, r in flow3.iterrows():\n",
    "        s = idx[f\"2: {r['output_super_class_2']}\"]\n",
    "        t = idx[f\"3: {r['output_super_class_3']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "\n",
    "    # hover info\n",
    "    incoming = dict.fromkeys(nodes,0)\n",
    "    outgoing = dict.fromkeys(nodes,0)\n",
    "    for s,t,v in zip(source,target,value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # x positions\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    # y positions for vertical spacing\n",
    "    lens = [len(col1),len(col2),len(col3),len(col4)]\n",
    "    y = []\n",
    "    for n in lens:\n",
    "        y.extend(list(np.linspace(0,1,n))) if n>1 else y.append(0.5)\n",
    "\n",
    "    # plot\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black',width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "f81a994e7f2d1cca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build & show a 3-hop Sankey with df1 pruned to only those that\n",
    "#       either directly go to motor (hop1→motor) or reach motor in hop2\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # 1) full hop-1, hop-2, hop-3\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # 2) find which hop-2 seeds (pre_root_id) reach motor in hop-3\n",
    "    motor_in_hop3 = df3_full.loc[df3_full['output_super_class']=='motor','pre_root_id'].unique()\n",
    "\n",
    "    # 3) prune df1: keep rows where\n",
    "    #    a) output_super_class == 'motor' (direct)\n",
    "    # OR b) the post_root_id is in motor_in_hop3 (indirect)\n",
    "    mask_direct = df1_full['output_super_class']=='motor'\n",
    "    mask_indirect = df1_full['post_root_id'].isin(motor_in_hop3)\n",
    "    df1 = df1_full[mask_direct | mask_indirect].reset_index(drop=True)\n",
    "\n",
    "    # 4) rebuild hop-2 & hop-3 from pruned df1\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "84321907a5aa9a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: 3-hop Sankey, prune df1 by direct/indirect/third-hop motor connection\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Compute all hops fully\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # 1. Direct: post_root_id in df1 directly to motor\n",
    "    mask_direct = df1_full['output_super_class'] == 'motor'\n",
    "\n",
    "    # 2. Indirect: post_root_id in df1 connects (in df2) to a motor neuron\n",
    "    motor_in_hop2 = df2_full.loc[df2_full['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    mask_hop2 = df1_full['post_root_id'].isin(motor_in_hop2)\n",
    "\n",
    "    # 3. Third-hop: post_root_id in df1 connects (via df2, df3) to a motor neuron\n",
    "    # Find roots in df3 that are motor; backtrack to get the pre_root_id in df2 (which are post_root_id in df1)\n",
    "    motor_in_hop3 = df3_full.loc[df3_full['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    # Now find which roots in df2 connect to those\n",
    "    hop2_roots_to_motor3 = df2_full[df2_full['post_root_id'].isin(motor_in_hop3)]['pre_root_id'].unique()\n",
    "    mask_hop3 = df1_full['post_root_id'].isin(hop2_roots_to_motor3)\n",
    "\n",
    "    # Combined mask: keep if any are True\n",
    "    df1 = df1_full[mask_direct | mask_hop2 | mask_hop3].reset_index(drop=True)\n",
    "\n",
    "    # Rebuild hops from the pruned df1\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "5ae1fe02ea463206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ff440400fbf028cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: 3-hop Sankey with filtering for motor at each hop\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Compute all hops fully\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df1 (1st hop) as before (direct motor or motor downstream in hop2 or hop3) ----\n",
    "    mask_direct1 = df1_full['output_super_class'] == 'motor'\n",
    "    motor_in_hop2 = df2_full.loc[df2_full['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    mask_hop2_1 = df1_full['post_root_id'].isin(motor_in_hop2)\n",
    "    motor_in_hop3 = df3_full.loc[df3_full['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    hop2_roots_to_motor3 = df2_full[df2_full['post_root_id'].isin(motor_in_hop3)]['pre_root_id'].unique()\n",
    "    mask_hop3_1 = df1_full['post_root_id'].isin(hop2_roots_to_motor3)\n",
    "    df1 = df1_full[mask_direct1 | mask_hop2_1 | mask_hop3_1].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df2 and df3 from pruned df1 ----\n",
    "    df2_full2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full2 = build_hop_df(df2_full2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df2 (direct to motor OR downstream to motor in df3) ----\n",
    "    mask_direct2 = df2_full2['output_super_class'] == 'motor'\n",
    "    motor_in_df3 = df3_full2.loc[df3_full2['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    mask_downstream2 = df2_full2['post_root_id'].isin(motor_in_df3)\n",
    "    df2 = df2_full2[mask_direct2 | mask_downstream2].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df3 from pruned df2 ----\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "582e935ded44f412",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: 3-hop Sankey, filtering at each hop for motor connectivity\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Compute all hops fully\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df1 (1st hop): direct to motor, or to df2/df3 roots that connect to motor\n",
    "    mask_direct1 = df1_full['output_super_class'] == 'motor'\n",
    "    motor_in_hop2 = df2_full.loc[df2_full['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    mask_hop2_1 = df1_full['post_root_id'].isin(motor_in_hop2)\n",
    "    motor_in_hop3 = df3_full.loc[df3_full['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    hop2_roots_to_motor3 = df2_full[df2_full['post_root_id'].isin(motor_in_hop3)]['pre_root_id'].unique()\n",
    "    mask_hop3_1 = df1_full['post_root_id'].isin(hop2_roots_to_motor3)\n",
    "    df1 = df1_full[mask_direct1 | mask_hop2_1 | mask_hop3_1].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df2 and df3 from pruned df1 ----\n",
    "    df2_full2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full2 = build_hop_df(df2_full2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df2: direct to motor, or to df3 roots that connect to motor\n",
    "    mask_direct2 = df2_full2['output_super_class'] == 'motor'\n",
    "    motor_in_df3 = df3_full2.loc[df3_full2['output_super_class'] == 'motor', 'pre_root_id'].unique()\n",
    "    mask_downstream2 = df2_full2['post_root_id'].isin(motor_in_df3)\n",
    "    df2 = df2_full2[mask_direct2 | mask_downstream2].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df3 from pruned df2 ----\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- FINAL: Only keep df3 rows that connect to motor\n",
    "    df3 = df3[df3['output_super_class'] == 'motor'].reset_index(drop=True)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "        # ---- SAVE AS SVG ----\n",
    "    svg_filename = f\"{title.replace(' ', '_')}.svg\"\n",
    "    fig.write_image(svg_filename)          # <— saves the figure\n",
    "    print(f\"Saved Sankey for {title} as {svg_filename}\")\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "673e28a8e10f4fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5cef6d8a2e2430b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7368f42c9a78f59f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "fc2a6a78a291192a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6509440b949b00ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build & show a 3-hop Sankey, but filter df2 by endocrine criteria\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # 1st hop\n",
    "    df1 = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "\n",
    "    # full 2nd & 3rd hops\n",
    "    df2_full = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # find 2nd-order roots that either are endocrine or feed into endocrine in 3rd hop\n",
    "    endocrine_pre_ids = df3_full.loc[df3_full['output_super_class']=='endocrine','pre_root_id'].unique()\n",
    "    df2_direct = df2_full[df2_full['output_super_class']=='endocrine']\n",
    "    df2_feed   = df2_full[df2_full['post_root_id'].isin(endocrine_pre_ids)]\n",
    "    df2 = pd.concat([df2_direct, df2_feed], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    # recompute 3rd hop from the filtered df2\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows for each hop ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble nodes & index mapping ---\n",
    "    col1 = [title]\n",
    "    col2 = sorted(flow1['target'].unique())\n",
    "    col3 = sorted(flow2['target'].unique())\n",
    "    col4 = sorted(flow3['target'].unique())\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- assign colors via Safe palette ---\n",
    "    all_classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = ['lightgrey' if n==title else color_map[n.split(': ',1)[1]] for n in nodes]\n",
    "\n",
    "    # --- build Sankey link lists ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- compute hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- x-axis positions for the 4 columns ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x,\n",
    "            color=node_colors,\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate the filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "6171daec8d3f2831",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6626163a016833ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build & show a 3-hop Sankey with vertical spacing\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # hop-1, hop-2, hop-3\n",
    "    df1 = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # summarize flows\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class')['syn_count']\n",
    "           .sum().reset_index(name='count')\n",
    "           .assign(source=title)\n",
    "    )\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'])['syn_count_2']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'])['syn_count_3']\n",
    "          .sum().reset_index(name='count')\n",
    "    )\n",
    "\n",
    "    # assemble nodes\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx   = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # colors\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    all_classes = sorted({\n",
    "        *df1['output_super_class'],\n",
    "        *df2['output_super_class'],\n",
    "        *df3['output_super_class']\n",
    "    })\n",
    "    color_map = {cls: palette[i % len(palette)] for i,cls in enumerate(all_classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else color_map[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # build links\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for _, r in flow1.iterrows():\n",
    "        s = idx[title]\n",
    "        t = idx[f\"1: {r['output_super_class']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "    for _, r in flow2.iterrows():\n",
    "        s = idx[f\"1: {r['output_super_class_1']}\"]\n",
    "        t = idx[f\"2: {r['output_super_class_2']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "    for _, r in flow3.iterrows():\n",
    "        s = idx[f\"2: {r['output_super_class_2']}\"]\n",
    "        t = idx[f\"3: {r['output_super_class_3']}\"]\n",
    "        source.append(s); target.append(t); value.append(r['count'])\n",
    "        link_colors.append(node_colors[s].replace('rgb','rgba').replace(')',',0.5)'))\n",
    "\n",
    "    # hover info\n",
    "    incoming = dict.fromkeys(nodes,0)\n",
    "    outgoing = dict.fromkeys(nodes,0)\n",
    "    for s,t,v in zip(source,target,value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # x positions\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    # y positions for vertical spacing\n",
    "    lens = [len(col1),len(col2),len(col3),len(col4)]\n",
    "    y = []\n",
    "    for n in lens:\n",
    "        y.extend(list(np.linspace(0,1,n))) if n>1 else y.append(0.5)\n",
    "\n",
    "    # plot\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='freeform',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black',width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "6ea68215341865c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: build & show a 3-hop Sankey with df1 pruned to only those that\n",
    "#       either directly go to endocrine (hop1→endocrine) or reach endocrine in hop2\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # 1) full hop-1, hop-2, hop-3\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # 2) find which hop-2 seeds (pre_root_id) reach endocrine in hop-3\n",
    "    endocrine_in_hop3 = df3_full.loc[df3_full['output_super_class']=='endocrine','pre_root_id'].unique()\n",
    "\n",
    "    # 3) prune df1: keep rows where\n",
    "    #    a) output_super_class == 'endocrine' (direct)\n",
    "    # OR b) the post_root_id is in endocrine_in_hop3 (indirect)\n",
    "    mask_direct = df1_full['output_super_class']=='endocrine'\n",
    "    mask_indirect = df1_full['post_root_id'].isin(endocrine_in_hop3)\n",
    "    df1 = df1_full[mask_direct | mask_indirect].reset_index(drop=True)\n",
    "\n",
    "    # 4) rebuild hop-2 & hop-3 from pruned df1\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "a4e860a7cfa1e9c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: 3-hop Sankey, prune df1 by direct/indirect/third-hop endocrine connection\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Compute all hops fully\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # 1. Direct: post_root_id in df1 directly to endocrine\n",
    "    mask_direct = df1_full['output_super_class'] == 'endocrine'\n",
    "\n",
    "    # 2. Indirect: post_root_id in df1 connects (in df2) to a endocrine neuron\n",
    "    endocrine_in_hop2 = df2_full.loc[df2_full['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    mask_hop2 = df1_full['post_root_id'].isin(endocrine_in_hop2)\n",
    "\n",
    "    # 3. Third-hop: post_root_id in df1 connects (via df2, df3) to a endocrine neuron\n",
    "    # Find roots in df3 that are endocrine; backtrack to get the pre_root_id in df2 (which are post_root_id in df1)\n",
    "    endocrine_in_hop3 = df3_full.loc[df3_full['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    # Now find which roots in df2 connect to those\n",
    "    hop2_roots_to_endocrine3 = df2_full[df2_full['post_root_id'].isin(endocrine_in_hop3)]['pre_root_id'].unique()\n",
    "    mask_hop3 = df1_full['post_root_id'].isin(hop2_roots_to_endocrine3)\n",
    "\n",
    "    # Combined mask: keep if any are True\n",
    "    df1 = df1_full[mask_direct | mask_hop2 | mask_hop3].reset_index(drop=True)\n",
    "\n",
    "    # Rebuild hops from the pruned df1\n",
    "    df2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "ecffad0beada5cd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: 3-hop Sankey with filtering for endocrine at each hop\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Compute all hops fully\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df1 (1st hop) as before (direct endocrine or endocrine downstream in hop2 or hop3) ----\n",
    "    mask_direct1 = df1_full['output_super_class'] == 'endocrine'\n",
    "    endocrine_in_hop2 = df2_full.loc[df2_full['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    mask_hop2_1 = df1_full['post_root_id'].isin(endocrine_in_hop2)\n",
    "    endocrine_in_hop3 = df3_full.loc[df3_full['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    hop2_roots_to_endocrine3 = df2_full[df2_full['post_root_id'].isin(endocrine_in_hop3)]['pre_root_id'].unique()\n",
    "    mask_hop3_1 = df1_full['post_root_id'].isin(hop2_roots_to_endocrine3)\n",
    "    df1 = df1_full[mask_direct1 | mask_hop2_1 | mask_hop3_1].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df2 and df3 from pruned df1 ----\n",
    "    df2_full2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full2 = build_hop_df(df2_full2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df2 (direct to endocrine OR downstream to endocrine in df3) ----\n",
    "    mask_direct2 = df2_full2['output_super_class'] == 'endocrine'\n",
    "    endocrine_in_df3 = df3_full2.loc[df3_full2['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    mask_downstream2 = df2_full2['post_root_id'].isin(endocrine_in_df3)\n",
    "    df2 = df2_full2[mask_direct2 | mask_downstream2].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df3 from pruned df2 ----\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "d1b99e912c12f563",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 0. Load FlyWire connectome and classification\n",
    "# ----------------------------------------------------------------------------\n",
    "connections = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/connections.csv.gz'\n",
    ")\n",
    "classification = pd.read_csv(\n",
    "    '/Users/yaolab/Library/CloudStorage/OneDrive-UniversityofFlorida/'\n",
    "    'YaoLabUF/YaoLab/Drosophila_brain_model/classification.csv.gz'\n",
    ")[['root_id','super_class']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1. Load your three PSO lists (each CSV has a 'root_id' column)\n",
    "# ----------------------------------------------------------------------------\n",
    "set_1 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_1.csv'\n",
    ")\n",
    "set_2 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_2.csv'\n",
    ")\n",
    "set_3 = pd.read_csv(\n",
    "    '/Users/yaolab/Downloads/taste-connectome-main/aPhN-SA_v3/set_3.csv'\n",
    ")\n",
    "\n",
    "sets = {\n",
    "    'DCSO':  set_1,\n",
    "    'aPhN1': set_2,\n",
    "    'aPhN2': set_3,\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Helper: filter connections by source IDs, threshold syn_count, attach superclass\n",
    "# ----------------------------------------------------------------------------\n",
    "def build_hop_df(src_ids, connections, classification, min_syn=5):\n",
    "    df = connections[connections['pre_root_id'].isin(src_ids)]\n",
    "    summed = (\n",
    "        df.groupby(['pre_root_id','post_root_id'], as_index=False)\n",
    "          .agg({'syn_count':'sum'})\n",
    "          .query('syn_count >= @min_syn')\n",
    "    )\n",
    "    merged = pd.merge(\n",
    "        summed,\n",
    "        classification.rename(columns={\n",
    "            'root_id':'post_root_id',\n",
    "            'super_class':'output_super_class'\n",
    "        }),\n",
    "        on='post_root_id', how='left'\n",
    "    )\n",
    "    return merged[['pre_root_id','post_root_id','output_super_class','syn_count']]\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Core: 3-hop Sankey, filtering at each hop for endocrine connectivity\n",
    "# ----------------------------------------------------------------------------\n",
    "def plot_sankey_dynamic(grn_df, title, connections, classification, min_syn=5):\n",
    "    # Compute all hops fully\n",
    "    df1_full = build_hop_df(grn_df['root_id'], connections, classification, min_syn)\n",
    "    df2_full = build_hop_df(df1_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full = build_hop_df(df2_full['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df1 (1st hop): direct to endocrine, or to df2/df3 roots that connect to endocrine\n",
    "    mask_direct1 = df1_full['output_super_class'] == 'endocrine'\n",
    "    endocrine_in_hop2 = df2_full.loc[df2_full['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    mask_hop2_1 = df1_full['post_root_id'].isin(endocrine_in_hop2)\n",
    "    endocrine_in_hop3 = df3_full.loc[df3_full['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    hop2_roots_to_endocrine3 = df2_full[df2_full['post_root_id'].isin(endocrine_in_hop3)]['pre_root_id'].unique()\n",
    "    mask_hop3_1 = df1_full['post_root_id'].isin(hop2_roots_to_endocrine3)\n",
    "    df1 = df1_full[mask_direct1 | mask_hop2_1 | mask_hop3_1].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df2 and df3 from pruned df1 ----\n",
    "    df2_full2 = build_hop_df(df1['post_root_id'].unique(), connections, classification, min_syn)\n",
    "    df3_full2 = build_hop_df(df2_full2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- Filter df2: direct to endocrine, or to df3 roots that connect to endocrine\n",
    "    mask_direct2 = df2_full2['output_super_class'] == 'endocrine'\n",
    "    endocrine_in_df3 = df3_full2.loc[df3_full2['output_super_class'] == 'endocrine', 'pre_root_id'].unique()\n",
    "    mask_downstream2 = df2_full2['post_root_id'].isin(endocrine_in_df3)\n",
    "    df2 = df2_full2[mask_direct2 | mask_downstream2].reset_index(drop=True)\n",
    "\n",
    "    # ---- Rebuild df3 from pruned df2 ----\n",
    "    df3 = build_hop_df(df2['post_root_id'].unique(), connections, classification, min_syn)\n",
    "\n",
    "    # ---- FINAL: Only keep df3 rows that connect to endocrine\n",
    "    df3 = df3[df3['output_super_class'] == 'endocrine'].reset_index(drop=True)\n",
    "\n",
    "    # --- summarize flows ---\n",
    "    flow1 = (\n",
    "        df1.groupby('output_super_class', as_index=False)['syn_count']\n",
    "           .sum().rename(columns={'syn_count':'count'})\n",
    "           .assign(source=title,\n",
    "                   target=lambda d: '1: ' + d['output_super_class'])\n",
    "    )\n",
    "\n",
    "    m12 = pd.merge(df1, df2,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_1','_2'))\n",
    "    flow2 = (\n",
    "        m12.groupby(['output_super_class_1','output_super_class_2'], as_index=False)\n",
    "           ['syn_count_2'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_1':'source',\n",
    "               'output_super_class_2':'target',\n",
    "               'syn_count_2':'count'\n",
    "           })\n",
    "    )\n",
    "    flow2['source'] = flow2['source'].apply(lambda c: '1: ' + c)\n",
    "    flow2['target'] = flow2['target'].apply(lambda c: '2: ' + c)\n",
    "\n",
    "    m23 = pd.merge(df2, df3,\n",
    "                   left_on='post_root_id', right_on='pre_root_id',\n",
    "                   suffixes=('_2','_3'))\n",
    "    flow3 = (\n",
    "        m23.groupby(['output_super_class_2','output_super_class_3'], as_index=False)\n",
    "           ['syn_count_3'].sum()\n",
    "           .rename(columns={\n",
    "               'output_super_class_2':'source',\n",
    "               'output_super_class_3':'target',\n",
    "               'syn_count_3':'count'\n",
    "           })\n",
    "    )\n",
    "    flow3['source'] = flow3['source'].apply(lambda c: '2: ' + c)\n",
    "    flow3['target'] = flow3['target'].apply(lambda c: '3: ' + c)\n",
    "\n",
    "    # --- assemble node lists & indices ---\n",
    "    col1 = [title]\n",
    "    col2 = [f\"1: {c}\" for c in sorted(df1['output_super_class'].unique())]\n",
    "    col3 = [f\"2: {c}\" for c in sorted(df2['output_super_class'].unique())]\n",
    "    col4 = [f\"3: {c}\" for c in sorted(df3['output_super_class'].unique())]\n",
    "    nodes = col1 + col2 + col3 + col4\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # --- color mapping ---\n",
    "    classes = sorted({n.split(': ',1)[1] for n in col2+col3+col4})\n",
    "    palette = px.colors.qualitative.Safe\n",
    "    cmap = {cls: palette[i % len(palette)] for i,cls in enumerate(classes)}\n",
    "    node_colors = [\n",
    "        'lightgrey' if n==title else cmap[n.split(': ',1)[1]]\n",
    "        for n in nodes\n",
    "    ]\n",
    "\n",
    "    # --- build Sankey link arrays ---\n",
    "    source, target, value, link_colors = [], [], [], []\n",
    "    for df in (flow1, flow2, flow3):\n",
    "        for _, r in df.iterrows():\n",
    "            s, t, v = idx[r['source']], idx[r['target']], r['count']\n",
    "            source.append(s)\n",
    "            target.append(t)\n",
    "            value.append(v)\n",
    "            rgba = node_colors[s].replace('rgb(', 'rgba(').replace(')', ',0.5)')\n",
    "            link_colors.append(rgba)\n",
    "\n",
    "    # --- hover info ---\n",
    "    incoming = dict.fromkeys(nodes, 0)\n",
    "    outgoing = dict.fromkeys(nodes, 0)\n",
    "    for s,t,v in zip(source, target, value):\n",
    "        outgoing[nodes[s]] += v\n",
    "        incoming[nodes[t]]  += v\n",
    "    customdata = [f\"Incoming: {incoming[n]}<br>Outgoing: {outgoing[n]}\" for n in nodes]\n",
    "\n",
    "    # --- layout positions ---\n",
    "    x = [0.0]*len(col1) + [0.33]*len(col2) + [0.66]*len(col3) + [1.0]*len(col4)\n",
    "    lens = [len(col1), len(col2), len(col3), len(col4)]\n",
    "    y = []\n",
    "    for L in lens:\n",
    "        y.extend(list(np.linspace(0,1,L))) if L>1 else y.append(0.5)\n",
    "\n",
    "    # --- plot Sankey ---\n",
    "    fig = go.Figure(go.Sankey(\n",
    "        arrangement='snap',\n",
    "        node=dict(\n",
    "            label=nodes,\n",
    "            x=x, y=y,\n",
    "            color=node_colors,\n",
    "            pad=15, thickness=20,\n",
    "            line=dict(color='black', width=0.5),\n",
    "            customdata=customdata,\n",
    "            hovertemplate='%{customdata}<extra>%{label}</extra>'\n",
    "        ),\n",
    "        link=dict(source=source, target=target, value=value, color=link_colors)\n",
    "    ))\n",
    "    fig.update_layout(title_text=title, font_size=14)\n",
    "        # ---- SAVE AS SVG ----\n",
    "    svg_filename = f\"{title.replace(' ', '_')}.svg\"\n",
    "    fig.write_image(svg_filename)          # <— saves the figure\n",
    "    print(f\"Saved Sankey for {title} as {svg_filename}\")\n",
    "    fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Generate filtered Sankey for each PSO set\n",
    "# ----------------------------------------------------------------------------\n",
    "for label, df in sets.items():\n",
    "    plot_sankey_dynamic(df, label, connections, classification)\n"
   ],
   "id": "a335a1ecde233539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d696ffb8e766a300",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "85e5e20057f48a45",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
